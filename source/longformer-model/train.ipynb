{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ed743c15",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/ec2-user/anaconda3/envs/python3/bin/python\n"
     ]
    }
   ],
   "source": [
    "!which python"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "bc010be2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting package metadata (current_repodata.json): done\n",
      "Solving environment: / \n",
      "The environment is inconsistent, please check the package plan carefully\n",
      "The following packages are causing the inconsistency:\n",
      "\n",
      "  - conda-forge/noarch::tqdm==4.62.3=pyhd8ed1ab_0\n",
      "  - conda-forge/noarch::black==21.11b1=pyhd8ed1ab_0\n",
      "  - conda-forge/linux-64::conda-package-handling==1.7.3=py38h497a2fe_1\n",
      "  - conda-forge/noarch::dask-core==2021.11.2=pyhd8ed1ab_0\n",
      "  - conda-forge/noarch::imageio==2.9.0=py_0\n",
      "  - conda-forge/linux-64::pytest==6.2.5=py38h578d9bd_1\n",
      "  - conda-forge/linux-64::watchdog==2.1.6=py38h578d9bd_1\n",
      "  - conda-forge/linux-64::aiohttp==3.8.1=py38h497a2fe_0\n",
      "  - conda-forge/linux-64::astropy==5.0=py38h6c62de6_0\n",
      "  - conda-forge/linux-64::bokeh==2.4.2=py38h578d9bd_0\n",
      "  - conda-forge/linux-64::distributed==2021.11.2=py38h578d9bd_0\n",
      "  - conda-forge/noarch::flask==2.0.2=pyhd8ed1ab_0\n",
      "  - conda-forge/linux-64::matplotlib-base==3.5.0=py38hf4fb855_0\n",
      "  - conda-forge/noarch::nbformat==5.1.3=pyhd8ed1ab_0\n",
      "  - conda-forge/noarch::pylint==2.12.1=pyhd8ed1ab_0\n",
      "  - conda-forge/noarch::dask==2021.11.2=pyhd8ed1ab_0\n",
      "  - conda-forge/noarch::nbclient==0.5.9=pyhd8ed1ab_0\n",
      "  - conda-forge/noarch::networkx==2.6.3=pyhd8ed1ab_1\n",
      "  - conda-forge/noarch::python-lsp-server==1.3.2=pyhd8ed1ab_0\n",
      "  - conda-forge/noarch::seaborn-base==0.11.2=pyhd8ed1ab_0\n",
      "  - conda-forge/linux-64::nbconvert==6.3.0=py38h578d9bd_1\n",
      "  - conda-forge/noarch::pyls-spyder==0.4.0=pyhd8ed1ab_0\n",
      "  - conda-forge/noarch::python-lsp-black==1.0.0=pyhd8ed1ab_0\n",
      "  - conda-forge/noarch::requests==2.26.0=pyhd8ed1ab_1\n",
      "  - conda-forge/noarch::seaborn==0.11.2=hd8ed1ab_0\n",
      "  - conda-forge/noarch::anaconda-client==1.8.0=pyhd8ed1ab_0\n",
      "  - conda-forge/linux-64::conda==4.11.0=py38h578d9bd_0\n",
      "  - conda-forge/noarch::cookiecutter==1.7.0=py_0\n",
      "  - conda-forge/noarch::jupyter_server==1.12.1=pyhd8ed1ab_0\n",
      "  - conda-forge/linux-64::matplotlib==3.5.0=py38h578d9bd_0\n",
      "  - conda-forge/noarch::pooch==1.5.2=pyhd8ed1ab_0\n",
      "  - conda-forge/noarch::anaconda-project==0.10.2=pyhd8ed1ab_0\n",
      "  - defaults/noarch::conda-token==0.3.0=pyhd3eb1b0_0\n",
      "  - conda-forge/noarch::ipyparallel==8.0.0=pyhd8ed1ab_0\n",
      "  - conda-forge/noarch::jupyterlab_server==2.8.2=pyhd8ed1ab_0\n",
      "  - conda-forge/noarch::notebook==6.4.6=pyha770c72_0\n",
      "  - conda-forge/linux-64::scikit-image==0.18.3=py38h43a58ef_0\n",
      "  - conda-forge/linux-64::nb_conda==2.2.1=py38h578d9bd_4\n",
      "  - conda-forge/noarch::nbclassic==0.3.4=pyhd8ed1ab_0\n",
      "  - conda-forge/linux-64::widgetsnbextension==3.5.2=py38h578d9bd_1\n",
      "  - conda-forge/noarch::ipywidgets==7.6.5=pyhd8ed1ab_0\n",
      "  - conda-forge/noarch::jupyterlab==3.2.4=pyhd8ed1ab_0\n",
      "  - conda-forge/linux-64::jupyter==1.0.0=py38h578d9bd_7\n",
      "  - conda-forge/noarch::numpydoc==1.1.0=py_1\n",
      "  - conda-forge/linux-64::spyder==5.2.0=py38h578d9bd_0\n",
      "  - conda-forge/noarch::sphinxcontrib-serializinghtml==1.1.5=pyhd8ed1ab_1\n",
      "  - conda-forge/noarch::sphinxcontrib-websupport==1.2.4=pyhd8ed1ab_1\n",
      "  - defaults/linux-64::_anaconda_depends==2021.11=py38_0\n",
      "failed with initial frozen solve. Retrying with flexible solve.\n",
      "Solving environment: failed with repodata from current_repodata.json, will retry with next repodata source.\n",
      "Collecting package metadata (repodata.json): done\n",
      "Solving environment: \\ \n",
      "The environment is inconsistent, please check the package plan carefully\n",
      "The following packages are causing the inconsistency:\n",
      "\n",
      "  - conda-forge/noarch::tqdm==4.62.3=pyhd8ed1ab_0\n",
      "  - conda-forge/noarch::black==21.11b1=pyhd8ed1ab_0\n",
      "  - conda-forge/linux-64::conda-package-handling==1.7.3=py38h497a2fe_1\n",
      "  - conda-forge/noarch::dask-core==2021.11.2=pyhd8ed1ab_0\n",
      "  - conda-forge/noarch::imageio==2.9.0=py_0\n",
      "  - conda-forge/linux-64::pytest==6.2.5=py38h578d9bd_1\n",
      "  - conda-forge/linux-64::watchdog==2.1.6=py38h578d9bd_1\n",
      "  - conda-forge/linux-64::aiohttp==3.8.1=py38h497a2fe_0\n",
      "  - conda-forge/linux-64::astropy==5.0=py38h6c62de6_0\n",
      "  - conda-forge/linux-64::bokeh==2.4.2=py38h578d9bd_0\n",
      "  - conda-forge/linux-64::distributed==2021.11.2=py38h578d9bd_0\n",
      "  - conda-forge/noarch::flask==2.0.2=pyhd8ed1ab_0\n",
      "  - conda-forge/linux-64::matplotlib-base==3.5.0=py38hf4fb855_0\n",
      "  - conda-forge/noarch::nbformat==5.1.3=pyhd8ed1ab_0\n",
      "  - conda-forge/noarch::pylint==2.12.1=pyhd8ed1ab_0\n",
      "  - conda-forge/noarch::dask==2021.11.2=pyhd8ed1ab_0\n",
      "  - conda-forge/noarch::nbclient==0.5.9=pyhd8ed1ab_0\n",
      "  - conda-forge/noarch::networkx==2.6.3=pyhd8ed1ab_1\n",
      "  - conda-forge/noarch::python-lsp-server==1.3.2=pyhd8ed1ab_0\n",
      "  - conda-forge/noarch::seaborn-base==0.11.2=pyhd8ed1ab_0\n",
      "  - conda-forge/linux-64::nbconvert==6.3.0=py38h578d9bd_1\n",
      "  - conda-forge/noarch::pyls-spyder==0.4.0=pyhd8ed1ab_0\n",
      "  - conda-forge/noarch::python-lsp-black==1.0.0=pyhd8ed1ab_0\n",
      "  - conda-forge/noarch::requests==2.26.0=pyhd8ed1ab_1\n",
      "  - conda-forge/noarch::seaborn==0.11.2=hd8ed1ab_0\n",
      "  - conda-forge/noarch::anaconda-client==1.8.0=pyhd8ed1ab_0\n",
      "  - conda-forge/linux-64::conda==4.11.0=py38h578d9bd_0\n",
      "  - conda-forge/noarch::cookiecutter==1.7.0=py_0\n",
      "  - conda-forge/noarch::jupyter_server==1.12.1=pyhd8ed1ab_0\n",
      "  - conda-forge/linux-64::matplotlib==3.5.0=py38h578d9bd_0\n",
      "  - conda-forge/noarch::pooch==1.5.2=pyhd8ed1ab_0\n",
      "  - conda-forge/noarch::anaconda-project==0.10.2=pyhd8ed1ab_0\n",
      "  - defaults/noarch::conda-token==0.3.0=pyhd3eb1b0_0\n",
      "  - conda-forge/noarch::ipyparallel==8.0.0=pyhd8ed1ab_0\n",
      "  - conda-forge/noarch::jupyterlab_server==2.8.2=pyhd8ed1ab_0\n",
      "  - conda-forge/noarch::notebook==6.4.6=pyha770c72_0\n",
      "  - conda-forge/linux-64::scikit-image==0.18.3=py38h43a58ef_0\n",
      "  - conda-forge/linux-64::nb_conda==2.2.1=py38h578d9bd_4\n",
      "  - conda-forge/noarch::nbclassic==0.3.4=pyhd8ed1ab_0\n",
      "  - conda-forge/linux-64::widgetsnbextension==3.5.2=py38h578d9bd_1\n",
      "  - conda-forge/noarch::ipywidgets==7.6.5=pyhd8ed1ab_0\n",
      "  - conda-forge/noarch::jupyterlab==3.2.4=pyhd8ed1ab_0\n",
      "  - conda-forge/linux-64::jupyter==1.0.0=py38h578d9bd_7\n",
      "  - conda-forge/noarch::numpydoc==1.1.0=py_1\n",
      "  - conda-forge/linux-64::spyder==5.2.0=py38h578d9bd_0\n",
      "  - conda-forge/noarch::sphinxcontrib-serializinghtml==1.1.5=pyhd8ed1ab_1\n",
      "  - conda-forge/noarch::sphinxcontrib-websupport==1.2.4=pyhd8ed1ab_1\n",
      "  - defaults/linux-64::_anaconda_depends==2021.11=py38_0\n",
      "done\n",
      "\n",
      "\n",
      "==> WARNING: A newer version of conda exists. <==\n",
      "  current version: 4.8.4\n",
      "  latest version: 4.13.0\n",
      "\n",
      "Please update conda by running\n",
      "\n",
      "    $ conda update -n base -c defaults conda\n",
      "\n",
      "\n",
      "\n",
      "## Package Plan ##\n",
      "\n",
      "  environment location: /home/ec2-user/anaconda3/envs/python3\n",
      "\n",
      "  added / updated specs:\n",
      "    - cudatoolkit=10.0\n",
      "\n",
      "\n",
      "The following packages will be downloaded:\n",
      "\n",
      "    package                    |            build\n",
      "    ---------------------------|-----------------\n",
      "    ca-certificates-2022.6.15  |       ha878542_0         149 KB  conda-forge\n",
      "    certifi-2022.6.15          |   py38h578d9bd_0         155 KB  conda-forge\n",
      "    colorama-0.4.5             |     pyhd8ed1ab_0          18 KB  conda-forge\n",
      "    cudatoolkit-10.0.130       |      h8c5a6a4_10       336.3 MB  conda-forge\n",
      "    dataclasses-0.8            |     pyhc8e2a94_3          10 KB  conda-forge\n",
      "    docutils-0.18.1            |   py38h578d9bd_1         737 KB  conda-forge\n",
      "    fsspec-2022.5.0            |     pyhd8ed1ab_0          96 KB  conda-forge\n",
      "    jsonschema-4.7.2           |     pyhd8ed1ab_0          63 KB  conda-forge\n",
      "    lxml-4.8.0                 |   py38h0a891b7_2         1.4 MB  conda-forge\n",
      "    openssl-1.1.1o             |       h166bdaf_0         2.1 MB  conda-forge\n",
      "    pillow-8.4.0               |   py38h8e6f84c_0         704 KB  conda-forge\n",
      "    pip-22.1.2                 |     pyhd8ed1ab_0         1.5 MB  conda-forge\n",
      "    pyyaml-6.0                 |   py38h0a891b7_4         182 KB  conda-forge\n",
      "    sphinx-5.0.2               |     pyh6c4a22f_0         1.5 MB  conda-forge\n",
      "    urllib3-1.26.10            |     pyhd8ed1ab_0         101 KB  conda-forge\n",
      "    websocket-client-1.3.3     |     pyhd8ed1ab_0          41 KB  conda-forge\n",
      "    werkzeug-2.1.2             |     pyhd8ed1ab_1         237 KB  conda-forge\n",
      "    ------------------------------------------------------------\n",
      "                                           Total:       345.3 MB\n",
      "\n",
      "The following NEW packages will be INSTALLED:\n",
      "\n",
      "  attrs              conda-forge/noarch::attrs-21.4.0-pyhd8ed1ab_0\n",
      "  colorama           conda-forge/noarch::colorama-0.4.5-pyhd8ed1ab_0\n",
      "  cudatoolkit        conda-forge/linux-64::cudatoolkit-10.0.130-h8c5a6a4_10\n",
      "  dataclasses        conda-forge/noarch::dataclasses-0.8-pyhc8e2a94_3\n",
      "  docutils           conda-forge/linux-64::docutils-0.18.1-py38h578d9bd_1\n",
      "  fsspec             conda-forge/noarch::fsspec-2022.5.0-pyhd8ed1ab_0\n",
      "  jsonschema         conda-forge/noarch::jsonschema-4.7.2-pyhd8ed1ab_0\n",
      "  lxml               conda-forge/linux-64::lxml-4.8.0-py38h0a891b7_2\n",
      "  nltk               conda-forge/noarch::nltk-3.6.7-pyhd8ed1ab_0\n",
      "  pillow             conda-forge/linux-64::pillow-8.4.0-py38h8e6f84c_0\n",
      "  pip                conda-forge/noarch::pip-22.1.2-pyhd8ed1ab_0\n",
      "  pyyaml             conda-forge/linux-64::pyyaml-6.0-py38h0a891b7_4\n",
      "  sphinx             conda-forge/noarch::sphinx-5.0.2-pyh6c4a22f_0\n",
      "  urllib3            conda-forge/noarch::urllib3-1.26.10-pyhd8ed1ab_0\n",
      "  websocket-client   conda-forge/noarch::websocket-client-1.3.3-pyhd8ed1ab_0\n",
      "  werkzeug           conda-forge/noarch::werkzeug-2.1.2-pyhd8ed1ab_1\n",
      "\n",
      "The following packages will be UPDATED:\n",
      "\n",
      "  ca-certificates                      2021.10.8-ha878542_0 --> 2022.6.15-ha878542_0\n",
      "  certifi                          2021.10.8-py38h578d9bd_1 --> 2022.6.15-py38h578d9bd_0\n",
      "  openssl                                 1.1.1l-h7f98852_0 --> 1.1.1o-h166bdaf_0\n",
      "\n",
      "\n",
      "\n",
      "Downloading and Extracting Packages\n",
      "jsonschema-4.7.2     | 63 KB     | ##################################### | 100% \n",
      "pillow-8.4.0         | 704 KB    | ##################################### | 100% \n",
      "cudatoolkit-10.0.130 | 336.3 MB  | ##################################### | 100% \n",
      "ca-certificates-2022 | 149 KB    | ##################################### | 100% \n",
      "openssl-1.1.1o       | 2.1 MB    | ##################################### | 100% \n",
      "dataclasses-0.8      | 10 KB     | ##################################### | 100% \n",
      "werkzeug-2.1.2       | 237 KB    | ##################################### | 100% \n",
      "fsspec-2022.5.0      | 96 KB     | ##################################### | 100% \n",
      "lxml-4.8.0           | 1.4 MB    | ##################################### | 100% \n",
      "pip-22.1.2           | 1.5 MB    | ##################################### | 100% \n",
      "urllib3-1.26.10      | 101 KB    | ##################################### | 100% \n",
      "docutils-0.18.1      | 737 KB    | ##################################### | 100% \n",
      "certifi-2022.6.15    | 155 KB    | ##################################### | 100% \n",
      "websocket-client-1.3 | 41 KB     | ##################################### | 100% \n",
      "pyyaml-6.0           | 182 KB    | ##################################### | 100% \n",
      "sphinx-5.0.2         | 1.5 MB    | ##################################### | 100% \n",
      "colorama-0.4.5       | 18 KB     | ##################################### | 100% \n",
      "Preparing transaction: done\n",
      "Verifying transaction: done\n",
      "Executing transaction: | By downloading and using the CUDA Toolkit conda packages, you accept the terms and conditions of the CUDA End User License Agreement (EULA): https://docs.nvidia.com/cuda/eula/index.html\n",
      "\n",
      "done\n",
      "Looking in indexes: https://pypi.org/simple, https://pip.repos.neuron.amazonaws.com\n",
      "Collecting git+https://github.com/allenai/longformer.git\n",
      "  Cloning https://github.com/allenai/longformer.git to /tmp/pip-req-build-unpzq232\n",
      "  Running command git clone --filter=blob:none --quiet https://github.com/allenai/longformer.git /tmp/pip-req-build-unpzq232\n",
      "  Resolved https://github.com/allenai/longformer.git to commit caefee668e39cacdece7dd603a0bebf24df6d8ca\n",
      "  Preparing metadata (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25hCollecting transformers@ git+http://github.com/ibeltagy/transformers.git@longformer_encoder_decoder#egg=transformers\n",
      "  Cloning http://github.com/ibeltagy/transformers.git (to revision longformer_encoder_decoder) to /tmp/pip-install-cne4x9nh/transformers_8925d2f5a07642c7bc03d91d47a181c8\n",
      "  Running command git clone --filter=blob:none --quiet http://github.com/ibeltagy/transformers.git /tmp/pip-install-cne4x9nh/transformers_8925d2f5a07642c7bc03d91d47a181c8\n",
      "  warning: redirecting to https://github.com/ibeltagy/transformers.git/\n",
      "  Running command git checkout -b longformer_encoder_decoder --track origin/longformer_encoder_decoder\n",
      "  warning: redirecting to https://github.com/ibeltagy/transformers.git/\n",
      "  Switched to a new branch 'longformer_encoder_decoder'\n",
      "  Branch 'longformer_encoder_decoder' set up to track remote branch 'longformer_encoder_decoder' from 'origin'.\n",
      "  Resolved http://github.com/ibeltagy/transformers.git to commit 52d6236dc15ad5142b4146ff74d2ec973fa3da22\n",
      "  Preparing metadata (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25hCollecting pytorch-lightning@ git+http://github.com/ibeltagy/pytorch-lightning.git@v0.8.5_fixes#egg=pytorch-lightning\n",
      "  Cloning http://github.com/ibeltagy/pytorch-lightning.git (to revision v0.8.5_fixes) to /tmp/pip-install-cne4x9nh/pytorch-lightning_9a8c24a5fd6c46c9a9d5370524cd7e26\n",
      "  Running command git clone --filter=blob:none --quiet http://github.com/ibeltagy/pytorch-lightning.git /tmp/pip-install-cne4x9nh/pytorch-lightning_9a8c24a5fd6c46c9a9d5370524cd7e26\n",
      "  warning: redirecting to https://github.com/ibeltagy/pytorch-lightning.git/\n",
      "  Running command git checkout -b v0.8.5_fixes --track origin/v0.8.5_fixes\n",
      "  warning: redirecting to https://github.com/ibeltagy/pytorch-lightning.git/\n",
      "  Switched to a new branch 'v0.8.5_fixes'\n",
      "  Branch 'v0.8.5_fixes' set up to track remote branch 'v0.8.5_fixes' from 'origin'.\n",
      "  Resolved http://github.com/ibeltagy/pytorch-lightning.git to commit 7ed5d849a0c76fa2199162f0283507e36601ded6\n",
      "  Installing build dependencies ... \u001b[?25ldone\n",
      "\u001b[?25h  Getting requirements to build wheel ... \u001b[?25ldone\n",
      "\u001b[?25h  Preparing metadata (pyproject.toml) ... \u001b[?25ldone\n",
      "\u001b[?25hRequirement already satisfied: torch>=1.6.0 in /home/ec2-user/anaconda3/envs/python3/lib/python3.8/site-packages (from longformer==0.1) (1.12.0)\n",
      "Requirement already satisfied: tensorboardX in /home/ec2-user/anaconda3/envs/python3/lib/python3.8/site-packages (from longformer==0.1) (2.5.1)\n",
      "Requirement already satisfied: test-tube==0.7.5 in /home/ec2-user/anaconda3/envs/python3/lib/python3.8/site-packages (from longformer==0.1) (0.7.5)\n",
      "Requirement already satisfied: nlp in /home/ec2-user/anaconda3/envs/python3/lib/python3.8/site-packages (from longformer==0.1) (0.4.0)\n",
      "Requirement already satisfied: rouge_score in /home/ec2-user/anaconda3/envs/python3/lib/python3.8/site-packages (from longformer==0.1) (0.0.4)\n",
      "Requirement already satisfied: imageio>=2.3.0 in /home/ec2-user/anaconda3/envs/python3/lib/python3.8/site-packages (from test-tube==0.7.5->longformer==0.1) (2.9.0)\n",
      "Requirement already satisfied: numpy>=1.13.3 in /home/ec2-user/anaconda3/envs/python3/lib/python3.8/site-packages (from test-tube==0.7.5->longformer==0.1) (1.20.3)\n",
      "Requirement already satisfied: pandas>=0.20.3 in /home/ec2-user/anaconda3/envs/python3/lib/python3.8/site-packages (from test-tube==0.7.5->longformer==0.1) (1.3.4)\n",
      "Requirement already satisfied: tensorboard>=1.15.0 in /home/ec2-user/anaconda3/envs/python3/lib/python3.8/site-packages (from test-tube==0.7.5->longformer==0.1) (2.9.1)\n",
      "Requirement already satisfied: future in /home/ec2-user/anaconda3/envs/python3/lib/python3.8/site-packages (from test-tube==0.7.5->longformer==0.1) (0.18.2)\n",
      "Requirement already satisfied: typing-extensions in /home/ec2-user/anaconda3/envs/python3/lib/python3.8/site-packages (from torch>=1.6.0->longformer==0.1) (4.0.0)\n",
      "Requirement already satisfied: xxhash in /home/ec2-user/anaconda3/envs/python3/lib/python3.8/site-packages (from nlp->longformer==0.1) (3.0.0)\n",
      "Requirement already satisfied: tqdm>=4.27 in /home/ec2-user/anaconda3/envs/python3/lib/python3.8/site-packages (from nlp->longformer==0.1) (4.62.3)\n",
      "Requirement already satisfied: requests>=2.19.0 in /home/ec2-user/anaconda3/envs/python3/lib/python3.8/site-packages (from nlp->longformer==0.1) (2.26.0)\n",
      "Requirement already satisfied: pyarrow>=0.16.0 in /home/ec2-user/anaconda3/envs/python3/lib/python3.8/site-packages (from nlp->longformer==0.1) (7.0.0)\n",
      "Requirement already satisfied: dill in /home/ec2-user/anaconda3/envs/python3/lib/python3.8/site-packages (from nlp->longformer==0.1) (0.3.4)\n",
      "Requirement already satisfied: filelock in /home/ec2-user/anaconda3/envs/python3/lib/python3.8/site-packages (from nlp->longformer==0.1) (3.4.0)\n",
      "Requirement already satisfied: PyYAML>=5.1 in /home/ec2-user/anaconda3/envs/python3/lib/python3.8/site-packages (from pytorch-lightning@ git+http://github.com/ibeltagy/pytorch-lightning.git@v0.8.5_fixes#egg=pytorch-lightning->longformer==0.1) (6.0)\n",
      "Requirement already satisfied: six>=1.14.0 in /home/ec2-user/anaconda3/envs/python3/lib/python3.8/site-packages (from rouge_score->longformer==0.1) (1.16.0)\n",
      "Requirement already satisfied: absl-py in /home/ec2-user/anaconda3/envs/python3/lib/python3.8/site-packages (from rouge_score->longformer==0.1) (1.2.0)\n",
      "Requirement already satisfied: nltk in /home/ec2-user/anaconda3/envs/python3/lib/python3.8/site-packages (from rouge_score->longformer==0.1) (3.7)\n",
      "Requirement already satisfied: protobuf<=3.20.1,>=3.8.0 in /home/ec2-user/anaconda3/envs/python3/lib/python3.8/site-packages (from tensorboardX->longformer==0.1) (3.19.1)\n",
      "Requirement already satisfied: tokenizers==0.8.1.rc2 in /home/ec2-user/anaconda3/envs/python3/lib/python3.8/site-packages (from transformers@ git+http://github.com/ibeltagy/transformers.git@longformer_encoder_decoder#egg=transformers->longformer==0.1) (0.8.1rc2)\n",
      "Requirement already satisfied: packaging in /home/ec2-user/anaconda3/envs/python3/lib/python3.8/site-packages (from transformers@ git+http://github.com/ibeltagy/transformers.git@longformer_encoder_decoder#egg=transformers->longformer==0.1) (21.3)\n",
      "Requirement already satisfied: regex!=2019.12.17 in /home/ec2-user/anaconda3/envs/python3/lib/python3.8/site-packages (from transformers@ git+http://github.com/ibeltagy/transformers.git@longformer_encoder_decoder#egg=transformers->longformer==0.1) (2021.11.10)\n",
      "Requirement already satisfied: sentencepiece!=0.1.92 in /home/ec2-user/anaconda3/envs/python3/lib/python3.8/site-packages (from transformers@ git+http://github.com/ibeltagy/transformers.git@longformer_encoder_decoder#egg=transformers->longformer==0.1) (0.1.96)\n",
      "Requirement already satisfied: sacremoses in /home/ec2-user/anaconda3/envs/python3/lib/python3.8/site-packages (from transformers@ git+http://github.com/ibeltagy/transformers.git@longformer_encoder_decoder#egg=transformers->longformer==0.1) (0.0.53)\n",
      "Requirement already satisfied: pillow in /home/ec2-user/anaconda3/envs/python3/lib/python3.8/site-packages (from imageio>=2.3.0->test-tube==0.7.5->longformer==0.1) (9.0.1)\n",
      "Requirement already satisfied: python-dateutil>=2.7.3 in /home/ec2-user/anaconda3/envs/python3/lib/python3.8/site-packages (from pandas>=0.20.3->test-tube==0.7.5->longformer==0.1) (2.8.2)\n",
      "Requirement already satisfied: pytz>=2017.3 in /home/ec2-user/anaconda3/envs/python3/lib/python3.8/site-packages (from pandas>=0.20.3->test-tube==0.7.5->longformer==0.1) (2021.3)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /home/ec2-user/anaconda3/envs/python3/lib/python3.8/site-packages (from requests>=2.19.0->nlp->longformer==0.1) (3.1)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /home/ec2-user/anaconda3/envs/python3/lib/python3.8/site-packages (from requests>=2.19.0->nlp->longformer==0.1) (1.26.10)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /home/ec2-user/anaconda3/envs/python3/lib/python3.8/site-packages (from requests>=2.19.0->nlp->longformer==0.1) (2022.6.15)\n",
      "Requirement already satisfied: charset-normalizer~=2.0.0 in /home/ec2-user/anaconda3/envs/python3/lib/python3.8/site-packages (from requests>=2.19.0->nlp->longformer==0.1) (2.0.8)\n",
      "Requirement already satisfied: grpcio>=1.24.3 in /home/ec2-user/anaconda3/envs/python3/lib/python3.8/site-packages (from tensorboard>=1.15.0->test-tube==0.7.5->longformer==0.1) (1.47.0)\n",
      "Requirement already satisfied: werkzeug>=1.0.1 in /home/ec2-user/anaconda3/envs/python3/lib/python3.8/site-packages (from tensorboard>=1.15.0->test-tube==0.7.5->longformer==0.1) (2.1.2)\n",
      "Requirement already satisfied: google-auth-oauthlib<0.5,>=0.4.1 in /home/ec2-user/anaconda3/envs/python3/lib/python3.8/site-packages (from tensorboard>=1.15.0->test-tube==0.7.5->longformer==0.1) (0.4.6)\n",
      "Requirement already satisfied: setuptools>=41.0.0 in /home/ec2-user/anaconda3/envs/python3/lib/python3.8/site-packages (from tensorboard>=1.15.0->test-tube==0.7.5->longformer==0.1) (59.4.0)\n",
      "Requirement already satisfied: google-auth<3,>=1.6.3 in /home/ec2-user/anaconda3/envs/python3/lib/python3.8/site-packages (from tensorboard>=1.15.0->test-tube==0.7.5->longformer==0.1) (2.9.1)\n",
      "Requirement already satisfied: tensorboard-data-server<0.7.0,>=0.6.0 in /home/ec2-user/anaconda3/envs/python3/lib/python3.8/site-packages (from tensorboard>=1.15.0->test-tube==0.7.5->longformer==0.1) (0.6.1)\n",
      "Requirement already satisfied: markdown>=2.6.8 in /home/ec2-user/anaconda3/envs/python3/lib/python3.8/site-packages (from tensorboard>=1.15.0->test-tube==0.7.5->longformer==0.1) (3.4.1)\n",
      "Requirement already satisfied: tensorboard-plugin-wit>=1.6.0 in /home/ec2-user/anaconda3/envs/python3/lib/python3.8/site-packages (from tensorboard>=1.15.0->test-tube==0.7.5->longformer==0.1) (1.8.1)\n",
      "Requirement already satisfied: wheel>=0.26 in /home/ec2-user/anaconda3/envs/python3/lib/python3.8/site-packages (from tensorboard>=1.15.0->test-tube==0.7.5->longformer==0.1) (0.37.0)\n",
      "Requirement already satisfied: click in /home/ec2-user/anaconda3/envs/python3/lib/python3.8/site-packages (from nltk->rouge_score->longformer==0.1) (8.0.3)\n",
      "Requirement already satisfied: joblib in /home/ec2-user/anaconda3/envs/python3/lib/python3.8/site-packages (from nltk->rouge_score->longformer==0.1) (1.1.0)\n",
      "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /home/ec2-user/anaconda3/envs/python3/lib/python3.8/site-packages (from packaging->transformers@ git+http://github.com/ibeltagy/transformers.git@longformer_encoder_decoder#egg=transformers->longformer==0.1) (3.0.6)\n",
      "Requirement already satisfied: rsa<5,>=3.1.4 in /home/ec2-user/anaconda3/envs/python3/lib/python3.8/site-packages (from google-auth<3,>=1.6.3->tensorboard>=1.15.0->test-tube==0.7.5->longformer==0.1) (4.7.2)\n",
      "Requirement already satisfied: pyasn1-modules>=0.2.1 in /home/ec2-user/anaconda3/envs/python3/lib/python3.8/site-packages (from google-auth<3,>=1.6.3->tensorboard>=1.15.0->test-tube==0.7.5->longformer==0.1) (0.2.8)\n",
      "Requirement already satisfied: cachetools<6.0,>=2.0.0 in /home/ec2-user/anaconda3/envs/python3/lib/python3.8/site-packages (from google-auth<3,>=1.6.3->tensorboard>=1.15.0->test-tube==0.7.5->longformer==0.1) (5.2.0)\n",
      "Requirement already satisfied: requests-oauthlib>=0.7.0 in /home/ec2-user/anaconda3/envs/python3/lib/python3.8/site-packages (from google-auth-oauthlib<0.5,>=0.4.1->tensorboard>=1.15.0->test-tube==0.7.5->longformer==0.1) (1.3.1)\n",
      "Requirement already satisfied: importlib-metadata>=4.4 in /home/ec2-user/anaconda3/envs/python3/lib/python3.8/site-packages (from markdown>=2.6.8->tensorboard>=1.15.0->test-tube==0.7.5->longformer==0.1) (4.8.2)\n",
      "Requirement already satisfied: zipp>=0.5 in /home/ec2-user/anaconda3/envs/python3/lib/python3.8/site-packages (from importlib-metadata>=4.4->markdown>=2.6.8->tensorboard>=1.15.0->test-tube==0.7.5->longformer==0.1) (3.6.0)\n",
      "Requirement already satisfied: pyasn1<0.5.0,>=0.4.6 in /home/ec2-user/anaconda3/envs/python3/lib/python3.8/site-packages (from pyasn1-modules>=0.2.1->google-auth<3,>=1.6.3->tensorboard>=1.15.0->test-tube==0.7.5->longformer==0.1) (0.4.8)\n",
      "Requirement already satisfied: oauthlib>=3.0.0 in /home/ec2-user/anaconda3/envs/python3/lib/python3.8/site-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<0.5,>=0.4.1->tensorboard>=1.15.0->test-tube==0.7.5->longformer==0.1) (3.2.0)\n"
     ]
    }
   ],
   "source": [
    "!conda install cudatoolkit=10.0 -y\n",
    "!pip install git+https://github.com/allenai/longformer.git"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "0e3b5067",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from longformer.longformer import Longformer, LongformerConfig\n",
    "from longformer.sliding_chunks import pad_to_window_size\n",
    "from transformers import RobertaTokenizer\n",
    "\n",
    "config = LongformerConfig.from_pretrained('longformer-base-4096/') \n",
    "# choose the attention mode 'n2', 'tvm' or 'sliding_chunks'\n",
    "# 'n2': for regular n2 attantion\n",
    "# 'tvm': a custom CUDA kernel implementation of our sliding window attention\n",
    "# 'sliding_chunks': a PyTorch implementation of our sliding window attention\n",
    "config.attention_mode = 'sliding_chunks'\n",
    "\n",
    "model = Longformer.from_pretrained('longformer-base-4096/', config=config)\n",
    "tokenizer = RobertaTokenizer.from_pretrained('roberta-base')\n",
    "tokenizer.model_max_length = model.config.max_position_embeddings\n",
    "\n",
    "SAMPLE_TEXT = ' '.join(['Hello world! '] * 1000)  # long input document\n",
    "\n",
    "input_ids = torch.tensor(tokenizer.encode(SAMPLE_TEXT)).unsqueeze(0)  # batch of size 1\n",
    "\n",
    "# TVM code doesn't work on CPU. Uncomment this if `config.attention_mode = 'tvm'`\n",
    "# model = model.cuda(); input_ids = input_ids.cuda()\n",
    "\n",
    "# Attention mask values -- 0: no attention, 1: local attention, 2: global attention\n",
    "attention_mask = torch.ones(input_ids.shape, dtype=torch.long, device=input_ids.device) # initialize to local attention\n",
    "attention_mask[:, [1, 4, 21,]] =  2  # Set global attention based on the task. For example,\n",
    "                                     # classification: the <s> token\n",
    "                                     # QA: question tokens\n",
    "\n",
    "# padding seqlen to the nearest multiple of 512. Needed for the 'sliding_chunks' attention\n",
    "input_ids, attention_mask = pad_to_window_size(\n",
    "        input_ids, attention_mask, config.attention_window[0], tokenizer.pad_token_id)\n",
    "\n",
    "output = model(input_ids, attention_mask=attention_mask)[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "e539d924",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Looking in indexes: https://pypi.org/simple, https://pip.repos.neuron.amazonaws.com\n",
      "Requirement already satisfied: datasets in /home/ec2-user/anaconda3/envs/python3/lib/python3.8/site-packages (2.3.2)\n",
      "Requirement already satisfied: tqdm>=4.62.1 in /home/ec2-user/anaconda3/envs/python3/lib/python3.8/site-packages (from datasets) (4.62.3)\n",
      "Requirement already satisfied: numpy>=1.17 in /home/ec2-user/anaconda3/envs/python3/lib/python3.8/site-packages (from datasets) (1.20.3)\n",
      "Requirement already satisfied: aiohttp in /home/ec2-user/anaconda3/envs/python3/lib/python3.8/site-packages (from datasets) (3.8.1)\n",
      "Requirement already satisfied: fsspec[http]>=2021.05.0 in /home/ec2-user/anaconda3/envs/python3/lib/python3.8/site-packages (from datasets) (2022.5.0)\n",
      "Requirement already satisfied: requests>=2.19.0 in /home/ec2-user/anaconda3/envs/python3/lib/python3.8/site-packages (from datasets) (2.26.0)\n",
      "Requirement already satisfied: pyarrow>=6.0.0 in /home/ec2-user/anaconda3/envs/python3/lib/python3.8/site-packages (from datasets) (7.0.0)\n",
      "Requirement already satisfied: responses<0.19 in /home/ec2-user/anaconda3/envs/python3/lib/python3.8/site-packages (from datasets) (0.18.0)\n",
      "Requirement already satisfied: packaging in /home/ec2-user/anaconda3/envs/python3/lib/python3.8/site-packages (from datasets) (21.3)\n",
      "Requirement already satisfied: xxhash in /home/ec2-user/anaconda3/envs/python3/lib/python3.8/site-packages (from datasets) (3.0.0)\n",
      "Requirement already satisfied: pandas in /home/ec2-user/anaconda3/envs/python3/lib/python3.8/site-packages (from datasets) (1.3.4)\n",
      "Requirement already satisfied: dill<0.3.6 in /home/ec2-user/anaconda3/envs/python3/lib/python3.8/site-packages (from datasets) (0.3.4)\n",
      "Requirement already satisfied: multiprocess in /home/ec2-user/anaconda3/envs/python3/lib/python3.8/site-packages (from datasets) (0.70.12.2)\n",
      "Requirement already satisfied: huggingface-hub<1.0.0,>=0.1.0 in /home/ec2-user/anaconda3/envs/python3/lib/python3.8/site-packages (from datasets) (0.8.1)\n",
      "Requirement already satisfied: filelock in /home/ec2-user/anaconda3/envs/python3/lib/python3.8/site-packages (from huggingface-hub<1.0.0,>=0.1.0->datasets) (3.4.0)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /home/ec2-user/anaconda3/envs/python3/lib/python3.8/site-packages (from huggingface-hub<1.0.0,>=0.1.0->datasets) (6.0)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /home/ec2-user/anaconda3/envs/python3/lib/python3.8/site-packages (from huggingface-hub<1.0.0,>=0.1.0->datasets) (4.0.0)\n",
      "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /home/ec2-user/anaconda3/envs/python3/lib/python3.8/site-packages (from packaging->datasets) (3.0.6)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /home/ec2-user/anaconda3/envs/python3/lib/python3.8/site-packages (from requests>=2.19.0->datasets) (2022.6.15)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /home/ec2-user/anaconda3/envs/python3/lib/python3.8/site-packages (from requests>=2.19.0->datasets) (1.26.10)\n",
      "Requirement already satisfied: charset-normalizer~=2.0.0 in /home/ec2-user/anaconda3/envs/python3/lib/python3.8/site-packages (from requests>=2.19.0->datasets) (2.0.8)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /home/ec2-user/anaconda3/envs/python3/lib/python3.8/site-packages (from requests>=2.19.0->datasets) (3.1)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in /home/ec2-user/anaconda3/envs/python3/lib/python3.8/site-packages (from aiohttp->datasets) (5.2.0)\n",
      "Requirement already satisfied: attrs>=17.3.0 in /home/ec2-user/anaconda3/envs/python3/lib/python3.8/site-packages (from aiohttp->datasets) (21.4.0)\n",
      "Requirement already satisfied: aiosignal>=1.1.2 in /home/ec2-user/anaconda3/envs/python3/lib/python3.8/site-packages (from aiohttp->datasets) (1.2.0)\n",
      "Requirement already satisfied: async-timeout<5.0,>=4.0.0a3 in /home/ec2-user/anaconda3/envs/python3/lib/python3.8/site-packages (from aiohttp->datasets) (4.0.1)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in /home/ec2-user/anaconda3/envs/python3/lib/python3.8/site-packages (from aiohttp->datasets) (1.2.0)\n",
      "Requirement already satisfied: yarl<2.0,>=1.0 in /home/ec2-user/anaconda3/envs/python3/lib/python3.8/site-packages (from aiohttp->datasets) (1.7.2)\n",
      "Requirement already satisfied: python-dateutil>=2.7.3 in /home/ec2-user/anaconda3/envs/python3/lib/python3.8/site-packages (from pandas->datasets) (2.8.2)\n",
      "Requirement already satisfied: pytz>=2017.3 in /home/ec2-user/anaconda3/envs/python3/lib/python3.8/site-packages (from pandas->datasets) (2021.3)\n",
      "Requirement already satisfied: six>=1.5 in /home/ec2-user/anaconda3/envs/python3/lib/python3.8/site-packages (from python-dateutil>=2.7.3->pandas->datasets) (1.16.0)\n"
     ]
    }
   ],
   "source": [
    "!pip install datasets "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ec104d7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I0722 08:09:27.267873 139690818238272 utils.py:157] NumExpr defaulting to 8 threads.\n",
      "I0722 08:09:27.468686 139690818238272 file_utils.py:39] PyTorch version 1.12.0+cu102 available.\n",
      "W0722 08:09:33.643760 139690818238272 builder.py:463] Using custom data configuration default-2e5c898ace65ed7d\n",
      "W0722 08:09:33.645892 139690818238272 builder.py:641] Reusing dataset csv (/home/ec2-user/.cache/huggingface/datasets/csv/default-2e5c898ace65ed7d/0.0.0/51cce309a08df9c4d82ffd9363bbe090bf173197fc01a71b034e8594995a1a58)\n",
      "100%|████████████████████████████████████████████| 3/3 [00:00<00:00, 922.50it/s]\n",
      "Namespace(adafactor=False, attention_dropout=0.1, attention_mode='sliding_chunks', attention_window=512, batch_size=1, dataset_name='test', debug=False, disable_checkpointing=False, epochs=1, fp32=False, from_pretrained=None, gpus=-1, grad_accum=1, grad_ckpt=False, label_smoothing=0.0, lr=3e-05, max_input_len=4000, max_output_len=256, model_path='longformer-encdec-base-16384', no_progress_bar=False, num_workers=0, resume_ckpt=None, save_dir='summarization', save_prefix='test', seed=1234, test=False, test_file='stary/model_b_data/test.csv', tokenizer='longformer-encdec-base-16384', train_file='stary/model_b_data/train.csv', val_every=1.0, val_percent_check=1.0, validation_file='stary/model_b_data/test.csv', warmup=1000)\n",
      "GPU available: True, used: True\n",
      "I0722 08:09:33.650579 139690818238272 distributed.py:29] GPU available: True, used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "I0722 08:09:33.650701 139690818238272 distributed.py:29] TPU available: False, using: 0 TPU cores\n",
      "CUDA_VISIBLE_DEVICES: [0]\n",
      "I0722 08:09:33.650866 139690818238272 distributed.py:29] CUDA_VISIBLE_DEVICES: [0]\n",
      "Using native 16bit precision.\n",
      "I0722 08:09:33.676346 139690818238272 auto_mix_precision.py:21] Using native 16bit precision.\n",
      "\n",
      "  | Name  | Type                                             | Params\n",
      "---------------------------------------------------------------------------\n",
      "0 | model | LongformerEncoderDecoderForConditionalGeneration | 161 M \n",
      "I0722 08:09:37.007698 139690818238272 lightning.py:1495] \n",
      "  | Name  | Type                                             | Params\n",
      "---------------------------------------------------------------------------\n",
      "0 | model | LongformerEncoderDecoderForConditionalGeneration | 161 M \n",
      "/home/ec2-user/anaconda3/envs/python3/lib/python3.8/site-packages/pytorch_lightning/utilities/distributed.py:25: UserWarning: Your val_dataloader has `shuffle=True`, it is best practice to turn this off for validation and test dataloaders.\n",
      "  warnings.warn(*args, **kwargs)\n",
      "/home/ec2-user/anaconda3/envs/python3/lib/python3.8/site-packages/pytorch_lightning/utilities/distributed.py:25: UserWarning: The dataloader, val dataloader 0, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 8 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  warnings.warn(*args, **kwargs)\n",
      "Validation sanity check: 100%|████████████████████| 2/2 [00:03<00:00,  1.83s/it]{'vloss': tensor(11.2350, device='cuda:0'), 'rouge1': tensor(0.0468, device='cuda:0'), 'rouge2': tensor(0., device='cuda:0'), 'rougeL': tensor(0.0234, device='cuda:0'), 'rougeLsum': tensor(0.0234, device='cuda:0'), 'acc': tensor(0., device='cuda:0')}\n",
      "/home/ec2-user/anaconda3/envs/python3/lib/python3.8/site-packages/pytorch_lightning/utilities/distributed.py:25: UserWarning: The dataloader, train dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 8 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  warnings.warn(*args, **kwargs)\n",
      "Epoch 1:   0%|            | 1/3195 [00:00<08:32,  6.24it/s, loss=9.068, v_num=0]/home/ec2-user/anaconda3/envs/python3/lib/python3.8/site-packages/torch/optim/lr_scheduler.py:131: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate\n",
      "  warnings.warn(\"Detected call of `lr_scheduler.step()` before `optimizer.step()`. \"\n",
      "Epoch 1:  78%|███████  | 2508/3195 [07:40<02:06,  5.44it/s, loss=0.334, v_num=0]\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 1:  79%|███████  | 2509/3195 [07:41<02:06,  5.44it/s, loss=0.334, v_num=0]\u001b[A\n",
      "Epoch 1:  79%|███████  | 2510/3195 [07:41<02:06,  5.44it/s, loss=0.334, v_num=0]\u001b[A\n",
      "Epoch 1:  79%|███████  | 2511/3195 [07:42<02:05,  5.43it/s, loss=0.334, v_num=0]\u001b[A\n",
      "Epoch 1:  79%|███████  | 2512/3195 [07:42<02:05,  5.43it/s, loss=0.334, v_num=0]\u001b[A\n",
      "Epoch 1:  79%|███████  | 2513/3195 [07:43<02:05,  5.42it/s, loss=0.334, v_num=0]\u001b[A\n",
      "Epoch 1:  79%|███████  | 2514/3195 [07:43<02:05,  5.42it/s, loss=0.334, v_num=0]\u001b[A\n",
      "Epoch 1:  79%|███████  | 2515/3195 [07:44<02:05,  5.42it/s, loss=0.334, v_num=0]\u001b[A\n",
      "Epoch 1:  79%|███████  | 2516/3195 [07:44<02:05,  5.41it/s, loss=0.334, v_num=0]\u001b[A\n",
      "Epoch 1:  79%|███████  | 2517/3195 [07:45<02:05,  5.41it/s, loss=0.334, v_num=0]\u001b[A\n",
      "Epoch 1:  79%|███████  | 2518/3195 [07:45<02:05,  5.41it/s, loss=0.334, v_num=0]\u001b[A\n",
      "Epoch 1:  79%|███████  | 2519/3195 [07:46<02:05,  5.40it/s, loss=0.334, v_num=0]\u001b[A\n",
      "Epoch 1:  79%|███████  | 2520/3195 [07:46<02:05,  5.40it/s, loss=0.334, v_num=0]\u001b[A\n",
      "Epoch 1:  79%|███████  | 2521/3195 [07:47<02:04,  5.39it/s, loss=0.334, v_num=0]\u001b[A\n",
      "Epoch 1:  79%|███████  | 2522/3195 [07:47<02:04,  5.39it/s, loss=0.334, v_num=0]\u001b[A\n",
      "Epoch 1:  79%|███████  | 2523/3195 [07:48<02:04,  5.39it/s, loss=0.334, v_num=0]\u001b[A\n",
      "Epoch 1:  79%|███████  | 2524/3195 [07:48<02:04,  5.38it/s, loss=0.334, v_num=0]\u001b[A\n",
      "Epoch 1:  79%|███████  | 2525/3195 [07:49<02:04,  5.38it/s, loss=0.334, v_num=0]\u001b[A\n",
      "Epoch 1:  79%|███████  | 2526/3195 [07:49<02:04,  5.38it/s, loss=0.334, v_num=0]\u001b[A\n",
      "Epoch 1:  79%|███████  | 2527/3195 [07:50<02:04,  5.37it/s, loss=0.334, v_num=0]\u001b[A\n",
      "Epoch 1:  79%|███████  | 2528/3195 [07:50<02:04,  5.37it/s, loss=0.334, v_num=0]\u001b[A\n",
      "Epoch 1:  79%|███████  | 2529/3195 [07:51<02:04,  5.36it/s, loss=0.334, v_num=0]\u001b[A\n",
      "Epoch 1:  79%|███████▏ | 2530/3195 [07:51<02:04,  5.36it/s, loss=0.334, v_num=0]\u001b[A\n",
      "Epoch 1:  79%|███████▏ | 2531/3195 [07:52<02:03,  5.36it/s, loss=0.334, v_num=0]\u001b[A\n",
      "Epoch 1:  79%|███████▏ | 2532/3195 [07:53<02:03,  5.35it/s, loss=0.334, v_num=0]\u001b[A\n",
      "Epoch 1:  79%|███████▏ | 2533/3195 [07:53<02:03,  5.35it/s, loss=0.334, v_num=0]\u001b[A\n",
      "Epoch 1:  79%|███████▏ | 2534/3195 [07:54<02:03,  5.35it/s, loss=0.334, v_num=0]\u001b[A\n",
      "Epoch 1:  79%|███████▏ | 2535/3195 [07:54<02:03,  5.34it/s, loss=0.334, v_num=0]\u001b[A\n",
      "Epoch 1:  79%|███████▏ | 2536/3195 [07:55<02:03,  5.34it/s, loss=0.334, v_num=0]\u001b[A\n",
      "Epoch 1:  79%|███████▏ | 2537/3195 [07:55<02:03,  5.33it/s, loss=0.334, v_num=0]\u001b[A\n",
      "Epoch 1:  79%|███████▏ | 2538/3195 [07:56<02:03,  5.33it/s, loss=0.334, v_num=0]\u001b[A\n",
      "Epoch 1:  79%|███████▏ | 2539/3195 [07:56<02:03,  5.33it/s, loss=0.334, v_num=0]\u001b[A\n",
      "Epoch 1:  79%|███████▏ | 2540/3195 [07:57<02:03,  5.32it/s, loss=0.334, v_num=0]\u001b[A\n",
      "Epoch 1:  80%|███████▏ | 2541/3195 [07:57<02:02,  5.32it/s, loss=0.334, v_num=0]\u001b[A\n",
      "Epoch 1:  80%|███████▏ | 2542/3195 [07:58<02:02,  5.32it/s, loss=0.334, v_num=0]\u001b[A\n",
      "Epoch 1:  80%|███████▏ | 2543/3195 [07:58<02:02,  5.31it/s, loss=0.334, v_num=0]\u001b[A\n",
      "Epoch 1:  80%|███████▏ | 2544/3195 [07:59<02:02,  5.31it/s, loss=0.334, v_num=0]\u001b[A\n",
      "Epoch 1:  80%|███████▏ | 2545/3195 [07:59<02:02,  5.31it/s, loss=0.334, v_num=0]\u001b[A\n",
      "Epoch 1:  80%|███████▏ | 2546/3195 [08:00<02:02,  5.30it/s, loss=0.334, v_num=0]\u001b[A\n",
      "Epoch 1:  80%|███████▏ | 2547/3195 [08:00<02:02,  5.30it/s, loss=0.334, v_num=0]\u001b[A\n",
      "Epoch 1:  80%|███████▏ | 2548/3195 [08:01<02:02,  5.29it/s, loss=0.334, v_num=0]\u001b[A\n",
      "Epoch 1:  80%|███████▏ | 2549/3195 [08:01<02:02,  5.29it/s, loss=0.334, v_num=0]\u001b[A\n",
      "Epoch 1:  80%|███████▏ | 2550/3195 [08:02<02:01,  5.29it/s, loss=0.334, v_num=0]\u001b[A\n",
      "Epoch 1:  80%|███████▏ | 2551/3195 [08:02<02:01,  5.28it/s, loss=0.334, v_num=0]\u001b[A\n",
      "Epoch 1:  80%|███████▏ | 2552/3195 [08:03<02:01,  5.28it/s, loss=0.334, v_num=0]\u001b[A\n",
      "Epoch 1:  80%|███████▏ | 2553/3195 [08:03<02:01,  5.28it/s, loss=0.334, v_num=0]\u001b[A\n",
      "Epoch 1:  80%|███████▏ | 2554/3195 [08:04<02:01,  5.27it/s, loss=0.334, v_num=0]\u001b[A\n",
      "Epoch 1:  80%|███████▏ | 2555/3195 [08:04<02:01,  5.27it/s, loss=0.334, v_num=0]\u001b[A\n",
      "Epoch 1:  80%|███████▏ | 2556/3195 [08:05<02:01,  5.27it/s, loss=0.334, v_num=0]\u001b[A\n",
      "Epoch 1:  80%|███████▏ | 2557/3195 [08:05<02:01,  5.26it/s, loss=0.334, v_num=0]\u001b[A\n",
      "Epoch 1:  80%|███████▏ | 2558/3195 [08:06<02:01,  5.26it/s, loss=0.334, v_num=0]\u001b[A\n",
      "Epoch 1:  80%|███████▏ | 2559/3195 [08:06<02:01,  5.25it/s, loss=0.334, v_num=0]\u001b[A\n",
      "Epoch 1:  80%|███████▏ | 2560/3195 [08:07<02:00,  5.25it/s, loss=0.334, v_num=0]\u001b[A\n",
      "Epoch 1:  80%|███████▏ | 2561/3195 [08:07<02:00,  5.25it/s, loss=0.334, v_num=0]\u001b[A\n",
      "Epoch 1:  80%|███████▏ | 2562/3195 [08:08<02:00,  5.24it/s, loss=0.334, v_num=0]\u001b[A\n",
      "Epoch 1:  80%|███████▏ | 2563/3195 [08:09<02:00,  5.24it/s, loss=0.334, v_num=0]\u001b[A\n",
      "Epoch 1:  80%|███████▏ | 2564/3195 [08:09<02:00,  5.24it/s, loss=0.334, v_num=0]\u001b[A\n",
      "Epoch 1:  80%|███████▏ | 2565/3195 [08:10<02:00,  5.23it/s, loss=0.334, v_num=0]\u001b[A\n",
      "Epoch 1:  80%|███████▏ | 2566/3195 [08:10<02:00,  5.23it/s, loss=0.334, v_num=0]\u001b[A\n",
      "Epoch 1:  80%|███████▏ | 2567/3195 [08:11<02:00,  5.23it/s, loss=0.334, v_num=0]\u001b[A\n",
      "Epoch 1:  80%|███████▏ | 2568/3195 [08:11<02:00,  5.22it/s, loss=0.334, v_num=0]\u001b[A\n",
      "Epoch 1:  80%|███████▏ | 2569/3195 [08:12<01:59,  5.22it/s, loss=0.334, v_num=0]\u001b[A\n",
      "Epoch 1:  80%|███████▏ | 2570/3195 [08:12<01:59,  5.22it/s, loss=0.334, v_num=0]\u001b[A\n",
      "Epoch 1:  80%|███████▏ | 2571/3195 [08:13<01:59,  5.21it/s, loss=0.334, v_num=0]\u001b[A\n",
      "Epoch 1:  81%|███████▏ | 2572/3195 [08:13<01:59,  5.21it/s, loss=0.334, v_num=0]\u001b[A\n",
      "Epoch 1:  81%|███████▏ | 2573/3195 [08:14<01:59,  5.21it/s, loss=0.334, v_num=0]\u001b[A\n",
      "Epoch 1:  81%|███████▎ | 2574/3195 [08:14<01:59,  5.20it/s, loss=0.334, v_num=0]\u001b[A\n",
      "Epoch 1:  81%|███████▎ | 2575/3195 [08:15<01:59,  5.20it/s, loss=0.334, v_num=0]\u001b[A\n",
      "Epoch 1:  81%|███████▎ | 2576/3195 [08:15<01:59,  5.20it/s, loss=0.334, v_num=0]\u001b[A\n",
      "Epoch 1:  81%|███████▎ | 2577/3195 [08:16<01:58,  5.19it/s, loss=0.334, v_num=0]\u001b[A\n",
      "Epoch 1:  81%|███████▎ | 2578/3195 [08:16<01:58,  5.19it/s, loss=0.334, v_num=0]\u001b[A\n",
      "Epoch 1:  81%|███████▎ | 2579/3195 [08:17<01:58,  5.19it/s, loss=0.334, v_num=0]\u001b[A\n",
      "Epoch 1:  81%|███████▎ | 2580/3195 [08:17<01:58,  5.18it/s, loss=0.334, v_num=0]\u001b[A\n",
      "Epoch 1:  81%|███████▎ | 2581/3195 [08:18<01:58,  5.18it/s, loss=0.334, v_num=0]\u001b[A\n",
      "Epoch 1:  81%|███████▎ | 2582/3195 [08:18<01:58,  5.18it/s, loss=0.334, v_num=0]\u001b[A\n",
      "Epoch 1:  81%|███████▎ | 2583/3195 [08:19<01:58,  5.17it/s, loss=0.334, v_num=0]\u001b[A\n",
      "Epoch 1:  81%|███████▎ | 2584/3195 [08:19<01:58,  5.17it/s, loss=0.334, v_num=0]\u001b[A\n",
      "Epoch 1:  81%|███████▎ | 2585/3195 [08:20<01:58,  5.17it/s, loss=0.334, v_num=0]\u001b[A\n",
      "Epoch 1:  81%|███████▎ | 2586/3195 [08:20<01:57,  5.16it/s, loss=0.334, v_num=0]\u001b[A\n",
      "Epoch 1:  81%|███████▎ | 2587/3195 [08:21<01:57,  5.16it/s, loss=0.334, v_num=0]\u001b[A\n",
      "Epoch 1:  81%|███████▎ | 2588/3195 [08:21<01:57,  5.16it/s, loss=0.334, v_num=0]\u001b[A\n",
      "Epoch 1:  81%|███████▎ | 2589/3195 [08:22<01:57,  5.15it/s, loss=0.334, v_num=0]\u001b[A\n",
      "Epoch 1:  81%|███████▎ | 2590/3195 [08:22<01:57,  5.15it/s, loss=0.334, v_num=0]\u001b[A\n",
      "Epoch 1:  81%|███████▎ | 2591/3195 [08:23<01:57,  5.15it/s, loss=0.334, v_num=0]\u001b[A\n",
      "Epoch 1:  81%|███████▎ | 2592/3195 [08:24<01:57,  5.14it/s, loss=0.334, v_num=0]\u001b[A\n",
      "Epoch 1:  81%|███████▎ | 2593/3195 [08:24<01:57,  5.14it/s, loss=0.334, v_num=0]\u001b[A\n",
      "Epoch 1:  81%|███████▎ | 2594/3195 [08:25<01:57,  5.14it/s, loss=0.334, v_num=0]\u001b[A\n",
      "Epoch 1:  81%|███████▎ | 2595/3195 [08:25<01:56,  5.13it/s, loss=0.334, v_num=0]\u001b[A\n",
      "Epoch 1:  81%|███████▎ | 2596/3195 [08:26<01:56,  5.13it/s, loss=0.334, v_num=0]\u001b[A\n",
      "Epoch 1:  81%|███████▎ | 2597/3195 [08:26<01:56,  5.13it/s, loss=0.334, v_num=0]\u001b[A\n",
      "Epoch 1:  81%|███████▎ | 2598/3195 [08:27<01:56,  5.12it/s, loss=0.334, v_num=0]\u001b[A\n",
      "Epoch 1:  81%|███████▎ | 2599/3195 [08:27<01:56,  5.12it/s, loss=0.334, v_num=0]\u001b[A\n",
      "Epoch 1:  81%|███████▎ | 2600/3195 [08:28<01:56,  5.12it/s, loss=0.334, v_num=0]\u001b[A\n",
      "Epoch 1:  81%|███████▎ | 2601/3195 [08:28<01:56,  5.11it/s, loss=0.334, v_num=0]\u001b[A\n",
      "Epoch 1:  81%|███████▎ | 2602/3195 [08:29<01:56,  5.11it/s, loss=0.334, v_num=0]\u001b[A\n",
      "Epoch 1:  81%|███████▎ | 2603/3195 [08:29<01:55,  5.11it/s, loss=0.334, v_num=0]\u001b[A\n",
      "Epoch 1:  82%|███████▎ | 2604/3195 [08:30<01:55,  5.10it/s, loss=0.334, v_num=0]\u001b[A\n",
      "Epoch 1:  82%|███████▎ | 2605/3195 [08:30<01:55,  5.10it/s, loss=0.334, v_num=0]\u001b[A\n",
      "Epoch 1:  82%|███████▎ | 2606/3195 [08:31<01:55,  5.10it/s, loss=0.334, v_num=0]\u001b[A\n",
      "Epoch 1:  82%|███████▎ | 2607/3195 [08:31<01:55,  5.09it/s, loss=0.334, v_num=0]\u001b[A\n",
      "Epoch 1:  82%|███████▎ | 2608/3195 [08:32<01:55,  5.09it/s, loss=0.334, v_num=0]\u001b[A\n",
      "Epoch 1:  82%|███████▎ | 2609/3195 [08:32<01:55,  5.09it/s, loss=0.334, v_num=0]\u001b[A\n",
      "Epoch 1:  82%|███████▎ | 2610/3195 [08:33<01:55,  5.08it/s, loss=0.334, v_num=0]\u001b[A\n",
      "Epoch 1:  82%|███████▎ | 2611/3195 [08:33<01:54,  5.08it/s, loss=0.334, v_num=0]\u001b[A\n",
      "Epoch 1:  82%|███████▎ | 2612/3195 [08:34<01:54,  5.08it/s, loss=0.334, v_num=0]\u001b[A\n",
      "Epoch 1:  82%|███████▎ | 2613/3195 [08:35<01:54,  5.07it/s, loss=0.334, v_num=0]\u001b[A\n",
      "Epoch 1:  82%|███████▎ | 2614/3195 [08:35<01:54,  5.07it/s, loss=0.334, v_num=0]\u001b[A\n",
      "Epoch 1:  82%|███████▎ | 2615/3195 [08:36<01:54,  5.07it/s, loss=0.334, v_num=0]\u001b[A\n",
      "Epoch 1:  82%|███████▎ | 2616/3195 [08:36<01:54,  5.06it/s, loss=0.334, v_num=0]\u001b[A\n",
      "Epoch 1:  82%|███████▎ | 2617/3195 [08:37<01:54,  5.06it/s, loss=0.334, v_num=0]\u001b[A\n",
      "Epoch 1:  82%|███████▎ | 2618/3195 [08:37<01:54,  5.06it/s, loss=0.334, v_num=0]\u001b[A\n",
      "Epoch 1:  82%|███████▍ | 2619/3195 [08:38<01:53,  5.05it/s, loss=0.334, v_num=0]\u001b[A\n",
      "Epoch 1:  82%|███████▍ | 2620/3195 [08:38<01:53,  5.05it/s, loss=0.334, v_num=0]\u001b[A\n",
      "Epoch 1:  82%|███████▍ | 2621/3195 [08:39<01:53,  5.05it/s, loss=0.334, v_num=0]\u001b[A\n",
      "Epoch 1:  82%|███████▍ | 2622/3195 [08:39<01:53,  5.05it/s, loss=0.334, v_num=0]\u001b[A\n",
      "Epoch 1:  82%|███████▍ | 2623/3195 [08:40<01:53,  5.04it/s, loss=0.334, v_num=0]\u001b[A\n",
      "Epoch 1:  82%|███████▍ | 2624/3195 [08:40<01:53,  5.04it/s, loss=0.334, v_num=0]\u001b[A\n",
      "Epoch 1:  82%|███████▍ | 2625/3195 [08:41<01:53,  5.04it/s, loss=0.334, v_num=0]\u001b[A\n",
      "Epoch 1:  82%|███████▍ | 2626/3195 [08:41<01:53,  5.03it/s, loss=0.334, v_num=0]\u001b[A\n",
      "Epoch 1:  82%|███████▍ | 2627/3195 [08:42<01:52,  5.03it/s, loss=0.334, v_num=0]\u001b[A\n",
      "Epoch 1:  82%|███████▍ | 2628/3195 [08:42<01:52,  5.03it/s, loss=0.334, v_num=0]\u001b[A\n",
      "Epoch 1:  82%|███████▍ | 2629/3195 [08:43<01:52,  5.02it/s, loss=0.334, v_num=0]\u001b[A\n",
      "Epoch 1:  82%|███████▍ | 2630/3195 [08:43<01:52,  5.02it/s, loss=0.334, v_num=0]\u001b[A\n",
      "Epoch 1:  82%|███████▍ | 2631/3195 [08:44<01:52,  5.02it/s, loss=0.334, v_num=0]\u001b[A\n",
      "Epoch 1:  82%|███████▍ | 2632/3195 [08:44<01:52,  5.01it/s, loss=0.334, v_num=0]\u001b[A\n",
      "Epoch 1:  82%|███████▍ | 2633/3195 [08:45<01:52,  5.01it/s, loss=0.334, v_num=0]\u001b[A\n",
      "Epoch 1:  82%|███████▍ | 2634/3195 [08:45<01:52,  5.01it/s, loss=0.334, v_num=0]\u001b[A\n",
      "Epoch 1:  82%|███████▍ | 2635/3195 [08:46<01:51,  5.01it/s, loss=0.334, v_num=0]\u001b[A\n",
      "Epoch 1:  83%|███████▍ | 2636/3195 [08:46<01:51,  5.00it/s, loss=0.334, v_num=0]\u001b[A\n",
      "Epoch 1:  83%|███████▍ | 2637/3195 [08:47<01:51,  5.00it/s, loss=0.334, v_num=0]\u001b[A\n",
      "Epoch 1:  83%|███████▍ | 2638/3195 [08:47<01:51,  5.00it/s, loss=0.334, v_num=0]\u001b[A\n",
      "Epoch 1:  83%|███████▍ | 2639/3195 [08:48<01:51,  4.99it/s, loss=0.334, v_num=0]\u001b[A\n",
      "Epoch 1:  83%|███████▍ | 2640/3195 [08:48<01:51,  4.99it/s, loss=0.334, v_num=0]\u001b[A\n",
      "Epoch 1:  83%|███████▍ | 2641/3195 [08:49<01:51,  4.99it/s, loss=0.334, v_num=0]\u001b[A\n",
      "Epoch 1:  83%|███████▍ | 2642/3195 [08:49<01:50,  4.99it/s, loss=0.334, v_num=0]\u001b[A\n",
      "Epoch 1:  83%|███████▍ | 2643/3195 [08:50<01:50,  4.98it/s, loss=0.334, v_num=0]\u001b[A\n",
      "Epoch 1:  83%|███████▍ | 2644/3195 [08:51<01:50,  4.98it/s, loss=0.334, v_num=0]\u001b[A\n",
      "Epoch 1:  83%|███████▍ | 2645/3195 [08:51<01:50,  4.98it/s, loss=0.334, v_num=0]\u001b[A\n",
      "Epoch 1:  83%|███████▍ | 2646/3195 [08:52<01:50,  4.97it/s, loss=0.334, v_num=0]\u001b[A\n",
      "Epoch 1:  83%|███████▍ | 2647/3195 [08:52<01:50,  4.97it/s, loss=0.334, v_num=0]\u001b[A\n",
      "Epoch 1:  83%|███████▍ | 2648/3195 [08:53<01:50,  4.97it/s, loss=0.334, v_num=0]\u001b[A\n",
      "Epoch 1:  83%|███████▍ | 2649/3195 [08:53<01:49,  4.96it/s, loss=0.334, v_num=0]\u001b[A\n",
      "Epoch 1:  83%|███████▍ | 2650/3195 [08:54<01:49,  4.96it/s, loss=0.334, v_num=0]\u001b[A\n",
      "Epoch 1:  83%|███████▍ | 2651/3195 [08:54<01:49,  4.96it/s, loss=0.334, v_num=0]\u001b[A\n",
      "Epoch 1:  83%|███████▍ | 2652/3195 [08:55<01:49,  4.96it/s, loss=0.334, v_num=0]\u001b[A\n",
      "Epoch 1:  83%|███████▍ | 2653/3195 [08:55<01:49,  4.95it/s, loss=0.334, v_num=0]\u001b[A\n",
      "Epoch 1:  83%|███████▍ | 2654/3195 [08:56<01:49,  4.95it/s, loss=0.334, v_num=0]\u001b[A\n",
      "Epoch 1:  83%|███████▍ | 2655/3195 [08:56<01:49,  4.95it/s, loss=0.334, v_num=0]\u001b[A\n",
      "Epoch 1:  83%|███████▍ | 2656/3195 [08:57<01:49,  4.94it/s, loss=0.334, v_num=0]\u001b[A\n",
      "Epoch 1:  83%|███████▍ | 2657/3195 [08:57<01:48,  4.94it/s, loss=0.334, v_num=0]\u001b[A\n",
      "Epoch 1:  83%|███████▍ | 2658/3195 [08:58<01:48,  4.94it/s, loss=0.334, v_num=0]\u001b[A\n",
      "Epoch 1:  83%|███████▍ | 2659/3195 [08:58<01:48,  4.94it/s, loss=0.334, v_num=0]\u001b[A\n",
      "Epoch 1:  83%|███████▍ | 2660/3195 [08:59<01:48,  4.93it/s, loss=0.334, v_num=0]\u001b[A\n",
      "Epoch 1:  83%|███████▍ | 2661/3195 [08:59<01:48,  4.93it/s, loss=0.334, v_num=0]\u001b[A\n",
      "Epoch 1:  83%|███████▍ | 2662/3195 [09:00<01:48,  4.93it/s, loss=0.334, v_num=0]\u001b[A\n",
      "Epoch 1:  83%|███████▌ | 2663/3195 [09:00<01:48,  4.92it/s, loss=0.334, v_num=0]\u001b[A\n",
      "Epoch 1:  83%|███████▌ | 2664/3195 [09:01<01:47,  4.92it/s, loss=0.334, v_num=0]\u001b[A\n",
      "Epoch 1:  83%|███████▌ | 2665/3195 [09:01<01:47,  4.92it/s, loss=0.334, v_num=0]\u001b[A\n",
      "Epoch 1:  83%|███████▌ | 2666/3195 [09:02<01:47,  4.92it/s, loss=0.334, v_num=0]\u001b[A\n",
      "Epoch 1:  83%|███████▌ | 2667/3195 [09:02<01:47,  4.91it/s, loss=0.334, v_num=0]\u001b[A\n",
      "Epoch 1:  84%|███████▌ | 2668/3195 [09:03<01:47,  4.91it/s, loss=0.334, v_num=0]\u001b[A\n",
      "Epoch 1:  84%|███████▌ | 2669/3195 [09:03<01:47,  4.91it/s, loss=0.334, v_num=0]\u001b[A\n",
      "Epoch 1:  84%|███████▌ | 2670/3195 [09:04<01:47,  4.90it/s, loss=0.334, v_num=0]\u001b[A\n",
      "Epoch 1:  84%|███████▌ | 2671/3195 [09:04<01:46,  4.90it/s, loss=0.334, v_num=0]\u001b[A\n",
      "Epoch 1:  84%|███████▌ | 2672/3195 [09:05<01:46,  4.90it/s, loss=0.334, v_num=0]\u001b[A\n",
      "Epoch 1:  84%|███████▌ | 2673/3195 [09:06<01:46,  4.90it/s, loss=0.334, v_num=0]\u001b[A\n",
      "Epoch 1:  84%|███████▌ | 2674/3195 [09:06<01:46,  4.89it/s, loss=0.334, v_num=0]\u001b[A\n",
      "Epoch 1:  84%|███████▌ | 2675/3195 [09:07<01:46,  4.89it/s, loss=0.334, v_num=0]\u001b[A\n",
      "Epoch 1:  84%|███████▌ | 2676/3195 [09:07<01:46,  4.89it/s, loss=0.334, v_num=0]\u001b[A\n",
      "Epoch 1:  84%|███████▌ | 2677/3195 [09:08<01:46,  4.88it/s, loss=0.334, v_num=0]\u001b[A\n",
      "Epoch 1:  84%|███████▌ | 2678/3195 [09:08<01:45,  4.88it/s, loss=0.334, v_num=0]\u001b[A\n",
      "Epoch 1:  84%|███████▌ | 2679/3195 [09:09<01:45,  4.88it/s, loss=0.334, v_num=0]\u001b[A\n",
      "Epoch 1:  84%|███████▌ | 2680/3195 [09:09<01:45,  4.88it/s, loss=0.334, v_num=0]\u001b[A\n",
      "Epoch 1:  84%|███████▌ | 2681/3195 [09:10<01:45,  4.87it/s, loss=0.334, v_num=0]\u001b[A\n",
      "Epoch 1:  84%|███████▌ | 2682/3195 [09:10<01:45,  4.87it/s, loss=0.334, v_num=0]\u001b[A\n",
      "Epoch 1:  84%|███████▌ | 2683/3195 [09:11<01:45,  4.87it/s, loss=0.334, v_num=0]\u001b[A\n",
      "Epoch 1:  84%|███████▌ | 2684/3195 [09:11<01:45,  4.86it/s, loss=0.334, v_num=0]\u001b[A\n",
      "Epoch 1:  84%|███████▌ | 2685/3195 [09:12<01:44,  4.86it/s, loss=0.334, v_num=0]\u001b[A\n",
      "Epoch 1:  84%|███████▌ | 2686/3195 [09:12<01:44,  4.86it/s, loss=0.334, v_num=0]\u001b[A\n",
      "Epoch 1:  84%|███████▌ | 2687/3195 [09:13<01:44,  4.86it/s, loss=0.334, v_num=0]\u001b[A\n",
      "Epoch 1:  84%|███████▌ | 2688/3195 [09:13<01:44,  4.85it/s, loss=0.334, v_num=0]\u001b[A\n",
      "Epoch 1:  84%|███████▌ | 2689/3195 [09:14<01:44,  4.85it/s, loss=0.334, v_num=0]\u001b[A\n",
      "Epoch 1:  84%|███████▌ | 2690/3195 [09:14<01:44,  4.85it/s, loss=0.334, v_num=0]\u001b[A\n",
      "Epoch 1:  84%|███████▌ | 2691/3195 [09:15<01:44,  4.84it/s, loss=0.334, v_num=0]\u001b[A\n",
      "Epoch 1:  84%|███████▌ | 2692/3195 [09:15<01:43,  4.84it/s, loss=0.334, v_num=0]\u001b[A\n",
      "Epoch 1:  84%|███████▌ | 2693/3195 [09:16<01:43,  4.84it/s, loss=0.334, v_num=0]\u001b[A\n",
      "Epoch 1:  84%|███████▌ | 2694/3195 [09:17<01:43,  4.84it/s, loss=0.334, v_num=0]\u001b[A\n",
      "Epoch 1:  84%|███████▌ | 2695/3195 [09:17<01:43,  4.83it/s, loss=0.334, v_num=0]\u001b[A\n",
      "Epoch 1:  84%|███████▌ | 2696/3195 [09:18<01:43,  4.83it/s, loss=0.334, v_num=0]\u001b[A\n",
      "Epoch 1:  84%|███████▌ | 2697/3195 [09:18<01:43,  4.83it/s, loss=0.334, v_num=0]\u001b[A\n",
      "Epoch 1:  84%|███████▌ | 2698/3195 [09:19<01:42,  4.83it/s, loss=0.334, v_num=0]\u001b[A\n",
      "Epoch 1:  84%|███████▌ | 2699/3195 [09:19<01:42,  4.82it/s, loss=0.334, v_num=0]\u001b[A\n",
      "Epoch 1:  85%|███████▌ | 2700/3195 [09:20<01:42,  4.82it/s, loss=0.334, v_num=0]\u001b[A\n",
      "Epoch 1:  85%|███████▌ | 2701/3195 [09:20<01:42,  4.82it/s, loss=0.334, v_num=0]\u001b[A\n",
      "Epoch 1:  85%|███████▌ | 2702/3195 [09:21<01:42,  4.81it/s, loss=0.334, v_num=0]\u001b[A\n",
      "Epoch 1:  85%|███████▌ | 2703/3195 [09:21<01:42,  4.81it/s, loss=0.334, v_num=0]\u001b[A\n",
      "Epoch 1:  85%|███████▌ | 2704/3195 [09:22<01:42,  4.81it/s, loss=0.334, v_num=0]\u001b[A\n",
      "Epoch 1:  85%|███████▌ | 2705/3195 [09:22<01:41,  4.81it/s, loss=0.334, v_num=0]\u001b[A\n",
      "Epoch 1:  85%|███████▌ | 2706/3195 [09:23<01:41,  4.80it/s, loss=0.334, v_num=0]\u001b[A\n",
      "Epoch 1:  85%|███████▋ | 2707/3195 [09:23<01:41,  4.80it/s, loss=0.334, v_num=0]\u001b[A\n",
      "Epoch 1:  85%|███████▋ | 2708/3195 [09:24<01:41,  4.80it/s, loss=0.334, v_num=0]\u001b[A\n",
      "Epoch 1:  85%|███████▋ | 2709/3195 [09:24<01:41,  4.80it/s, loss=0.334, v_num=0]\u001b[A\n",
      "Epoch 1:  85%|███████▋ | 2710/3195 [09:25<01:41,  4.79it/s, loss=0.334, v_num=0]\u001b[A\n",
      "Epoch 1:  85%|███████▋ | 2711/3195 [09:25<01:41,  4.79it/s, loss=0.334, v_num=0]\u001b[A\n",
      "Epoch 1:  85%|███████▋ | 2712/3195 [09:26<01:40,  4.79it/s, loss=0.334, v_num=0]\u001b[A\n",
      "Epoch 1:  85%|███████▋ | 2713/3195 [09:26<01:40,  4.79it/s, loss=0.334, v_num=0]\u001b[A\n",
      "Epoch 1:  85%|███████▋ | 2714/3195 [09:27<01:40,  4.78it/s, loss=0.334, v_num=0]\u001b[A\n",
      "Epoch 1:  85%|███████▋ | 2715/3195 [09:27<01:40,  4.78it/s, loss=0.334, v_num=0]\u001b[A\n",
      "Epoch 1:  85%|███████▋ | 2716/3195 [09:28<01:40,  4.78it/s, loss=0.334, v_num=0]\u001b[A\n",
      "Epoch 1:  85%|███████▋ | 2717/3195 [09:28<01:40,  4.78it/s, loss=0.334, v_num=0]\u001b[A\n",
      "Epoch 1:  85%|███████▋ | 2718/3195 [09:29<01:39,  4.77it/s, loss=0.334, v_num=0]\u001b[A\n",
      "Epoch 1:  85%|███████▋ | 2719/3195 [09:29<01:39,  4.77it/s, loss=0.334, v_num=0]\u001b[A\n",
      "Epoch 1:  85%|███████▋ | 2720/3195 [09:30<01:39,  4.77it/s, loss=0.334, v_num=0]\u001b[A\n",
      "Epoch 1:  85%|███████▋ | 2721/3195 [09:30<01:39,  4.77it/s, loss=0.334, v_num=0]\u001b[A\n",
      "Epoch 1:  85%|███████▋ | 2722/3195 [09:31<01:39,  4.76it/s, loss=0.334, v_num=0]\u001b[A\n",
      "Epoch 1:  85%|███████▋ | 2723/3195 [09:31<01:39,  4.76it/s, loss=0.334, v_num=0]\u001b[A\n",
      "Epoch 1:  85%|███████▋ | 2724/3195 [09:32<01:38,  4.76it/s, loss=0.334, v_num=0]\u001b[A\n",
      "Epoch 1:  85%|███████▋ | 2725/3195 [09:33<01:38,  4.76it/s, loss=0.334, v_num=0]\u001b[A\n",
      "Epoch 1:  85%|███████▋ | 2726/3195 [09:33<01:38,  4.75it/s, loss=0.334, v_num=0]\u001b[A\n",
      "Epoch 1:  85%|███████▋ | 2727/3195 [09:34<01:38,  4.75it/s, loss=0.334, v_num=0]\u001b[A\n",
      "Epoch 1:  85%|███████▋ | 2728/3195 [09:34<01:38,  4.75it/s, loss=0.334, v_num=0]\u001b[A\n",
      "Epoch 1:  85%|███████▋ | 2729/3195 [09:35<01:38,  4.75it/s, loss=0.334, v_num=0]\u001b[A\n",
      "Epoch 1:  85%|███████▋ | 2730/3195 [09:35<01:38,  4.74it/s, loss=0.334, v_num=0]\u001b[A\n",
      "Epoch 1:  85%|███████▋ | 2731/3195 [09:36<01:37,  4.74it/s, loss=0.334, v_num=0]\u001b[A\n",
      "Epoch 1:  86%|███████▋ | 2732/3195 [09:36<01:37,  4.74it/s, loss=0.334, v_num=0]\u001b[A\n",
      "Epoch 1:  86%|███████▋ | 2733/3195 [09:37<01:37,  4.74it/s, loss=0.334, v_num=0]\u001b[A\n",
      "Epoch 1:  86%|███████▋ | 2734/3195 [09:37<01:37,  4.73it/s, loss=0.334, v_num=0]\u001b[A\n",
      "Epoch 1:  86%|███████▋ | 2735/3195 [09:38<01:37,  4.73it/s, loss=0.334, v_num=0]\u001b[A\n",
      "Epoch 1:  86%|███████▋ | 2736/3195 [09:38<01:37,  4.73it/s, loss=0.334, v_num=0]\u001b[A\n",
      "Epoch 1:  86%|███████▋ | 2737/3195 [09:39<01:36,  4.73it/s, loss=0.334, v_num=0]\u001b[A\n",
      "Epoch 1:  86%|███████▋ | 2738/3195 [09:39<01:36,  4.72it/s, loss=0.334, v_num=0]\u001b[A\n",
      "Epoch 1:  86%|███████▋ | 2739/3195 [09:40<01:36,  4.72it/s, loss=0.334, v_num=0]\u001b[A\n",
      "Epoch 1:  86%|███████▋ | 2740/3195 [09:40<01:36,  4.72it/s, loss=0.334, v_num=0]\u001b[A\n",
      "Epoch 1:  86%|███████▋ | 2741/3195 [09:41<01:36,  4.72it/s, loss=0.334, v_num=0]\u001b[A\n",
      "Epoch 1:  86%|███████▋ | 2742/3195 [09:41<01:36,  4.71it/s, loss=0.334, v_num=0]\u001b[A\n",
      "Epoch 1:  86%|███████▋ | 2743/3195 [09:42<01:35,  4.71it/s, loss=0.334, v_num=0]\u001b[A\n",
      "Epoch 1:  86%|███████▋ | 2744/3195 [09:42<01:35,  4.71it/s, loss=0.334, v_num=0]\u001b[A\n",
      "Epoch 1:  86%|███████▋ | 2745/3195 [09:43<01:35,  4.71it/s, loss=0.334, v_num=0]\u001b[A\n",
      "Epoch 1:  86%|███████▋ | 2746/3195 [09:43<01:35,  4.70it/s, loss=0.334, v_num=0]\u001b[A\n",
      "Epoch 1:  86%|███████▋ | 2747/3195 [09:44<01:35,  4.70it/s, loss=0.334, v_num=0]\u001b[A\n",
      "Epoch 1:  86%|███████▋ | 2748/3195 [09:44<01:35,  4.70it/s, loss=0.334, v_num=0]\u001b[A\n",
      "Epoch 1:  86%|███████▋ | 2749/3195 [09:45<01:34,  4.70it/s, loss=0.334, v_num=0]\u001b[A\n",
      "Epoch 1:  86%|███████▋ | 2750/3195 [09:45<01:34,  4.69it/s, loss=0.334, v_num=0]\u001b[A\n",
      "Epoch 1:  86%|███████▋ | 2751/3195 [09:46<01:34,  4.69it/s, loss=0.334, v_num=0]\u001b[A\n",
      "Epoch 1:  86%|███████▊ | 2752/3195 [09:46<01:34,  4.69it/s, loss=0.334, v_num=0]\u001b[A\n",
      "Epoch 1:  86%|███████▊ | 2753/3195 [09:47<01:34,  4.69it/s, loss=0.334, v_num=0]\u001b[A\n",
      "Epoch 1:  86%|███████▊ | 2754/3195 [09:47<01:34,  4.68it/s, loss=0.334, v_num=0]\u001b[A\n",
      "Epoch 1:  86%|███████▊ | 2755/3195 [09:48<01:33,  4.68it/s, loss=0.334, v_num=0]\u001b[A\n",
      "Epoch 1:  86%|███████▊ | 2756/3195 [09:48<01:33,  4.68it/s, loss=0.334, v_num=0]\u001b[A\n",
      "Epoch 1:  86%|███████▊ | 2757/3195 [09:49<01:33,  4.68it/s, loss=0.334, v_num=0]\u001b[A\n",
      "Epoch 1:  86%|███████▊ | 2758/3195 [09:49<01:33,  4.67it/s, loss=0.334, v_num=0]\u001b[A\n",
      "Epoch 1:  86%|███████▊ | 2759/3195 [09:50<01:33,  4.67it/s, loss=0.334, v_num=0]\u001b[A\n",
      "Epoch 1:  86%|███████▊ | 2760/3195 [09:50<01:33,  4.67it/s, loss=0.334, v_num=0]\u001b[A\n",
      "Epoch 1:  86%|███████▊ | 2761/3195 [09:51<01:32,  4.67it/s, loss=0.334, v_num=0]\u001b[A\n",
      "Epoch 1:  86%|███████▊ | 2762/3195 [09:52<01:32,  4.67it/s, loss=0.334, v_num=0]\u001b[A\n",
      "Epoch 1:  86%|███████▊ | 2763/3195 [09:52<01:32,  4.66it/s, loss=0.334, v_num=0]\u001b[A\n",
      "Epoch 1:  87%|███████▊ | 2764/3195 [09:53<01:32,  4.66it/s, loss=0.334, v_num=0]\u001b[A\n",
      "Epoch 1:  87%|███████▊ | 2765/3195 [09:53<01:32,  4.66it/s, loss=0.334, v_num=0]\u001b[A\n",
      "Epoch 1:  87%|███████▊ | 2766/3195 [09:54<01:32,  4.66it/s, loss=0.334, v_num=0]\u001b[A\n",
      "Epoch 1:  87%|███████▊ | 2767/3195 [09:54<01:31,  4.65it/s, loss=0.334, v_num=0]\u001b[A\n",
      "Epoch 1:  87%|███████▊ | 2768/3195 [09:55<01:31,  4.65it/s, loss=0.334, v_num=0]\u001b[A\n",
      "Epoch 1:  87%|███████▊ | 2769/3195 [09:55<01:31,  4.65it/s, loss=0.334, v_num=0]\u001b[A\n",
      "Epoch 1:  87%|███████▊ | 2770/3195 [09:56<01:31,  4.65it/s, loss=0.334, v_num=0]\u001b[A\n",
      "Epoch 1:  87%|███████▊ | 2771/3195 [09:56<01:31,  4.64it/s, loss=0.334, v_num=0]\u001b[A\n",
      "Epoch 1:  87%|███████▊ | 2772/3195 [09:57<01:31,  4.64it/s, loss=0.334, v_num=0]\u001b[A\n",
      "Epoch 1:  87%|███████▊ | 2773/3195 [09:57<01:30,  4.64it/s, loss=0.334, v_num=0]\u001b[A\n",
      "Epoch 1:  87%|███████▊ | 2774/3195 [09:58<01:30,  4.64it/s, loss=0.334, v_num=0]\u001b[A\n",
      "Epoch 1:  87%|███████▊ | 2775/3195 [09:58<01:30,  4.64it/s, loss=0.334, v_num=0]\u001b[A\n",
      "Epoch 1:  87%|███████▊ | 2776/3195 [09:59<01:30,  4.63it/s, loss=0.334, v_num=0]\u001b[A\n",
      "Epoch 1:  87%|███████▊ | 2777/3195 [09:59<01:30,  4.63it/s, loss=0.334, v_num=0]\u001b[A\n",
      "Epoch 1:  87%|███████▊ | 2778/3195 [10:00<01:30,  4.63it/s, loss=0.334, v_num=0]\u001b[A\n",
      "Epoch 1:  87%|███████▊ | 2779/3195 [10:00<01:29,  4.63it/s, loss=0.334, v_num=0]\u001b[A\n",
      "Epoch 1:  87%|███████▊ | 2780/3195 [10:01<01:29,  4.62it/s, loss=0.334, v_num=0]\u001b[A\n",
      "Epoch 1:  87%|███████▊ | 2781/3195 [10:01<01:29,  4.62it/s, loss=0.334, v_num=0]\u001b[A\n",
      "Epoch 1:  87%|███████▊ | 2782/3195 [10:02<01:29,  4.62it/s, loss=0.334, v_num=0]\u001b[A\n",
      "Epoch 1:  87%|███████▊ | 2783/3195 [10:02<01:29,  4.62it/s, loss=0.334, v_num=0]\u001b[A\n",
      "Epoch 1:  87%|███████▊ | 2784/3195 [10:03<01:29,  4.61it/s, loss=0.334, v_num=0]\u001b[A\n",
      "Epoch 1:  87%|███████▊ | 2785/3195 [10:03<01:28,  4.61it/s, loss=0.334, v_num=0]\u001b[A\n",
      "Epoch 1:  87%|███████▊ | 2786/3195 [10:04<01:28,  4.61it/s, loss=0.334, v_num=0]\u001b[A\n",
      "Epoch 1:  87%|███████▊ | 2787/3195 [10:04<01:28,  4.61it/s, loss=0.334, v_num=0]\u001b[A\n",
      "Epoch 1:  87%|███████▊ | 2788/3195 [10:05<01:28,  4.61it/s, loss=0.334, v_num=0]\u001b[A\n",
      "Epoch 1:  87%|███████▊ | 2789/3195 [10:05<01:28,  4.60it/s, loss=0.334, v_num=0]\u001b[A\n",
      "Epoch 1:  87%|███████▊ | 2790/3195 [10:06<01:28,  4.60it/s, loss=0.334, v_num=0]\u001b[A\n",
      "Epoch 1:  87%|███████▊ | 2791/3195 [10:06<01:27,  4.60it/s, loss=0.334, v_num=0]\u001b[A\n",
      "Epoch 1:  87%|███████▊ | 2792/3195 [10:07<01:27,  4.60it/s, loss=0.334, v_num=0]\u001b[A\n",
      "Epoch 1:  87%|███████▊ | 2793/3195 [10:08<01:27,  4.59it/s, loss=0.334, v_num=0]\u001b[A\n",
      "Epoch 1:  87%|███████▊ | 2794/3195 [10:08<01:27,  4.59it/s, loss=0.334, v_num=0]\u001b[A\n",
      "Epoch 1:  87%|███████▊ | 2795/3195 [10:09<01:27,  4.59it/s, loss=0.334, v_num=0]\u001b[A\n",
      "Epoch 1:  88%|███████▉ | 2796/3195 [10:09<01:26,  4.59it/s, loss=0.334, v_num=0]\u001b[A\n",
      "Epoch 1:  88%|███████▉ | 2797/3195 [10:10<01:26,  4.58it/s, loss=0.334, v_num=0]\u001b[A\n",
      "Epoch 1:  88%|███████▉ | 2798/3195 [10:10<01:26,  4.58it/s, loss=0.334, v_num=0]\u001b[A\n",
      "Epoch 1:  88%|███████▉ | 2799/3195 [10:11<01:26,  4.58it/s, loss=0.334, v_num=0]\u001b[A\n",
      "Epoch 1:  88%|███████▉ | 2800/3195 [10:11<01:26,  4.58it/s, loss=0.334, v_num=0]\u001b[A\n",
      "Epoch 1:  88%|███████▉ | 2801/3195 [10:12<01:26,  4.58it/s, loss=0.334, v_num=0]\u001b[A\n",
      "Epoch 1:  88%|███████▉ | 2802/3195 [10:12<01:25,  4.57it/s, loss=0.334, v_num=0]\u001b[A\n",
      "Epoch 1:  88%|███████▉ | 2803/3195 [10:13<01:25,  4.57it/s, loss=0.334, v_num=0]\u001b[A\n",
      "Epoch 1:  88%|███████▉ | 2804/3195 [10:13<01:25,  4.57it/s, loss=0.334, v_num=0]\u001b[A\n",
      "Epoch 1:  88%|███████▉ | 2805/3195 [10:14<01:25,  4.57it/s, loss=0.334, v_num=0]\u001b[A\n",
      "Epoch 1:  88%|███████▉ | 2806/3195 [10:14<01:25,  4.57it/s, loss=0.334, v_num=0]\u001b[A\n",
      "Epoch 1:  88%|███████▉ | 2807/3195 [10:15<01:25,  4.56it/s, loss=0.334, v_num=0]\u001b[A\n",
      "Epoch 1:  88%|███████▉ | 2808/3195 [10:15<01:24,  4.56it/s, loss=0.334, v_num=0]\u001b[A\n",
      "Epoch 1:  88%|███████▉ | 2809/3195 [10:16<01:24,  4.56it/s, loss=0.334, v_num=0]\u001b[A\n",
      "Epoch 1:  88%|███████▉ | 2810/3195 [10:16<01:24,  4.56it/s, loss=0.334, v_num=0]\u001b[A\n",
      "Epoch 1:  88%|███████▉ | 2811/3195 [10:17<01:24,  4.55it/s, loss=0.334, v_num=0]\u001b[A\n",
      "Epoch 1:  88%|███████▉ | 2812/3195 [10:17<01:24,  4.55it/s, loss=0.334, v_num=0]\u001b[A\n",
      "Epoch 1:  88%|███████▉ | 2813/3195 [10:18<01:23,  4.55it/s, loss=0.334, v_num=0]\u001b[A\n",
      "Epoch 1:  88%|███████▉ | 2814/3195 [10:18<01:23,  4.55it/s, loss=0.334, v_num=0]\u001b[A\n",
      "Epoch 1:  88%|███████▉ | 2815/3195 [10:19<01:23,  4.55it/s, loss=0.334, v_num=0]\u001b[A\n",
      "Epoch 1:  88%|███████▉ | 2816/3195 [10:19<01:23,  4.54it/s, loss=0.334, v_num=0]\u001b[A\n",
      "Epoch 1:  88%|███████▉ | 2817/3195 [10:20<01:23,  4.54it/s, loss=0.334, v_num=0]\u001b[A\n",
      "Epoch 1:  88%|███████▉ | 2818/3195 [10:20<01:23,  4.54it/s, loss=0.334, v_num=0]\u001b[A\n",
      "Epoch 1:  88%|███████▉ | 2819/3195 [10:21<01:22,  4.54it/s, loss=0.334, v_num=0]\u001b[A\n",
      "Epoch 1:  88%|███████▉ | 2820/3195 [10:21<01:22,  4.53it/s, loss=0.334, v_num=0]\u001b[A\n",
      "Epoch 1:  88%|███████▉ | 2821/3195 [10:22<01:22,  4.53it/s, loss=0.334, v_num=0]\u001b[A\n",
      "Epoch 1:  88%|███████▉ | 2822/3195 [10:22<01:22,  4.53it/s, loss=0.334, v_num=0]\u001b[A\n",
      "Epoch 1:  88%|███████▉ | 2823/3195 [10:23<01:22,  4.53it/s, loss=0.334, v_num=0]\u001b[A\n",
      "Epoch 1:  88%|███████▉ | 2824/3195 [10:23<01:21,  4.53it/s, loss=0.334, v_num=0]\u001b[A\n",
      "Epoch 1:  88%|███████▉ | 2825/3195 [10:24<01:21,  4.52it/s, loss=0.334, v_num=0]\u001b[A\n",
      "Epoch 1:  88%|███████▉ | 2826/3195 [10:24<01:21,  4.52it/s, loss=0.334, v_num=0]\u001b[A\n",
      "Epoch 1:  88%|███████▉ | 2827/3195 [10:25<01:21,  4.52it/s, loss=0.334, v_num=0]\u001b[A\n",
      "Epoch 1:  89%|███████▉ | 2828/3195 [10:25<01:21,  4.52it/s, loss=0.334, v_num=0]\u001b[A\n",
      "Epoch 1:  89%|███████▉ | 2829/3195 [10:26<01:21,  4.52it/s, loss=0.334, v_num=0]\u001b[A\n",
      "Epoch 1:  89%|███████▉ | 2830/3195 [10:26<01:20,  4.51it/s, loss=0.334, v_num=0]\u001b[A\n",
      "Epoch 1:  89%|███████▉ | 2831/3195 [10:27<01:20,  4.51it/s, loss=0.334, v_num=0]\u001b[A\n",
      "Epoch 1:  89%|███████▉ | 2832/3195 [10:27<01:20,  4.51it/s, loss=0.334, v_num=0]\u001b[A\n",
      "Epoch 1:  89%|███████▉ | 2833/3195 [10:28<01:20,  4.51it/s, loss=0.334, v_num=0]\u001b[A\n",
      "Epoch 1:  89%|███████▉ | 2834/3195 [10:29<01:20,  4.51it/s, loss=0.334, v_num=0]\u001b[A\n",
      "Epoch 1:  89%|███████▉ | 2835/3195 [10:29<01:19,  4.50it/s, loss=0.334, v_num=0]\u001b[A\n",
      "Epoch 1:  89%|███████▉ | 2836/3195 [10:30<01:19,  4.50it/s, loss=0.334, v_num=0]\u001b[A\n",
      "Epoch 1:  89%|███████▉ | 2837/3195 [10:30<01:19,  4.50it/s, loss=0.334, v_num=0]\u001b[A\n",
      "Epoch 1:  89%|███████▉ | 2838/3195 [10:31<01:19,  4.50it/s, loss=0.334, v_num=0]\u001b[A\n",
      "Epoch 1:  89%|███████▉ | 2839/3195 [10:31<01:19,  4.49it/s, loss=0.334, v_num=0]\u001b[A\n",
      "Epoch 1:  89%|████████ | 2840/3195 [10:32<01:19,  4.49it/s, loss=0.334, v_num=0]\u001b[A\n",
      "Epoch 1:  89%|████████ | 2841/3195 [10:32<01:18,  4.49it/s, loss=0.334, v_num=0]\u001b[A\n",
      "Epoch 1:  89%|████████ | 2842/3195 [10:33<01:18,  4.49it/s, loss=0.334, v_num=0]\u001b[A\n",
      "Epoch 1:  89%|████████ | 2843/3195 [10:33<01:18,  4.49it/s, loss=0.334, v_num=0]\u001b[A\n",
      "Epoch 1:  89%|████████ | 2844/3195 [10:34<01:18,  4.48it/s, loss=0.334, v_num=0]\u001b[A\n",
      "Epoch 1:  89%|████████ | 2845/3195 [10:34<01:18,  4.48it/s, loss=0.334, v_num=0]\u001b[A\n",
      "Epoch 1:  89%|████████ | 2846/3195 [10:35<01:17,  4.48it/s, loss=0.334, v_num=0]\u001b[A\n",
      "Epoch 1:  89%|████████ | 2847/3195 [10:35<01:17,  4.48it/s, loss=0.334, v_num=0]\u001b[A\n",
      "Epoch 1:  89%|████████ | 2848/3195 [10:36<01:17,  4.48it/s, loss=0.334, v_num=0]\u001b[A\n",
      "Epoch 1:  89%|████████ | 2849/3195 [10:36<01:17,  4.47it/s, loss=0.334, v_num=0]\u001b[A\n",
      "Epoch 1:  89%|████████ | 2850/3195 [10:37<01:17,  4.47it/s, loss=0.334, v_num=0]\u001b[A\n",
      "Epoch 1:  89%|████████ | 2851/3195 [10:37<01:16,  4.47it/s, loss=0.334, v_num=0]\u001b[A\n",
      "Epoch 1:  89%|████████ | 2852/3195 [10:38<01:16,  4.47it/s, loss=0.334, v_num=0]\u001b[A\n",
      "Epoch 1:  89%|████████ | 2853/3195 [10:38<01:16,  4.47it/s, loss=0.334, v_num=0]\u001b[A\n",
      "Epoch 1:  89%|████████ | 2854/3195 [10:39<01:16,  4.46it/s, loss=0.334, v_num=0]\u001b[A\n",
      "Epoch 1:  89%|████████ | 2855/3195 [10:39<01:16,  4.46it/s, loss=0.334, v_num=0]\u001b[A\n",
      "Epoch 1:  89%|████████ | 2856/3195 [10:40<01:16,  4.46it/s, loss=0.334, v_num=0]\u001b[A\n",
      "Epoch 1:  89%|████████ | 2857/3195 [10:40<01:15,  4.46it/s, loss=0.334, v_num=0]\u001b[A\n",
      "Epoch 1:  89%|████████ | 2858/3195 [10:41<01:15,  4.46it/s, loss=0.334, v_num=0]\u001b[A\n",
      "Epoch 1:  89%|████████ | 2859/3195 [10:41<01:15,  4.45it/s, loss=0.334, v_num=0]\u001b[A\n",
      "Epoch 1:  90%|████████ | 2860/3195 [10:42<01:15,  4.45it/s, loss=0.334, v_num=0]\u001b[A\n",
      "Epoch 1:  90%|████████ | 2861/3195 [10:42<01:15,  4.45it/s, loss=0.334, v_num=0]\u001b[A\n",
      "Epoch 1:  90%|████████ | 2862/3195 [10:43<01:14,  4.45it/s, loss=0.334, v_num=0]\u001b[A\n",
      "Epoch 1:  90%|████████ | 2863/3195 [10:43<01:14,  4.45it/s, loss=0.334, v_num=0]\u001b[A\n",
      "Epoch 1:  90%|████████ | 2864/3195 [10:44<01:14,  4.44it/s, loss=0.334, v_num=0]\u001b[A\n",
      "Epoch 1:  90%|████████ | 2865/3195 [10:44<01:14,  4.44it/s, loss=0.334, v_num=0]\u001b[A\n",
      "Epoch 1:  90%|████████ | 2866/3195 [10:45<01:14,  4.44it/s, loss=0.334, v_num=0]\u001b[A\n",
      "Epoch 1:  90%|████████ | 2867/3195 [10:45<01:13,  4.44it/s, loss=0.334, v_num=0]\u001b[A\n",
      "Epoch 1:  90%|████████ | 2868/3195 [10:46<01:13,  4.44it/s, loss=0.334, v_num=0]\u001b[A\n",
      "Epoch 1:  90%|████████ | 2869/3195 [10:46<01:13,  4.43it/s, loss=0.334, v_num=0]\u001b[A\n",
      "Epoch 1:  90%|████████ | 2870/3195 [10:47<01:13,  4.43it/s, loss=0.334, v_num=0]\u001b[A\n",
      "Epoch 1:  90%|████████ | 2871/3195 [10:48<01:13,  4.43it/s, loss=0.334, v_num=0]\u001b[A\n",
      "Epoch 1:  90%|████████ | 2872/3195 [10:48<01:12,  4.43it/s, loss=0.334, v_num=0]\u001b[A\n",
      "Epoch 1:  90%|████████ | 2873/3195 [10:49<01:12,  4.43it/s, loss=0.334, v_num=0]\u001b[A\n",
      "Epoch 1:  90%|████████ | 2874/3195 [10:49<01:12,  4.42it/s, loss=0.334, v_num=0]\u001b[A\n",
      "Epoch 1:  90%|████████ | 2875/3195 [10:50<01:12,  4.42it/s, loss=0.334, v_num=0]\u001b[A\n",
      "Epoch 1:  90%|████████ | 2876/3195 [10:50<01:12,  4.42it/s, loss=0.334, v_num=0]\u001b[A\n",
      "Epoch 1:  90%|████████ | 2877/3195 [10:51<01:11,  4.42it/s, loss=0.334, v_num=0]\u001b[A\n",
      "Epoch 1:  90%|████████ | 2878/3195 [10:51<01:11,  4.42it/s, loss=0.334, v_num=0]\u001b[A\n",
      "Epoch 1:  90%|████████ | 2879/3195 [10:52<01:11,  4.41it/s, loss=0.334, v_num=0]\u001b[A\n",
      "Epoch 1:  90%|████████ | 2880/3195 [10:52<01:11,  4.41it/s, loss=0.334, v_num=0]\u001b[A\n",
      "Epoch 1:  90%|████████ | 2881/3195 [10:53<01:11,  4.41it/s, loss=0.334, v_num=0]\u001b[A\n",
      "Epoch 1:  90%|████████ | 2882/3195 [10:53<01:10,  4.41it/s, loss=0.334, v_num=0]\u001b[A\n",
      "Epoch 1:  90%|████████ | 2883/3195 [10:54<01:10,  4.41it/s, loss=0.334, v_num=0]\u001b[A\n",
      "Epoch 1:  90%|████████ | 2884/3195 [10:54<01:10,  4.40it/s, loss=0.334, v_num=0]\u001b[A\n",
      "Epoch 1:  90%|████████▏| 2885/3195 [10:55<01:10,  4.40it/s, loss=0.334, v_num=0]\u001b[A\n",
      "Epoch 1:  90%|████████▏| 2886/3195 [10:55<01:10,  4.40it/s, loss=0.334, v_num=0]\u001b[A\n",
      "Epoch 1:  90%|████████▏| 2887/3195 [10:56<01:10,  4.40it/s, loss=0.334, v_num=0]\u001b[A\n",
      "Epoch 1:  90%|████████▏| 2888/3195 [10:56<01:09,  4.40it/s, loss=0.334, v_num=0]\u001b[A\n",
      "Epoch 1:  90%|████████▏| 2889/3195 [10:57<01:09,  4.40it/s, loss=0.334, v_num=0]\u001b[A\n",
      "Epoch 1:  90%|████████▏| 2890/3195 [10:57<01:09,  4.39it/s, loss=0.334, v_num=0]\u001b[A\n",
      "Epoch 1:  90%|████████▏| 2891/3195 [10:58<01:09,  4.39it/s, loss=0.334, v_num=0]\u001b[A\n",
      "Epoch 1:  91%|████████▏| 2892/3195 [10:58<01:09,  4.39it/s, loss=0.334, v_num=0]\u001b[A\n",
      "Epoch 1:  91%|████████▏| 2893/3195 [10:59<01:08,  4.39it/s, loss=0.334, v_num=0]\u001b[A\n",
      "Epoch 1:  91%|████████▏| 2894/3195 [10:59<01:08,  4.39it/s, loss=0.334, v_num=0]\u001b[A\n",
      "Epoch 1:  91%|████████▏| 2895/3195 [11:00<01:08,  4.38it/s, loss=0.334, v_num=0]\u001b[A\n",
      "Epoch 1:  91%|████████▏| 2896/3195 [11:00<01:08,  4.38it/s, loss=0.334, v_num=0]\u001b[A\n",
      "Epoch 1:  91%|████████▏| 2897/3195 [11:01<01:08,  4.38it/s, loss=0.334, v_num=0]\u001b[A\n",
      "Epoch 1:  91%|████████▏| 2898/3195 [11:02<01:07,  4.38it/s, loss=0.334, v_num=0]\u001b[A\n",
      "Epoch 1:  91%|████████▏| 2899/3195 [11:02<01:07,  4.38it/s, loss=0.334, v_num=0]\u001b[A\n",
      "Epoch 1:  91%|████████▏| 2900/3195 [11:03<01:07,  4.37it/s, loss=0.334, v_num=0]\u001b[A\n",
      "Epoch 1:  91%|████████▏| 2901/3195 [11:03<01:07,  4.37it/s, loss=0.334, v_num=0]\u001b[A\n",
      "Epoch 1:  91%|████████▏| 2902/3195 [11:04<01:07,  4.37it/s, loss=0.334, v_num=0]\u001b[A\n",
      "Epoch 1:  91%|████████▏| 2903/3195 [11:04<01:06,  4.37it/s, loss=0.334, v_num=0]\u001b[A\n",
      "Epoch 1:  91%|████████▏| 2904/3195 [11:05<01:06,  4.37it/s, loss=0.334, v_num=0]\u001b[A\n",
      "Epoch 1:  91%|████████▏| 2905/3195 [11:05<01:06,  4.36it/s, loss=0.334, v_num=0]\u001b[A\n",
      "Epoch 1:  91%|████████▏| 2906/3195 [11:06<01:06,  4.36it/s, loss=0.334, v_num=0]\u001b[A\n",
      "Epoch 1:  91%|████████▏| 2907/3195 [11:06<01:06,  4.36it/s, loss=0.334, v_num=0]\u001b[A\n",
      "Epoch 1:  91%|████████▏| 2908/3195 [11:07<01:05,  4.36it/s, loss=0.334, v_num=0]\u001b[A\n",
      "Epoch 1:  91%|████████▏| 2909/3195 [11:07<01:05,  4.36it/s, loss=0.334, v_num=0]\u001b[A\n",
      "Epoch 1:  91%|████████▏| 2910/3195 [11:08<01:05,  4.36it/s, loss=0.334, v_num=0]\u001b[A\n",
      "Epoch 1:  91%|████████▏| 2911/3195 [11:08<01:05,  4.35it/s, loss=0.334, v_num=0]\u001b[A\n",
      "Epoch 1:  91%|████████▏| 2912/3195 [11:09<01:05,  4.35it/s, loss=0.334, v_num=0]\u001b[A\n",
      "Epoch 1:  91%|████████▏| 2913/3195 [11:09<01:04,  4.35it/s, loss=0.334, v_num=0]\u001b[A\n",
      "Epoch 1:  91%|████████▏| 2914/3195 [11:10<01:04,  4.35it/s, loss=0.334, v_num=0]\u001b[A\n",
      "Epoch 1:  91%|████████▏| 2915/3195 [11:10<01:04,  4.35it/s, loss=0.334, v_num=0]\u001b[A\n",
      "Epoch 1:  91%|████████▏| 2916/3195 [11:11<01:04,  4.34it/s, loss=0.334, v_num=0]\u001b[A\n",
      "Epoch 1:  91%|████████▏| 2917/3195 [11:11<01:04,  4.34it/s, loss=0.334, v_num=0]\u001b[A\n",
      "Epoch 1:  91%|████████▏| 2918/3195 [11:12<01:03,  4.34it/s, loss=0.334, v_num=0]\u001b[A\n",
      "Epoch 1:  91%|████████▏| 2919/3195 [11:12<01:03,  4.34it/s, loss=0.334, v_num=0]\u001b[A\n",
      "Epoch 1:  91%|████████▏| 2920/3195 [11:13<01:03,  4.34it/s, loss=0.334, v_num=0]\u001b[A\n",
      "Epoch 1:  91%|████████▏| 2921/3195 [11:13<01:03,  4.33it/s, loss=0.334, v_num=0]\u001b[A\n",
      "Epoch 1:  91%|████████▏| 2922/3195 [11:14<01:03,  4.33it/s, loss=0.334, v_num=0]\u001b[A\n",
      "Epoch 1:  91%|████████▏| 2923/3195 [11:14<01:02,  4.33it/s, loss=0.334, v_num=0]\u001b[A\n",
      "Epoch 1:  92%|████████▏| 2924/3195 [11:15<01:02,  4.33it/s, loss=0.334, v_num=0]\u001b[A\n",
      "Epoch 1:  92%|████████▏| 2925/3195 [11:15<01:02,  4.33it/s, loss=0.334, v_num=0]\u001b[A\n",
      "Epoch 1:  92%|████████▏| 2926/3195 [11:16<01:02,  4.33it/s, loss=0.334, v_num=0]\u001b[A\n",
      "Epoch 1:  92%|████████▏| 2927/3195 [11:16<01:01,  4.32it/s, loss=0.334, v_num=0]\u001b[A\n",
      "Epoch 1:  92%|████████▏| 2928/3195 [11:17<01:01,  4.32it/s, loss=0.334, v_num=0]\u001b[A\n",
      "Epoch 1:  92%|████████▎| 2929/3195 [11:17<01:01,  4.32it/s, loss=0.334, v_num=0]\u001b[A\n",
      "Epoch 1:  92%|████████▎| 2930/3195 [11:18<01:01,  4.32it/s, loss=0.334, v_num=0]\u001b[A\n",
      "Epoch 1:  92%|████████▎| 2931/3195 [11:18<01:01,  4.32it/s, loss=0.334, v_num=0]\u001b[A\n",
      "Epoch 1:  92%|████████▎| 2932/3195 [11:19<01:00,  4.31it/s, loss=0.334, v_num=0]\u001b[A\n",
      "Epoch 1:  92%|████████▎| 2933/3195 [11:20<01:00,  4.31it/s, loss=0.334, v_num=0]\u001b[A\n",
      "Epoch 1:  92%|████████▎| 2934/3195 [11:20<01:00,  4.31it/s, loss=0.334, v_num=0]\u001b[A\n",
      "Epoch 1:  92%|████████▎| 2935/3195 [11:21<01:00,  4.31it/s, loss=0.334, v_num=0]\u001b[A\n",
      "Epoch 1:  92%|████████▎| 2936/3195 [11:21<01:00,  4.31it/s, loss=0.334, v_num=0]\u001b[A\n",
      "Epoch 1:  92%|████████▎| 2937/3195 [11:22<00:59,  4.31it/s, loss=0.334, v_num=0]\u001b[A\n",
      "Epoch 1:  92%|████████▎| 2938/3195 [11:22<00:59,  4.30it/s, loss=0.334, v_num=0]\u001b[A\n",
      "Epoch 1:  92%|████████▎| 2939/3195 [11:23<00:59,  4.30it/s, loss=0.334, v_num=0]\u001b[A\n",
      "Epoch 1:  92%|████████▎| 2940/3195 [11:23<00:59,  4.30it/s, loss=0.334, v_num=0]\u001b[A\n",
      "Epoch 1:  92%|████████▎| 2941/3195 [11:24<00:59,  4.30it/s, loss=0.334, v_num=0]\u001b[A\n",
      "Epoch 1:  92%|████████▎| 2942/3195 [11:24<00:58,  4.30it/s, loss=0.334, v_num=0]\u001b[A\n",
      "Epoch 1:  92%|████████▎| 2943/3195 [11:25<00:58,  4.30it/s, loss=0.334, v_num=0]\u001b[A\n",
      "Epoch 1:  92%|████████▎| 2944/3195 [11:25<00:58,  4.29it/s, loss=0.334, v_num=0]\u001b[A\n",
      "Epoch 1:  92%|████████▎| 2945/3195 [11:26<00:58,  4.29it/s, loss=0.334, v_num=0]\u001b[A\n",
      "Epoch 1:  92%|████████▎| 2946/3195 [11:26<00:58,  4.29it/s, loss=0.334, v_num=0]\u001b[A\n",
      "Epoch 1:  92%|████████▎| 2947/3195 [11:27<00:57,  4.29it/s, loss=0.334, v_num=0]\u001b[A\n",
      "Epoch 1:  92%|████████▎| 2948/3195 [11:27<00:57,  4.29it/s, loss=0.334, v_num=0]\u001b[A\n",
      "Epoch 1:  92%|████████▎| 2949/3195 [11:28<00:57,  4.28it/s, loss=0.334, v_num=0]\u001b[A\n",
      "Epoch 1:  92%|████████▎| 2950/3195 [11:28<00:57,  4.28it/s, loss=0.334, v_num=0]\u001b[A\n",
      "Epoch 1:  92%|████████▎| 2951/3195 [11:29<00:56,  4.28it/s, loss=0.334, v_num=0]\u001b[A\n",
      "Epoch 1:  92%|████████▎| 2952/3195 [11:29<00:56,  4.28it/s, loss=0.334, v_num=0]\u001b[A\n",
      "Epoch 1:  92%|████████▎| 2953/3195 [11:30<00:56,  4.28it/s, loss=0.334, v_num=0]\u001b[A\n",
      "Epoch 1:  92%|████████▎| 2954/3195 [11:30<00:56,  4.28it/s, loss=0.334, v_num=0]\u001b[A\n",
      "Epoch 1:  92%|████████▎| 2955/3195 [11:31<00:56,  4.27it/s, loss=0.334, v_num=0]\u001b[A\n",
      "Epoch 1:  93%|████████▎| 2956/3195 [11:31<00:55,  4.27it/s, loss=0.334, v_num=0]\u001b[A\n",
      "Epoch 1:  93%|████████▎| 2957/3195 [11:32<00:55,  4.27it/s, loss=0.334, v_num=0]\u001b[A\n",
      "Epoch 1:  93%|████████▎| 2958/3195 [11:32<00:55,  4.27it/s, loss=0.334, v_num=0]\u001b[A\n",
      "Epoch 1:  93%|████████▎| 2959/3195 [11:33<00:55,  4.27it/s, loss=0.334, v_num=0]\u001b[A\n",
      "Epoch 1:  93%|████████▎| 2960/3195 [11:33<00:55,  4.27it/s, loss=0.334, v_num=0]\u001b[A\n",
      "Epoch 1:  93%|████████▎| 2961/3195 [11:34<00:54,  4.26it/s, loss=0.334, v_num=0]\u001b[A\n",
      "Epoch 1:  93%|████████▎| 2962/3195 [11:34<00:54,  4.26it/s, loss=0.334, v_num=0]\u001b[A\n",
      "Epoch 1:  93%|████████▎| 2963/3195 [11:35<00:54,  4.26it/s, loss=0.334, v_num=0]\u001b[A\n",
      "Epoch 1:  93%|████████▎| 2964/3195 [11:35<00:54,  4.26it/s, loss=0.334, v_num=0]\u001b[A\n",
      "Epoch 1:  93%|████████▎| 2965/3195 [11:36<00:54,  4.26it/s, loss=0.334, v_num=0]\u001b[A\n",
      "Epoch 1:  93%|████████▎| 2966/3195 [11:36<00:53,  4.26it/s, loss=0.334, v_num=0]\u001b[A\n",
      "Epoch 1:  93%|████████▎| 2967/3195 [11:37<00:53,  4.25it/s, loss=0.334, v_num=0]\u001b[A\n",
      "Epoch 1:  93%|████████▎| 2968/3195 [11:38<00:53,  4.25it/s, loss=0.334, v_num=0]\u001b[A\n",
      "Epoch 1:  93%|████████▎| 2969/3195 [11:38<00:53,  4.25it/s, loss=0.334, v_num=0]\u001b[A\n",
      "Epoch 1:  93%|████████▎| 2970/3195 [11:39<00:52,  4.25it/s, loss=0.334, v_num=0]\u001b[A\n",
      "Epoch 1:  93%|████████▎| 2971/3195 [11:39<00:52,  4.25it/s, loss=0.334, v_num=0]\u001b[A\n",
      "Epoch 1:  93%|████████▎| 2972/3195 [11:40<00:52,  4.25it/s, loss=0.334, v_num=0]\u001b[A\n",
      "Epoch 1:  93%|████████▎| 2973/3195 [11:40<00:52,  4.24it/s, loss=0.334, v_num=0]\u001b[A\n",
      "Epoch 1:  93%|████████▍| 2974/3195 [11:41<00:52,  4.24it/s, loss=0.334, v_num=0]\u001b[A\n",
      "Epoch 1:  93%|████████▍| 2975/3195 [11:41<00:51,  4.24it/s, loss=0.334, v_num=0]\u001b[A\n",
      "Epoch 1:  93%|████████▍| 2976/3195 [11:42<00:51,  4.24it/s, loss=0.334, v_num=0]\u001b[A\n",
      "Epoch 1:  93%|████████▍| 2977/3195 [11:42<00:51,  4.24it/s, loss=0.334, v_num=0]\u001b[A\n",
      "Epoch 1:  93%|████████▍| 2978/3195 [11:43<00:51,  4.24it/s, loss=0.334, v_num=0]\u001b[A\n",
      "Epoch 1:  93%|████████▍| 2979/3195 [11:43<00:51,  4.23it/s, loss=0.334, v_num=0]\u001b[A\n",
      "Epoch 1:  93%|████████▍| 2980/3195 [11:44<00:50,  4.23it/s, loss=0.334, v_num=0]\u001b[A\n",
      "Epoch 1:  93%|████████▍| 2981/3195 [11:44<00:50,  4.23it/s, loss=0.334, v_num=0]\u001b[A\n",
      "Epoch 1:  93%|████████▍| 2982/3195 [11:45<00:50,  4.23it/s, loss=0.334, v_num=0]\u001b[A\n",
      "Epoch 1:  93%|████████▍| 2983/3195 [11:45<00:50,  4.23it/s, loss=0.334, v_num=0]\u001b[A\n",
      "Epoch 1:  93%|████████▍| 2984/3195 [11:46<00:49,  4.23it/s, loss=0.334, v_num=0]\u001b[A\n",
      "Epoch 1:  93%|████████▍| 2985/3195 [11:46<00:49,  4.22it/s, loss=0.334, v_num=0]\u001b[A\n",
      "Epoch 1:  93%|████████▍| 2986/3195 [11:47<00:49,  4.22it/s, loss=0.334, v_num=0]\u001b[A\n",
      "Epoch 1:  93%|████████▍| 2987/3195 [11:47<00:49,  4.22it/s, loss=0.334, v_num=0]\u001b[A\n",
      "Epoch 1:  94%|████████▍| 2988/3195 [11:48<00:49,  4.22it/s, loss=0.334, v_num=0]\u001b[A\n",
      "Epoch 1:  94%|████████▍| 2989/3195 [11:48<00:48,  4.22it/s, loss=0.334, v_num=0]\u001b[A\n",
      "Epoch 1:  94%|████████▍| 2990/3195 [11:49<00:48,  4.21it/s, loss=0.334, v_num=0]\u001b[A\n",
      "Epoch 1:  94%|████████▍| 2991/3195 [11:49<00:48,  4.21it/s, loss=0.334, v_num=0]\u001b[A\n",
      "Epoch 1:  94%|████████▍| 2992/3195 [11:50<00:48,  4.21it/s, loss=0.334, v_num=0]\u001b[A\n",
      "Epoch 1:  94%|████████▍| 2993/3195 [11:50<00:47,  4.21it/s, loss=0.334, v_num=0]\u001b[A\n",
      "Epoch 1:  94%|████████▍| 2994/3195 [11:51<00:47,  4.21it/s, loss=0.334, v_num=0]\u001b[A\n",
      "Epoch 1:  94%|████████▍| 2995/3195 [11:51<00:47,  4.21it/s, loss=0.334, v_num=0]\u001b[A\n",
      "Epoch 1:  94%|████████▍| 2996/3195 [11:52<00:47,  4.21it/s, loss=0.334, v_num=0]\u001b[A\n",
      "Epoch 1:  94%|████████▍| 2997/3195 [11:52<00:47,  4.20it/s, loss=0.334, v_num=0]\u001b[A\n",
      "Epoch 1:  94%|████████▍| 2998/3195 [11:53<00:46,  4.20it/s, loss=0.334, v_num=0]\u001b[A\n",
      "Epoch 1:  94%|████████▍| 2999/3195 [11:54<00:46,  4.20it/s, loss=0.334, v_num=0]\u001b[A\n",
      "Epoch 1:  94%|████████▍| 3000/3195 [11:54<00:46,  4.20it/s, loss=0.334, v_num=0]\u001b[A\n",
      "Epoch 1:  94%|████████▍| 3001/3195 [11:55<00:46,  4.20it/s, loss=0.334, v_num=0]\u001b[A\n",
      "Epoch 1:  94%|████████▍| 3002/3195 [11:55<00:46,  4.20it/s, loss=0.334, v_num=0]\u001b[A\n",
      "Epoch 1:  94%|████████▍| 3003/3195 [11:56<00:45,  4.19it/s, loss=0.334, v_num=0]\u001b[A\n",
      "Epoch 1:  94%|████████▍| 3004/3195 [11:56<00:45,  4.19it/s, loss=0.334, v_num=0]\u001b[A\n",
      "Epoch 1:  94%|████████▍| 3005/3195 [11:57<00:45,  4.19it/s, loss=0.334, v_num=0]\u001b[A\n",
      "Epoch 1:  94%|████████▍| 3006/3195 [11:57<00:45,  4.19it/s, loss=0.334, v_num=0]\u001b[A\n",
      "Epoch 1:  94%|████████▍| 3007/3195 [11:58<00:44,  4.19it/s, loss=0.334, v_num=0]\u001b[A\n",
      "Epoch 1:  94%|████████▍| 3008/3195 [11:58<00:44,  4.19it/s, loss=0.334, v_num=0]\u001b[A\n",
      "Epoch 1:  94%|████████▍| 3009/3195 [11:59<00:44,  4.18it/s, loss=0.334, v_num=0]\u001b[A\n",
      "Epoch 1:  94%|████████▍| 3010/3195 [11:59<00:44,  4.18it/s, loss=0.334, v_num=0]\u001b[A\n",
      "Epoch 1:  94%|████████▍| 3011/3195 [12:00<00:44,  4.18it/s, loss=0.334, v_num=0]\u001b[A\n",
      "Epoch 1:  94%|████████▍| 3012/3195 [12:00<00:43,  4.18it/s, loss=0.334, v_num=0]\u001b[A\n",
      "Epoch 1:  94%|████████▍| 3013/3195 [12:01<00:43,  4.18it/s, loss=0.334, v_num=0]\u001b[A\n",
      "Epoch 1:  94%|████████▍| 3014/3195 [12:01<00:43,  4.18it/s, loss=0.334, v_num=0]\u001b[A\n",
      "Epoch 1:  94%|████████▍| 3015/3195 [12:02<00:43,  4.17it/s, loss=0.334, v_num=0]\u001b[A\n",
      "Epoch 1:  94%|████████▍| 3016/3195 [12:02<00:42,  4.17it/s, loss=0.334, v_num=0]\u001b[A\n",
      "Epoch 1:  94%|████████▍| 3017/3195 [12:03<00:42,  4.17it/s, loss=0.334, v_num=0]\u001b[A\n",
      "Epoch 1:  94%|████████▌| 3018/3195 [12:03<00:42,  4.17it/s, loss=0.334, v_num=0]\u001b[A\n",
      "Epoch 1:  94%|████████▌| 3019/3195 [12:04<00:42,  4.17it/s, loss=0.334, v_num=0]\u001b[A\n",
      "Epoch 1:  95%|████████▌| 3020/3195 [12:04<00:42,  4.17it/s, loss=0.334, v_num=0]\u001b[A\n",
      "Epoch 1:  95%|████████▌| 3021/3195 [12:05<00:41,  4.16it/s, loss=0.334, v_num=0]\u001b[A\n",
      "Epoch 1:  95%|████████▌| 3022/3195 [12:05<00:41,  4.16it/s, loss=0.334, v_num=0]\u001b[A\n",
      "Epoch 1:  95%|████████▌| 3023/3195 [12:06<00:41,  4.16it/s, loss=0.334, v_num=0]\u001b[A\n",
      "Epoch 1:  95%|████████▌| 3024/3195 [12:06<00:41,  4.16it/s, loss=0.334, v_num=0]\u001b[A\n",
      "Epoch 1:  95%|████████▌| 3025/3195 [12:07<00:40,  4.16it/s, loss=0.334, v_num=0]\u001b[A\n",
      "Epoch 1:  95%|████████▌| 3026/3195 [12:08<00:40,  4.16it/s, loss=0.334, v_num=0]\u001b[A\n",
      "Epoch 1:  95%|████████▌| 3027/3195 [12:08<00:40,  4.15it/s, loss=0.334, v_num=0]\u001b[A\n",
      "Epoch 1:  95%|████████▌| 3028/3195 [12:09<00:40,  4.15it/s, loss=0.334, v_num=0]\u001b[A\n",
      "Epoch 1:  95%|████████▌| 3029/3195 [12:09<00:39,  4.15it/s, loss=0.334, v_num=0]\u001b[A\n",
      "Epoch 1:  95%|████████▌| 3030/3195 [12:10<00:39,  4.15it/s, loss=0.334, v_num=0]\u001b[A\n",
      "Epoch 1:  95%|████████▌| 3031/3195 [12:10<00:39,  4.15it/s, loss=0.334, v_num=0]\u001b[A\n",
      "Epoch 1:  95%|████████▌| 3032/3195 [12:11<00:39,  4.15it/s, loss=0.334, v_num=0]\u001b[A\n",
      "Epoch 1:  95%|████████▌| 3033/3195 [12:11<00:39,  4.15it/s, loss=0.334, v_num=0]\u001b[A\n",
      "Epoch 1:  95%|████████▌| 3034/3195 [12:12<00:38,  4.14it/s, loss=0.334, v_num=0]\u001b[A\n",
      "Epoch 1:  95%|████████▌| 3035/3195 [12:12<00:38,  4.14it/s, loss=0.334, v_num=0]\u001b[A\n",
      "Epoch 1:  95%|████████▌| 3036/3195 [12:13<00:38,  4.14it/s, loss=0.334, v_num=0]\u001b[A\n",
      "Epoch 1:  95%|████████▌| 3037/3195 [12:13<00:38,  4.14it/s, loss=0.334, v_num=0]\u001b[A\n",
      "Epoch 1:  95%|████████▌| 3038/3195 [12:14<00:37,  4.14it/s, loss=0.334, v_num=0]\u001b[A\n",
      "Epoch 1:  95%|████████▌| 3039/3195 [12:14<00:37,  4.14it/s, loss=0.334, v_num=0]\u001b[A\n",
      "Epoch 1:  95%|████████▌| 3040/3195 [12:15<00:37,  4.13it/s, loss=0.334, v_num=0]\u001b[A\n",
      "Epoch 1:  95%|████████▌| 3041/3195 [12:15<00:37,  4.13it/s, loss=0.334, v_num=0]\u001b[A\n",
      "Epoch 1:  95%|████████▌| 3042/3195 [12:16<00:37,  4.13it/s, loss=0.334, v_num=0]\u001b[A\n",
      "Epoch 1:  95%|████████▌| 3043/3195 [12:16<00:36,  4.13it/s, loss=0.334, v_num=0]\u001b[A\n",
      "Epoch 1:  95%|████████▌| 3044/3195 [12:17<00:36,  4.13it/s, loss=0.334, v_num=0]\u001b[A\n",
      "Epoch 1:  95%|████████▌| 3045/3195 [12:17<00:36,  4.13it/s, loss=0.334, v_num=0]\u001b[A\n",
      "Epoch 1:  95%|████████▌| 3046/3195 [12:18<00:36,  4.13it/s, loss=0.334, v_num=0]\u001b[A\n",
      "Epoch 1:  95%|████████▌| 3047/3195 [12:18<00:35,  4.12it/s, loss=0.334, v_num=0]\u001b[A\n",
      "Epoch 1:  95%|████████▌| 3048/3195 [12:19<00:35,  4.12it/s, loss=0.334, v_num=0]\u001b[A\n",
      "Epoch 1:  95%|████████▌| 3049/3195 [12:19<00:35,  4.12it/s, loss=0.334, v_num=0]\u001b[A\n",
      "Epoch 1:  95%|████████▌| 3050/3195 [12:20<00:35,  4.12it/s, loss=0.334, v_num=0]\u001b[A\n",
      "Epoch 1:  95%|████████▌| 3051/3195 [12:20<00:34,  4.12it/s, loss=0.334, v_num=0]\u001b[A\n",
      "Epoch 1:  96%|████████▌| 3052/3195 [12:21<00:34,  4.12it/s, loss=0.334, v_num=0]\u001b[A\n",
      "Epoch 1:  96%|████████▌| 3053/3195 [12:21<00:34,  4.11it/s, loss=0.334, v_num=0]\u001b[A\n",
      "Epoch 1:  96%|████████▌| 3054/3195 [12:22<00:34,  4.11it/s, loss=0.334, v_num=0]\u001b[A\n",
      "Epoch 1:  96%|████████▌| 3055/3195 [12:23<00:34,  4.11it/s, loss=0.334, v_num=0]\u001b[A\n",
      "Epoch 1:  96%|████████▌| 3056/3195 [12:23<00:33,  4.11it/s, loss=0.334, v_num=0]\u001b[A\n",
      "Epoch 1:  96%|████████▌| 3057/3195 [12:24<00:33,  4.11it/s, loss=0.334, v_num=0]\u001b[A\n",
      "Epoch 1:  96%|████████▌| 3058/3195 [12:24<00:33,  4.11it/s, loss=0.334, v_num=0]\u001b[A\n",
      "Epoch 1:  96%|████████▌| 3059/3195 [12:25<00:33,  4.11it/s, loss=0.334, v_num=0]\u001b[A\n",
      "Epoch 1:  96%|████████▌| 3060/3195 [12:25<00:32,  4.10it/s, loss=0.334, v_num=0]\u001b[A\n",
      "Epoch 1:  96%|████████▌| 3061/3195 [12:26<00:32,  4.10it/s, loss=0.334, v_num=0]\u001b[A\n",
      "Epoch 1:  96%|████████▋| 3062/3195 [12:26<00:32,  4.10it/s, loss=0.334, v_num=0]\u001b[A\n",
      "Epoch 1:  96%|████████▋| 3063/3195 [12:27<00:32,  4.10it/s, loss=0.334, v_num=0]\u001b[A\n",
      "Epoch 1:  96%|████████▋| 3064/3195 [12:27<00:31,  4.10it/s, loss=0.334, v_num=0]\u001b[A\n",
      "Epoch 1:  96%|████████▋| 3065/3195 [12:28<00:31,  4.10it/s, loss=0.334, v_num=0]\u001b[A\n",
      "Epoch 1:  96%|████████▋| 3066/3195 [12:28<00:31,  4.10it/s, loss=0.334, v_num=0]\u001b[A\n",
      "Epoch 1:  96%|████████▋| 3067/3195 [12:29<00:31,  4.09it/s, loss=0.334, v_num=0]\u001b[A\n",
      "Epoch 1:  96%|████████▋| 3068/3195 [12:29<00:31,  4.09it/s, loss=0.334, v_num=0]\u001b[A\n",
      "Epoch 1:  96%|████████▋| 3069/3195 [12:30<00:30,  4.09it/s, loss=0.334, v_num=0]\u001b[A\n",
      "Epoch 1:  96%|████████▋| 3070/3195 [12:30<00:30,  4.09it/s, loss=0.334, v_num=0]\u001b[A\n",
      "Epoch 1:  96%|████████▋| 3071/3195 [12:31<00:30,  4.09it/s, loss=0.334, v_num=0]\u001b[A\n",
      "Epoch 1:  96%|████████▋| 3072/3195 [12:31<00:30,  4.09it/s, loss=0.334, v_num=0]\u001b[A\n",
      "Epoch 1:  96%|████████▋| 3073/3195 [12:32<00:29,  4.08it/s, loss=0.334, v_num=0]\u001b[A\n",
      "Epoch 1:  96%|████████▋| 3074/3195 [12:32<00:29,  4.08it/s, loss=0.334, v_num=0]\u001b[A\n",
      "Epoch 1:  96%|████████▋| 3075/3195 [12:33<00:29,  4.08it/s, loss=0.334, v_num=0]\u001b[A\n",
      "Epoch 1:  96%|████████▋| 3076/3195 [12:33<00:29,  4.08it/s, loss=0.334, v_num=0]\u001b[A\n",
      "Epoch 1:  96%|████████▋| 3077/3195 [12:34<00:28,  4.08it/s, loss=0.334, v_num=0]\u001b[A\n",
      "Epoch 1:  96%|████████▋| 3078/3195 [12:34<00:28,  4.08it/s, loss=0.334, v_num=0]\u001b[A\n",
      "Epoch 1:  96%|████████▋| 3079/3195 [12:35<00:28,  4.08it/s, loss=0.334, v_num=0]\u001b[A\n",
      "Epoch 1:  96%|████████▋| 3080/3195 [12:35<00:28,  4.07it/s, loss=0.334, v_num=0]\u001b[A\n",
      "Epoch 1:  96%|████████▋| 3081/3195 [12:36<00:27,  4.07it/s, loss=0.334, v_num=0]\u001b[A\n",
      "Epoch 1:  96%|████████▋| 3082/3195 [12:36<00:27,  4.07it/s, loss=0.334, v_num=0]\u001b[A\n",
      "Epoch 1:  96%|████████▋| 3083/3195 [12:37<00:27,  4.07it/s, loss=0.334, v_num=0]\u001b[A\n",
      "Epoch 1:  97%|████████▋| 3084/3195 [12:37<00:27,  4.07it/s, loss=0.334, v_num=0]\u001b[A\n",
      "Epoch 1:  97%|████████▋| 3085/3195 [12:38<00:27,  4.07it/s, loss=0.334, v_num=0]\u001b[A\n",
      "Epoch 1:  97%|████████▋| 3086/3195 [12:39<00:26,  4.07it/s, loss=0.334, v_num=0]\u001b[A\n",
      "Epoch 1:  97%|████████▋| 3087/3195 [12:39<00:26,  4.06it/s, loss=0.334, v_num=0]\u001b[A\n",
      "Epoch 1:  97%|████████▋| 3088/3195 [12:40<00:26,  4.06it/s, loss=0.334, v_num=0]\u001b[A\n",
      "Epoch 1:  97%|████████▋| 3089/3195 [12:40<00:26,  4.06it/s, loss=0.334, v_num=0]\u001b[A\n",
      "Epoch 1:  97%|████████▋| 3090/3195 [12:41<00:25,  4.06it/s, loss=0.334, v_num=0]\u001b[A\n",
      "Epoch 1:  97%|████████▋| 3091/3195 [12:41<00:25,  4.06it/s, loss=0.334, v_num=0]\u001b[A\n",
      "Epoch 1:  97%|████████▋| 3092/3195 [12:42<00:25,  4.06it/s, loss=0.334, v_num=0]\u001b[A\n",
      "Epoch 1:  97%|████████▋| 3093/3195 [12:42<00:25,  4.06it/s, loss=0.334, v_num=0]\u001b[A\n",
      "Epoch 1:  97%|████████▋| 3094/3195 [12:43<00:24,  4.05it/s, loss=0.334, v_num=0]\u001b[A\n",
      "Epoch 1:  97%|████████▋| 3095/3195 [12:43<00:24,  4.05it/s, loss=0.334, v_num=0]\u001b[A\n",
      "Epoch 1:  97%|████████▋| 3096/3195 [12:44<00:24,  4.05it/s, loss=0.334, v_num=0]\u001b[A\n",
      "Epoch 1:  97%|████████▋| 3097/3195 [12:44<00:24,  4.05it/s, loss=0.334, v_num=0]\u001b[A\n",
      "Epoch 1:  97%|████████▋| 3098/3195 [12:45<00:23,  4.05it/s, loss=0.334, v_num=0]\u001b[A\n",
      "Epoch 1:  97%|████████▋| 3099/3195 [12:45<00:23,  4.05it/s, loss=0.334, v_num=0]\u001b[A\n",
      "Epoch 1:  97%|████████▋| 3100/3195 [12:46<00:23,  4.05it/s, loss=0.334, v_num=0]\u001b[A\n",
      "Epoch 1:  97%|████████▋| 3101/3195 [12:46<00:23,  4.04it/s, loss=0.334, v_num=0]\u001b[A\n",
      "Epoch 1:  97%|████████▋| 3102/3195 [12:47<00:23,  4.04it/s, loss=0.334, v_num=0]\u001b[A\n",
      "Epoch 1:  97%|████████▋| 3103/3195 [12:47<00:22,  4.04it/s, loss=0.334, v_num=0]\u001b[A\n",
      "Epoch 1:  97%|████████▋| 3104/3195 [12:48<00:22,  4.04it/s, loss=0.334, v_num=0]\u001b[A\n",
      "Epoch 1:  97%|████████▋| 3105/3195 [12:48<00:22,  4.04it/s, loss=0.334, v_num=0]\u001b[A\n",
      "Epoch 1:  97%|████████▋| 3106/3195 [12:49<00:22,  4.04it/s, loss=0.334, v_num=0]\u001b[A\n",
      "Epoch 1:  97%|████████▊| 3107/3195 [12:49<00:21,  4.04it/s, loss=0.334, v_num=0]\u001b[A\n",
      "Epoch 1:  97%|████████▊| 3108/3195 [12:50<00:21,  4.03it/s, loss=0.334, v_num=0]\u001b[A\n",
      "Epoch 1:  97%|████████▊| 3109/3195 [12:50<00:21,  4.03it/s, loss=0.334, v_num=0]\u001b[A\n",
      "Epoch 1:  97%|████████▊| 3110/3195 [12:51<00:21,  4.03it/s, loss=0.334, v_num=0]\u001b[A\n",
      "Epoch 1:  97%|████████▊| 3111/3195 [12:51<00:20,  4.03it/s, loss=0.334, v_num=0]\u001b[A\n",
      "Epoch 1:  97%|████████▊| 3112/3195 [12:52<00:20,  4.03it/s, loss=0.334, v_num=0]\u001b[A\n",
      "Epoch 1:  97%|████████▊| 3113/3195 [12:52<00:20,  4.03it/s, loss=0.334, v_num=0]\u001b[A\n",
      "Epoch 1:  97%|████████▊| 3114/3195 [12:53<00:20,  4.03it/s, loss=0.334, v_num=0]\u001b[A\n",
      "Epoch 1:  97%|████████▊| 3115/3195 [12:53<00:19,  4.03it/s, loss=0.334, v_num=0]\u001b[A\n",
      "Epoch 1:  98%|████████▊| 3116/3195 [12:54<00:19,  4.02it/s, loss=0.334, v_num=0]\u001b[A\n",
      "Epoch 1:  98%|████████▊| 3117/3195 [12:54<00:19,  4.02it/s, loss=0.334, v_num=0]\u001b[A\n",
      "Epoch 1:  98%|████████▊| 3118/3195 [12:55<00:19,  4.02it/s, loss=0.334, v_num=0]\u001b[A\n",
      "Epoch 1:  98%|████████▊| 3119/3195 [12:55<00:18,  4.02it/s, loss=0.334, v_num=0]\u001b[A\n",
      "Epoch 1:  98%|████████▊| 3120/3195 [12:56<00:18,  4.02it/s, loss=0.334, v_num=0]\u001b[A\n",
      "Epoch 1:  98%|████████▊| 3121/3195 [12:56<00:18,  4.02it/s, loss=0.334, v_num=0]\u001b[A\n",
      "Epoch 1:  98%|████████▊| 3122/3195 [12:57<00:18,  4.02it/s, loss=0.334, v_num=0]\u001b[A\n",
      "Epoch 1:  98%|████████▊| 3123/3195 [12:58<00:17,  4.01it/s, loss=0.334, v_num=0]\u001b[A\n",
      "Epoch 1:  98%|████████▊| 3124/3195 [12:58<00:17,  4.01it/s, loss=0.334, v_num=0]\u001b[A\n",
      "Epoch 1:  98%|████████▊| 3125/3195 [12:59<00:17,  4.01it/s, loss=0.334, v_num=0]\u001b[A\n",
      "Epoch 1:  98%|████████▊| 3126/3195 [12:59<00:17,  4.01it/s, loss=0.334, v_num=0]\u001b[A\n",
      "Epoch 1:  98%|████████▊| 3127/3195 [13:00<00:16,  4.01it/s, loss=0.334, v_num=0]\u001b[A\n",
      "Epoch 1:  98%|████████▊| 3128/3195 [13:00<00:16,  4.01it/s, loss=0.334, v_num=0]\u001b[A\n",
      "Epoch 1:  98%|████████▊| 3129/3195 [13:01<00:16,  4.01it/s, loss=0.334, v_num=0]\u001b[A\n",
      "Epoch 1:  98%|████████▊| 3130/3195 [13:01<00:16,  4.00it/s, loss=0.334, v_num=0]\u001b[A\n",
      "Epoch 1:  98%|████████▊| 3131/3195 [13:02<00:15,  4.00it/s, loss=0.334, v_num=0]\u001b[A\n",
      "Epoch 1:  98%|████████▊| 3132/3195 [13:02<00:15,  4.00it/s, loss=0.334, v_num=0]\u001b[A\n",
      "Epoch 1:  98%|████████▊| 3133/3195 [13:03<00:15,  4.00it/s, loss=0.334, v_num=0]\u001b[A\n",
      "Epoch 1:  98%|████████▊| 3134/3195 [13:03<00:15,  4.00it/s, loss=0.334, v_num=0]\u001b[A\n",
      "Epoch 1:  98%|████████▊| 3135/3195 [13:04<00:15,  4.00it/s, loss=0.334, v_num=0]\u001b[A\n",
      "Epoch 1:  98%|████████▊| 3136/3195 [13:04<00:14,  4.00it/s, loss=0.334, v_num=0]\u001b[A\n",
      "Epoch 1:  98%|████████▊| 3137/3195 [13:05<00:14,  3.99it/s, loss=0.334, v_num=0]\u001b[A\n",
      "Epoch 1:  98%|████████▊| 3138/3195 [13:05<00:14,  3.99it/s, loss=0.334, v_num=0]\u001b[A\n",
      "Epoch 1:  98%|████████▊| 3139/3195 [13:06<00:14,  3.99it/s, loss=0.334, v_num=0]\u001b[A\n",
      "Epoch 1:  98%|████████▊| 3140/3195 [13:06<00:13,  3.99it/s, loss=0.334, v_num=0]\u001b[A\n",
      "Epoch 1:  98%|████████▊| 3141/3195 [13:07<00:13,  3.99it/s, loss=0.334, v_num=0]\u001b[A\n",
      "Epoch 1:  98%|████████▊| 3142/3195 [13:07<00:13,  3.99it/s, loss=0.334, v_num=0]\u001b[A\n",
      "Epoch 1:  98%|████████▊| 3143/3195 [13:08<00:13,  3.99it/s, loss=0.334, v_num=0]\u001b[A\n",
      "Epoch 1:  98%|████████▊| 3144/3195 [13:08<00:12,  3.99it/s, loss=0.334, v_num=0]\u001b[A\n",
      "Epoch 1:  98%|████████▊| 3145/3195 [13:09<00:12,  3.98it/s, loss=0.334, v_num=0]\u001b[A\n",
      "Epoch 1:  98%|████████▊| 3146/3195 [13:09<00:12,  3.98it/s, loss=0.334, v_num=0]\u001b[A\n",
      "Epoch 1:  98%|████████▊| 3147/3195 [13:10<00:12,  3.98it/s, loss=0.334, v_num=0]\u001b[A\n",
      "Epoch 1:  99%|████████▊| 3148/3195 [13:10<00:11,  3.98it/s, loss=0.334, v_num=0]\u001b[A\n",
      "Epoch 1:  99%|████████▊| 3149/3195 [13:11<00:11,  3.98it/s, loss=0.334, v_num=0]\u001b[A\n",
      "Epoch 1:  99%|████████▊| 3150/3195 [13:11<00:11,  3.98it/s, loss=0.334, v_num=0]\u001b[A\n",
      "Epoch 1:  99%|████████▉| 3151/3195 [13:12<00:11,  3.98it/s, loss=0.334, v_num=0]\u001b[A\n",
      "Epoch 1:  99%|████████▉| 3152/3195 [13:12<00:10,  3.97it/s, loss=0.334, v_num=0]\u001b[A\n",
      "Epoch 1:  99%|████████▉| 3153/3195 [13:13<00:10,  3.97it/s, loss=0.334, v_num=0]\u001b[A\n",
      "Epoch 1:  99%|████████▉| 3154/3195 [13:13<00:10,  3.97it/s, loss=0.334, v_num=0]\u001b[A\n",
      "Epoch 1:  99%|████████▉| 3155/3195 [13:14<00:10,  3.97it/s, loss=0.334, v_num=0]\u001b[A\n",
      "Epoch 1:  99%|████████▉| 3156/3195 [13:15<00:09,  3.97it/s, loss=0.334, v_num=0]\u001b[A\n",
      "Epoch 1:  99%|████████▉| 3157/3195 [13:15<00:09,  3.97it/s, loss=0.334, v_num=0]\u001b[A\n",
      "Epoch 1:  99%|████████▉| 3158/3195 [13:16<00:09,  3.97it/s, loss=0.334, v_num=0]\u001b[A\n",
      "Epoch 1:  99%|████████▉| 3159/3195 [13:16<00:09,  3.97it/s, loss=0.334, v_num=0]\u001b[A\n",
      "Epoch 1:  99%|████████▉| 3160/3195 [13:17<00:08,  3.96it/s, loss=0.334, v_num=0]\u001b[A\n",
      "Epoch 1:  99%|████████▉| 3161/3195 [13:17<00:08,  3.96it/s, loss=0.334, v_num=0]\u001b[A\n",
      "Epoch 1:  99%|████████▉| 3162/3195 [13:18<00:08,  3.96it/s, loss=0.334, v_num=0]\u001b[A\n",
      "Epoch 1:  99%|████████▉| 3163/3195 [13:18<00:08,  3.96it/s, loss=0.334, v_num=0]\u001b[A\n",
      "Epoch 1:  99%|████████▉| 3164/3195 [13:19<00:07,  3.96it/s, loss=0.334, v_num=0]\u001b[A\n",
      "Epoch 1:  99%|████████▉| 3165/3195 [13:19<00:07,  3.96it/s, loss=0.334, v_num=0]\u001b[A\n",
      "Epoch 1:  99%|████████▉| 3166/3195 [13:20<00:07,  3.96it/s, loss=0.334, v_num=0]\u001b[A\n",
      "Epoch 1:  99%|████████▉| 3167/3195 [13:20<00:07,  3.96it/s, loss=0.334, v_num=0]\u001b[A\n",
      "Epoch 1:  99%|████████▉| 3168/3195 [13:21<00:06,  3.95it/s, loss=0.334, v_num=0]\u001b[A\n",
      "Epoch 1:  99%|████████▉| 3169/3195 [13:21<00:06,  3.95it/s, loss=0.334, v_num=0]\u001b[A\n",
      "Epoch 1:  99%|████████▉| 3170/3195 [13:22<00:06,  3.95it/s, loss=0.334, v_num=0]\u001b[A\n",
      "Epoch 1:  99%|████████▉| 3171/3195 [13:22<00:06,  3.95it/s, loss=0.334, v_num=0]\u001b[A\n",
      "Epoch 1:  99%|████████▉| 3172/3195 [13:23<00:05,  3.95it/s, loss=0.334, v_num=0]\u001b[A\n",
      "Epoch 1:  99%|████████▉| 3173/3195 [13:23<00:05,  3.95it/s, loss=0.334, v_num=0]\u001b[A\n",
      "Epoch 1:  99%|████████▉| 3174/3195 [13:24<00:05,  3.95it/s, loss=0.334, v_num=0]\u001b[A\n",
      "Epoch 1:  99%|████████▉| 3175/3195 [13:24<00:05,  3.95it/s, loss=0.334, v_num=0]\u001b[A\n",
      "Epoch 1:  99%|████████▉| 3176/3195 [13:25<00:04,  3.94it/s, loss=0.334, v_num=0]\u001b[A\n",
      "Epoch 1:  99%|████████▉| 3177/3195 [13:25<00:04,  3.94it/s, loss=0.334, v_num=0]\u001b[A\n",
      "Epoch 1:  99%|████████▉| 3178/3195 [13:26<00:04,  3.94it/s, loss=0.334, v_num=0]\u001b[A\n",
      "Epoch 1:  99%|████████▉| 3179/3195 [13:26<00:04,  3.94it/s, loss=0.334, v_num=0]\u001b[A\n",
      "Epoch 1: 100%|████████▉| 3180/3195 [13:27<00:03,  3.94it/s, loss=0.334, v_num=0]\u001b[A\n",
      "Epoch 1: 100%|████████▉| 3181/3195 [13:27<00:03,  3.94it/s, loss=0.334, v_num=0]\u001b[A\n",
      "Epoch 1: 100%|████████▉| 3182/3195 [13:28<00:03,  3.94it/s, loss=0.334, v_num=0]\u001b[A\n",
      "Epoch 1: 100%|████████▉| 3183/3195 [13:28<00:03,  3.94it/s, loss=0.334, v_num=0]\u001b[A\n",
      "Epoch 1: 100%|████████▉| 3184/3195 [13:29<00:02,  3.93it/s, loss=0.334, v_num=0]\u001b[A\n",
      "Epoch 1: 100%|████████▉| 3185/3195 [13:29<00:02,  3.93it/s, loss=0.334, v_num=0]\u001b[A\n",
      "Epoch 1: 100%|████████▉| 3186/3195 [13:30<00:02,  3.93it/s, loss=0.334, v_num=0]\u001b[A\n",
      "Epoch 1: 100%|████████▉| 3187/3195 [13:30<00:02,  3.93it/s, loss=0.334, v_num=0]\u001b[A\n",
      "Epoch 1: 100%|████████▉| 3188/3195 [13:31<00:01,  3.93it/s, loss=0.334, v_num=0]\u001b[A\n",
      "Epoch 1: 100%|████████▉| 3189/3195 [13:31<00:01,  3.93it/s, loss=0.334, v_num=0]\u001b[A\n",
      "Epoch 1: 100%|████████▉| 3190/3195 [13:32<00:01,  3.93it/s, loss=0.334, v_num=0]\u001b[A\n",
      "Epoch 1: 100%|████████▉| 3191/3195 [13:32<00:01,  3.93it/s, loss=0.334, v_num=0]\u001b[A\n",
      "Epoch 1: 100%|████████▉| 3192/3195 [13:33<00:00,  3.92it/s, loss=0.334, v_num=0]\u001b[A\n",
      "Epoch 1: 100%|████████▉| 3193/3195 [13:34<00:00,  3.92it/s, loss=0.334, v_num=0]\u001b[A\n",
      "Epoch 1: 100%|████████▉| 3194/3195 [13:34<00:00,  3.92it/s, loss=0.334, v_num=0]\u001b[A\n",
      "Epoch 1: 100%|█████████| 3195/3195 [13:35<00:00,  3.92it/s, loss=0.334, v_num=0]{'vloss': tensor(0.1688, device='cuda:0'), 'rouge1': tensor(0.9414, device='cuda:0'), 'rouge2': tensor(0.9219, device='cuda:0'), 'rougeL': tensor(0.9414, device='cuda:0'), 'rougeLsum': tensor(0.9414, device='cuda:0'), 'acc': tensor(0.7656, device='cuda:0')}\n",
      "\n",
      "Epoch 00000: avg_val_loss reached 0.16879 (best 0.16879), saving model to /home/ec2-user/SageMaker/summarization/test/_ckpt_epoch_0_v3.ckpt as top 5\n",
      "I0722 08:23:15.758078 139690818238272 model_checkpoint.py:338] \n",
      "Epoch 00000: avg_val_loss reached 0.16879 (best 0.16879), saving model to /home/ec2-user/SageMaker/summarization/test/_ckpt_epoch_0_v3.ckpt as top 5\n",
      "Epoch 1: 100%|█| 3195/3195 [13:38<00:00,  3.90it/s, loss=0.334, v_num=0, vloss=0\n",
      "Epoch 1: 100%|█| 3195/3195 [13:38<00:00,  3.90it/s, loss=0.334, v_num=0, vloss=0\u001b[A\n",
      "/home/ec2-user/anaconda3/envs/python3/lib/python3.8/site-packages/pytorch_lightning/utilities/distributed.py:25: UserWarning: Your test_dataloader has `shuffle=True`, it is best practice to turn this off for validation and test dataloaders.\n",
      "  warnings.warn(*args, **kwargs)\n",
      "/home/ec2-user/anaconda3/envs/python3/lib/python3.8/site-packages/pytorch_lightning/utilities/distributed.py:25: UserWarning: The dataloader, test dataloader 0, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 8 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  warnings.warn(*args, **kwargs)\n",
      "Testing:  14%|████▋                            | 97/687 [00:50<05:00,  1.96it/s]"
     ]
    }
   ],
   "source": [
    "!python longformer/scripts/summarization_opt.py\\\n",
    "--model_path 'longformer-encdec-base-16384'\\\n",
    "--tokenizer 'longformer-encdec-base-16384'\\\n",
    "--epochs 1\\\n",
    "--max_input_len 4000 \\\n",
    "--batch_size 1\\\n",
    "--dataset_name \"test\"\\\n",
    "--train_file 'stary/model_b_data/train.csv'\\\n",
    "--test_file 'stary/model_b_data/test.csv'\\\n",
    "--validation_file 'stary/model_b_data/test.csv'"
   ]
  },
  {
   "cell_type": "raw",
   "id": "e6aace68",
   "metadata": {},
   "source": [
    "soufrom collections import OrderedDict   #导入此模块\n",
    "base_weights = torch.load(ckpt_pth)['state_dict']\n",
    "new_state_dict = OrderedDict()\n",
    "\n",
    "for k, v in base_weights.items():\n",
    "    #print (k)\n",
    "    if k=='model.final_logits_bias':\n",
    "        new_state_dict['final_logits_bias'] = v \n",
    "        new_state_dict[k] = v \n",
    "    else:\n",
    "        new_state_dict[k] = v \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "8d81148b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch \n",
    "from longformer import LongformerEncoderDecoderForConditionalGeneration, LongformerEncoderDecoderConfig\n",
    "from transformers import AutoTokenizer\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "#config = LongformerEncoderDecoderConfig.from_pretrained('/home/ec2-user/SageMaker/summarization/test/_ckpt_epoch_0_v0.ckpt')\n",
    "config = LongformerEncoderDecoderConfig.from_pretrained('longformer-encdec-base-16384')\n",
    "#config.attention_dropout = self.args.attention_dropout\n",
    "#config.gradient_checkpointing = self.args.grad_ckpt\n",
    "config.attention_mode = 'sliding_chunks'\n",
    "#config.attention_window = [self.args.attention_window] * config.encoder_layers\n",
    "\n",
    "model.model = LongformerEncoderDecoderForConditionalGeneration.from_pretrained('longformer-encdec-base-16384',config = config)\n",
    "#ckpt_pth = '/home/ec2-user/SageMaker/summarization/test/_ckpt_epoch_0_v2.ckpt'\n",
    "#x = torch.load(ckpt_pth)['state_dict']\n",
    "#x['model.model.final_logits_bias'] = x['model.final_logits_bias']\n",
    "\n",
    "model.load_state_dict(new_state_dict)\n",
    "#model = torch.load(ckpt_pth,map_location=device)\n",
    "model.eval()\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained('longformer-encdec-base-16384')\n",
    "\n",
    "tokenizer.model_max_length = 4000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "b17c8035",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "SAMPLE_TEXT = ' '.join(['Hello world! '] * 200)  # long input document\n",
    "\n",
    "input_ids = torch.tensor(tokenizer.encode(SAMPLE_TEXT)).unsqueeze(0)\n",
    "\n",
    "attention_mask = torch.ones(input_ids.shape, dtype=torch.long, device=input_ids.device)\n",
    "attention_mask[input_ids == tokenizer.pad_token_id] = 0\n",
    "attention_mask[:, 0] = 2 \n",
    "half_padding_mod = model.config.attention_window[0]\n",
    "input_ids, attention_mask = pad_to_window_size(  # ideally, should be moved inside the LongformerModel\n",
    "                input_ids, attention_mask, half_padding_mod, tokenizer.pad_token_id)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "a7866801",
   "metadata": {},
   "outputs": [],
   "source": [
    "generated_ids =  model.model.generate(input_ids=input_ids, attention_mask=attention_mask,\n",
    "                                            use_cache=True, max_length=256,\n",
    "                                            num_beams=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "87bd0b40",
   "metadata": {},
   "outputs": [],
   "source": [
    "generated_str = tokenizer.batch_decode(generated_ids.tolist(), skip_special_tokens=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "a2b0d9fe",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Bro said the sentence']"
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "generated_str"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "conda_python3",
   "language": "python",
   "name": "conda_python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
