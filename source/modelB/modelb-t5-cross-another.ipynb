{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d3dc8a0c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#preprocess\n",
    "import json\n",
    "import os\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "4a2f04de",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_json(json_file):\n",
    "    # Opening JSON file \n",
    "    f = open(json_file) \n",
    "\n",
    "    # returns JSON object as  \n",
    "    # a dictionary \n",
    "    data = json.load(f) \n",
    "    content_ls = [' '.join(data['content'][str(i)]['word_list']) for i in range(len(data['content']))]\n",
    "    #label_ls = [data['content'][str(i)]['dialogue_label'] for i in range(len(data['content']))]\n",
    "    label_ls = [int(data['content'][str(i)]['ner_label'][0]) for i in range(len(data['content']))]\n",
    "    role_dict = data['role_id']\n",
    "    return content_ls,label_ls,role_dict\n",
    "\n",
    "def edit_b(x,role):\n",
    "    res = []\n",
    "    for i in range(len(x)):\n",
    "        if i<3:\n",
    "            res_str = \"prefix: \"+','.join(x[:i])+\" center: \"+x[i] + \" after: \"+ ','.join(x[i:i+2]) + \" roles: \"+str(list(role.values()))\n",
    "            res.append(res_str) \n",
    "        elif len(x)-i<3:\n",
    "            res_str = \"prefix: \"+','.join(x[i-2:i])+\" center: \"+x[i] + \" after: \"+ ','.join(x[i:])+\" roles: \"+str(list(role.values()))\n",
    "            res.append(res_str)\n",
    "        else:\n",
    "            res_str = \"prefix: \"+','.join(x[i-2:i])+\" center: \"+x[i] + \" after: \"+ ','.join(x[i:i+2])+\" roles: \"+str(list(role.values()))\n",
    "            res.append(res_str)\n",
    "    return res\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "175f9aae",
   "metadata": {},
   "outputs": [],
   "source": [
    "#load one book\n",
    "def load_book(book_path,tag):\n",
    "    chapter_ls = os.listdir(book_path)\n",
    "    cut=int(len(chapter_ls)*0.9)\n",
    "    if tag is True:\n",
    "        chapter_ls = chapter_ls[:cut]\n",
    "    else:\n",
    "        chapter_ls = chapter_ls[cut:]\n",
    "        \n",
    "    print (\"<<< books: \", chapter_ls)\n",
    "    res = []\n",
    "    for i in chapter_ls:\n",
    "        if i[-4:]==\"json\":\n",
    "            json_file = os.path.join(book_path,i)\n",
    "            content_ls, label_ls, role_dict = load_json(json_file)\n",
    "            content_ls = edit_b(content_ls,role_dict)\n",
    "            \n",
    "            df_res = pd.DataFrame({'sentence1_key':content_ls,'label':label_ls})\n",
    "            df_res = df_res[df_res['label']!=0]\n",
    "            df_res[\"label\"] = df_res[\"label\"].map(lambda x: role_dict[str(x)])\n",
    "            df_res[\"label\"] = df_res[\"label\"].map(lambda x: x+\" said the sentence\")\n",
    "\n",
    "            df_res['sentence1_key'] = df_res['sentence1_key'].map(lambda x: x.replace('“','\"'))\n",
    "            df_res['sentence1_key'] = df_res['sentence1_key'].map(lambda x: x.replace('”','\"'))\n",
    "            res.append(df_res)\n",
    "    res_table = pd.concat(res)\n",
    "\n",
    "    return res_table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "70ad745f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<<< books:  [\"20%(2271377The Mafia's Good Wife)(1).json\", '80%(2059119Heart of Freeman).json', '60%_(2040244Lunar wolvesHis to own Book 1Complete).json', '30%_(2165912The Curse Of Violet Wraith).json', '60%_(2144894A Moonlit Encounter).json', '.ipynb_checkpoints', '20%_(1993322ASHER RICK).json', '20%_(2070697Revenge on my Ex-Husband).json', '60%_(2164082New Husband For My Wife) .json']\n",
      "<<< books:  ['80%(2192588love&mate) (1).json', '85%_(2061307His Ruthless Assistant (completed )).json']\n"
     ]
    }
   ],
   "source": [
    "book_path = '../new_example_ten_json'\n",
    "train = load_book(book_path,True)\n",
    "test = load_book(book_path,False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "33d20fb8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train size (2508, 2), test size(172, 2)\n"
     ]
    }
   ],
   "source": [
    "test1, test2 = train_test_split(test,test_size=0.25,random_state=0)\n",
    "train = pd.concat([train,test1])\n",
    "print (\"train size {}, test size{}\".format(train.shape,test2.shape))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "61ec1f60",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>sentence1_key</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1222</th>\n",
       "      <td>prefix: \"What do you care? What do you care if...</td>\n",
       "      <td>Kade said the sentence</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>333</th>\n",
       "      <td>prefix: \"There are 15 coming so far. Some are ...</td>\n",
       "      <td>Annie said the sentence</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>378</th>\n",
       "      <td>prefix: \"Goldclaw is not far from here. There’...</td>\n",
       "      <td>Kade said the sentence</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>322</th>\n",
       "      <td>prefix: \"What the hell do you want Brooke \",\"Z...</td>\n",
       "      <td>Zach said the sentence</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>260</th>\n",
       "      <td>prefix: He looks at me for a moment, his eyes ...</td>\n",
       "      <td>Annie said the sentence</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                          sentence1_key  \\\n",
       "1222  prefix: \"What do you care? What do you care if...   \n",
       "333   prefix: \"There are 15 coming so far. Some are ...   \n",
       "378   prefix: \"Goldclaw is not far from here. There’...   \n",
       "322   prefix: \"What the hell do you want Brooke \",\"Z...   \n",
       "260   prefix: He looks at me for a moment, his eyes ...   \n",
       "\n",
       "                        label  \n",
       "1222   Kade said the sentence  \n",
       "333   Annie said the sentence  \n",
       "378    Kade said the sentence  \n",
       "322    Zach said the sentence  \n",
       "260   Annie said the sentence  "
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test1.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "2e96bf3f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "path = '../model_b_data'\n",
    "\n",
    "# Check whether the specified path exists or not\n",
    "isExist = os.path.exists(path)\n",
    "\n",
    "if not isExist:\n",
    "    os.makedirs(path)\n",
    "    print(\"The new directory is created!\")\n",
    "    \n",
    "train[[\"label\",\"sentence1_key\"]].to_csv('../model_b_data/train.csv',index=False,encoding='utf-8')\n",
    "test[[\"label\",\"sentence1_key\"]].to_csv('../model_b_data/test.csv',index=False,encoding='utf-8')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "257720ae",
   "metadata": {},
   "source": [
    "# train - model a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "c566f59c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import boto3\n",
    "import sagemaker\n",
    "import os\n",
    "from sagemaker import get_execution_role\n",
    "\n",
    "sess = sagemaker.Session()\n",
    "role = get_execution_role()\n",
    "\n",
    "prefix='stary-datalab-modelb'\n",
    "\n",
    "bucket = sess.default_bucket() \n",
    "boto3.Session().resource(\"s3\").Bucket(bucket).Object(\n",
    "    os.path.join(prefix, \"train/train.csv\")\n",
    ").upload_file(\"../model_b_data/train.csv\")\n",
    "boto3.Session().resource(\"s3\").Bucket(bucket).Object(\n",
    "    os.path.join(prefix, \"test/test.csv\")\n",
    ").upload_file(\"../model_b_data/test.csv\")\n",
    "\n",
    "training_input_path = f's3://{sess.default_bucket()}/{prefix}/train/train.csv'\n",
    "test_input_path = f's3://{sess.default_bucket()}/{prefix}/test/test.csv'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "153fb9e1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2022-07-14 06:27:11 Starting - Starting the training job...\n",
      "2022-07-14 06:27:36 Starting - Preparing the instances for trainingProfilerReport-1657780031: InProgress\n",
      ".........\n",
      "2022-07-14 06:28:55 Downloading - Downloading input data...\n",
      "2022-07-14 06:29:35 Training - Downloading the training image..................\n",
      "2022-07-14 06:32:37 Training - Training image download completed. Training in progress..\u001b[34mbash: cannot set terminal process group (-1): Inappropriate ioctl for device\u001b[0m\n",
      "\u001b[34mbash: no job control in this shell\u001b[0m\n",
      "\u001b[34m2022-07-14 06:32:39,811 sagemaker-training-toolkit INFO     Imported framework sagemaker_pytorch_container.training\u001b[0m\n",
      "\u001b[34m2022-07-14 06:32:39,835 sagemaker_pytorch_container.training INFO     Block until all host DNS lookups succeed.\u001b[0m\n",
      "\u001b[34m2022-07-14 06:32:39,842 sagemaker_pytorch_container.training INFO     Invoking user training script.\u001b[0m\n",
      "\u001b[34m2022-07-14 06:32:40,229 sagemaker-training-toolkit INFO     Installing dependencies from requirements.txt:\u001b[0m\n",
      "\u001b[34m/opt/conda/bin/python3.6 -m pip install -r requirements.txt\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: datasets>=1.1.3 in /opt/conda/lib/python3.6/site-packages (from -r requirements.txt (line 1)) (1.6.2)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: sentencepiece!=0.1.92 in /opt/conda/lib/python3.6/site-packages (from -r requirements.txt (line 2)) (0.1.91)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: protobuf in /opt/conda/lib/python3.6/site-packages (from -r requirements.txt (line 3)) (3.17.1)\u001b[0m\n",
      "\u001b[34mCollecting rouge-score\n",
      "  Downloading rouge_score-0.0.4-py2.py3-none-any.whl (22 kB)\u001b[0m\n",
      "\u001b[34mCollecting nltk\n",
      "  Downloading nltk-3.6.7-py3-none-any.whl (1.5 MB)\u001b[0m\n",
      "\u001b[34mCollecting py7zr\n",
      "  Downloading py7zr-0.19.0-py3-none-any.whl (65 kB)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: torch>=1.3 in /opt/conda/lib/python3.6/site-packages (from -r requirements.txt (line 7)) (1.7.1)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: pandas in /opt/conda/lib/python3.6/site-packages (from datasets>=1.1.3->-r requirements.txt (line 1)) (1.1.5)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: xxhash in /opt/conda/lib/python3.6/site-packages (from datasets>=1.1.3->-r requirements.txt (line 1)) (2.0.2)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: multiprocess in /opt/conda/lib/python3.6/site-packages (from datasets>=1.1.3->-r requirements.txt (line 1)) (0.70.11.1)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: numpy>=1.17 in /opt/conda/lib/python3.6/site-packages (from datasets>=1.1.3->-r requirements.txt (line 1)) (1.19.1)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: requests>=2.19.0 in /opt/conda/lib/python3.6/site-packages (from datasets>=1.1.3->-r requirements.txt (line 1)) (2.25.1)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: tqdm<4.50.0,>=4.27 in /opt/conda/lib/python3.6/site-packages (from datasets>=1.1.3->-r requirements.txt (line 1)) (4.49.0)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: importlib-metadata in /opt/conda/lib/python3.6/site-packages (from datasets>=1.1.3->-r requirements.txt (line 1)) (4.0.1)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: fsspec in /opt/conda/lib/python3.6/site-packages (from datasets>=1.1.3->-r requirements.txt (line 1)) (2021.5.0)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: dataclasses in /opt/conda/lib/python3.6/site-packages (from datasets>=1.1.3->-r requirements.txt (line 1)) (0.8)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: packaging in /opt/conda/lib/python3.6/site-packages (from datasets>=1.1.3->-r requirements.txt (line 1)) (20.9)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: huggingface-hub<0.1.0 in /opt/conda/lib/python3.6/site-packages (from datasets>=1.1.3->-r requirements.txt (line 1)) (0.0.8)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: dill in /opt/conda/lib/python3.6/site-packages (from datasets>=1.1.3->-r requirements.txt (line 1)) (0.3.3)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: pyarrow>=1.0.0<4.0.0 in /opt/conda/lib/python3.6/site-packages (from datasets>=1.1.3->-r requirements.txt (line 1)) (4.0.0)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: typing-extensions in /opt/conda/lib/python3.6/site-packages (from torch>=1.3->-r requirements.txt (line 7)) (3.10.0.0)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: filelock in /opt/conda/lib/python3.6/site-packages (from huggingface-hub<0.1.0->datasets>=1.1.3->-r requirements.txt (line 1)) (3.0.12)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: urllib3<1.27,>=1.21.1 in /opt/conda/lib/python3.6/site-packages (from requests>=2.19.0->datasets>=1.1.3->-r requirements.txt (line 1)) (1.25.11)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.6/site-packages (from requests>=2.19.0->datasets>=1.1.3->-r requirements.txt (line 1)) (2020.12.5)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: chardet<5,>=3.0.2 in /opt/conda/lib/python3.6/site-packages (from requests>=2.19.0->datasets>=1.1.3->-r requirements.txt (line 1)) (3.0.4)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: idna<3,>=2.5 in /opt/conda/lib/python3.6/site-packages (from requests>=2.19.0->datasets>=1.1.3->-r requirements.txt (line 1)) (2.10)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: six>=1.9 in /opt/conda/lib/python3.6/site-packages (from protobuf->-r requirements.txt (line 3)) (1.16.0)\u001b[0m\n",
      "\u001b[34mCollecting absl-py\n",
      "  Downloading absl_py-1.1.0-py3-none-any.whl (123 kB)\u001b[0m\n",
      "\u001b[34mCollecting regex>=2021.8.3\n",
      "  Downloading regex-2022.7.9-cp36-cp36m-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (749 kB)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: joblib in /opt/conda/lib/python3.6/site-packages (from nltk->-r requirements.txt (line 5)) (1.0.1)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: click in /opt/conda/lib/python3.6/site-packages (from nltk->-r requirements.txt (line 5)) (7.1.2)\u001b[0m\n",
      "\u001b[34mCollecting texttable\n",
      "  Downloading texttable-1.6.4-py2.py3-none-any.whl (10 kB)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: psutil in /opt/conda/lib/python3.6/site-packages (from py7zr->-r requirements.txt (line 6)) (5.8.0)\u001b[0m\n",
      "\u001b[34mCollecting pybcj>=0.6.0\n",
      "  Downloading pybcj-0.6.1-cp36-cp36m-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (49 kB)\u001b[0m\n",
      "\u001b[34mCollecting pycryptodomex>=3.6.6\n",
      "  Downloading pycryptodomex-3.15.0-cp35-abi3-manylinux2010_x86_64.whl (2.3 MB)\u001b[0m\n",
      "\u001b[34mCollecting pyzstd>=0.14.4\n",
      "  Downloading pyzstd-0.15.2-cp36-cp36m-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (2.4 MB)\u001b[0m\n",
      "\u001b[34mCollecting multivolumefile>=0.2.3\n",
      "  Downloading multivolumefile-0.2.3-py3-none-any.whl (17 kB)\u001b[0m\n",
      "\u001b[34mCollecting brotli>=1.0.9\n",
      "  Downloading Brotli-1.0.9-cp36-cp36m-manylinux1_x86_64.whl (357 kB)\u001b[0m\n",
      "\u001b[34mCollecting pyppmd<0.19.0,>=0.18.1\n",
      "  Downloading pyppmd-0.18.3-cp36-cp36m-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (132 kB)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: zipp>=0.5 in /opt/conda/lib/python3.6/site-packages (from importlib-metadata->datasets>=1.1.3->-r requirements.txt (line 1)) (3.4.1)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: pyparsing>=2.0.2 in /opt/conda/lib/python3.6/site-packages (from packaging->datasets>=1.1.3->-r requirements.txt (line 1)) (2.4.7)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: python-dateutil>=2.7.3 in /opt/conda/lib/python3.6/site-packages (from pandas->datasets>=1.1.3->-r requirements.txt (line 1)) (2.8.1)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: pytz>=2017.2 in /opt/conda/lib/python3.6/site-packages (from pandas->datasets>=1.1.3->-r requirements.txt (line 1)) (2021.1)\u001b[0m\n",
      "\u001b[34mInstalling collected packages: regex, texttable, pyzstd, pyppmd, pycryptodomex, pybcj, nltk, multivolumefile, brotli, absl-py, rouge-score, py7zr\n",
      "  Attempting uninstall: regex\n",
      "    Found existing installation: regex 2021.4.4\n",
      "    Uninstalling regex-2021.4.4:\n",
      "      Successfully uninstalled regex-2021.4.4\u001b[0m\n",
      "\u001b[34mSuccessfully installed absl-py-1.1.0 brotli-1.0.9 multivolumefile-0.2.3 nltk-3.6.7 py7zr-0.19.0 pybcj-0.6.1 pycryptodomex-3.15.0 pyppmd-0.18.3 pyzstd-0.15.2 regex-2022.7.9 rouge-score-0.0.4 texttable-1.6.4\u001b[0m\n",
      "\u001b[34mWARNING: Running pip as root will break packages and permissions. You should install packages reliably by using venv: https://pip.pypa.io/warnings/venv\u001b[0m\n",
      "\u001b[34m2022-07-14 06:32:47,886 sagemaker-training-toolkit INFO     Invoking user script\u001b[0m\n",
      "\u001b[34mTraining Env:\u001b[0m\n",
      "\u001b[34m{\n",
      "    \"additional_framework_parameters\": {},\n",
      "    \"channel_input_dirs\": {\n",
      "        \"test\": \"/opt/ml/input/data/test\",\n",
      "        \"train\": \"/opt/ml/input/data/train\",\n",
      "        \"validation\": \"/opt/ml/input/data/validation\"\n",
      "    },\n",
      "    \"current_host\": \"algo-1\",\n",
      "    \"framework_module\": \"sagemaker_pytorch_container.training:main\",\n",
      "    \"hosts\": [\n",
      "        \"algo-1\"\n",
      "    ],\n",
      "    \"hyperparameters\": {\n",
      "        \"do_eval\": true,\n",
      "        \"do_train\": true,\n",
      "        \"eval_steps\": 5000,\n",
      "        \"evaluation_strategy\": \"epoch\",\n",
      "        \"gradient_accumulation_steps\": 2,\n",
      "        \"hypothesis_column\": \"label\",\n",
      "        \"learning_rate\": 0.0003,\n",
      "        \"max_source_length\": 128,\n",
      "        \"max_target_length\": 128,\n",
      "        \"model_name_or_path\": \"t5-base\",\n",
      "        \"num_train_epochs\": 10,\n",
      "        \"output_dir\": \"/opt/ml/model\",\n",
      "        \"per_device_train_batch_size\": 2,\n",
      "        \"reference_column\": \"sentence1_key\",\n",
      "        \"save_strategy\": \"steps\",\n",
      "        \"save_total_limit\": 1,\n",
      "        \"test_file\": \"/opt/ml/input/data/test/test.csv\",\n",
      "        \"train_file\": \"/opt/ml/input/data/train/train.csv\",\n",
      "        \"validation_file\": \"/opt/ml/input/data/validation/test.csv\"\n",
      "    },\n",
      "    \"input_config_dir\": \"/opt/ml/input/config\",\n",
      "    \"input_data_config\": {\n",
      "        \"test\": {\n",
      "            \"TrainingInputMode\": \"File\",\n",
      "            \"S3DistributionType\": \"FullyReplicated\",\n",
      "            \"RecordWrapperType\": \"None\"\n",
      "        },\n",
      "        \"train\": {\n",
      "            \"TrainingInputMode\": \"File\",\n",
      "            \"S3DistributionType\": \"FullyReplicated\",\n",
      "            \"RecordWrapperType\": \"None\"\n",
      "        },\n",
      "        \"validation\": {\n",
      "            \"TrainingInputMode\": \"File\",\n",
      "            \"S3DistributionType\": \"FullyReplicated\",\n",
      "            \"RecordWrapperType\": \"None\"\n",
      "        }\n",
      "    },\n",
      "    \"input_dir\": \"/opt/ml/input\",\n",
      "    \"is_master\": true,\n",
      "    \"job_name\": \"train-modelb-stary-0713-crossbook-2022-07-14-06-27-10-862\",\n",
      "    \"log_level\": 20,\n",
      "    \"master_hostname\": \"algo-1\",\n",
      "    \"model_dir\": \"/opt/ml/model\",\n",
      "    \"module_dir\": \"s3://sagemaker-us-east-1-726335585155/train-modelb-stary-0713-crossbook-2022-07-14-06-27-10-862/source/sourcedir.tar.gz\",\n",
      "    \"module_name\": \"run_train\",\n",
      "    \"network_interface_name\": \"eth0\",\n",
      "    \"num_cpus\": 8,\n",
      "    \"num_gpus\": 1,\n",
      "    \"output_data_dir\": \"/opt/ml/output/data\",\n",
      "    \"output_dir\": \"/opt/ml/output\",\n",
      "    \"output_intermediate_dir\": \"/opt/ml/output/intermediate\",\n",
      "    \"resource_config\": {\n",
      "        \"current_host\": \"algo-1\",\n",
      "        \"current_instance_type\": \"ml.p3.2xlarge\",\n",
      "        \"current_group_name\": \"homogeneousCluster\",\n",
      "        \"hosts\": [\n",
      "            \"algo-1\"\n",
      "        ],\n",
      "        \"instance_groups\": [\n",
      "            {\n",
      "                \"instance_group_name\": \"homogeneousCluster\",\n",
      "                \"instance_type\": \"ml.p3.2xlarge\",\n",
      "                \"hosts\": [\n",
      "                    \"algo-1\"\n",
      "                ]\n",
      "            }\n",
      "        ],\n",
      "        \"network_interface_name\": \"eth0\"\n",
      "    },\n",
      "    \"user_entry_point\": \"run_train.py\"\u001b[0m\n",
      "\u001b[34m}\u001b[0m\n",
      "\u001b[34mEnvironment variables:\u001b[0m\n",
      "\u001b[34mSM_HOSTS=[\"algo-1\"]\u001b[0m\n",
      "\u001b[34mSM_NETWORK_INTERFACE_NAME=eth0\u001b[0m\n",
      "\u001b[34mSM_HPS={\"do_eval\":true,\"do_train\":true,\"eval_steps\":5000,\"evaluation_strategy\":\"epoch\",\"gradient_accumulation_steps\":2,\"hypothesis_column\":\"label\",\"learning_rate\":0.0003,\"max_source_length\":128,\"max_target_length\":128,\"model_name_or_path\":\"t5-base\",\"num_train_epochs\":10,\"output_dir\":\"/opt/ml/model\",\"per_device_train_batch_size\":2,\"reference_column\":\"sentence1_key\",\"save_strategy\":\"steps\",\"save_total_limit\":1,\"test_file\":\"/opt/ml/input/data/test/test.csv\",\"train_file\":\"/opt/ml/input/data/train/train.csv\",\"validation_file\":\"/opt/ml/input/data/validation/test.csv\"}\u001b[0m\n",
      "\u001b[34mSM_USER_ENTRY_POINT=run_train.py\u001b[0m\n",
      "\u001b[34mSM_FRAMEWORK_PARAMS={}\u001b[0m\n",
      "\u001b[34mSM_RESOURCE_CONFIG={\"current_group_name\":\"homogeneousCluster\",\"current_host\":\"algo-1\",\"current_instance_type\":\"ml.p3.2xlarge\",\"hosts\":[\"algo-1\"],\"instance_groups\":[{\"hosts\":[\"algo-1\"],\"instance_group_name\":\"homogeneousCluster\",\"instance_type\":\"ml.p3.2xlarge\"}],\"network_interface_name\":\"eth0\"}\u001b[0m\n",
      "\u001b[34mSM_INPUT_DATA_CONFIG={\"test\":{\"RecordWrapperType\":\"None\",\"S3DistributionType\":\"FullyReplicated\",\"TrainingInputMode\":\"File\"},\"train\":{\"RecordWrapperType\":\"None\",\"S3DistributionType\":\"FullyReplicated\",\"TrainingInputMode\":\"File\"},\"validation\":{\"RecordWrapperType\":\"None\",\"S3DistributionType\":\"FullyReplicated\",\"TrainingInputMode\":\"File\"}}\u001b[0m\n",
      "\u001b[34mSM_OUTPUT_DATA_DIR=/opt/ml/output/data\u001b[0m\n",
      "\u001b[34mSM_CHANNELS=[\"test\",\"train\",\"validation\"]\u001b[0m\n",
      "\u001b[34mSM_CURRENT_HOST=algo-1\u001b[0m\n",
      "\u001b[34mSM_MODULE_NAME=run_train\u001b[0m\n",
      "\u001b[34mSM_LOG_LEVEL=20\u001b[0m\n",
      "\u001b[34mSM_FRAMEWORK_MODULE=sagemaker_pytorch_container.training:main\u001b[0m\n",
      "\u001b[34mSM_INPUT_DIR=/opt/ml/input\u001b[0m\n",
      "\u001b[34mSM_INPUT_CONFIG_DIR=/opt/ml/input/config\u001b[0m\n",
      "\u001b[34mSM_OUTPUT_DIR=/opt/ml/output\u001b[0m\n",
      "\u001b[34mSM_NUM_CPUS=8\u001b[0m\n",
      "\u001b[34mSM_NUM_GPUS=1\u001b[0m\n",
      "\u001b[34mSM_MODEL_DIR=/opt/ml/model\u001b[0m\n",
      "\u001b[34mSM_MODULE_DIR=s3://sagemaker-us-east-1-726335585155/train-modelb-stary-0713-crossbook-2022-07-14-06-27-10-862/source/sourcedir.tar.gz\u001b[0m\n",
      "\u001b[34mSM_TRAINING_ENV={\"additional_framework_parameters\":{},\"channel_input_dirs\":{\"test\":\"/opt/ml/input/data/test\",\"train\":\"/opt/ml/input/data/train\",\"validation\":\"/opt/ml/input/data/validation\"},\"current_host\":\"algo-1\",\"framework_module\":\"sagemaker_pytorch_container.training:main\",\"hosts\":[\"algo-1\"],\"hyperparameters\":{\"do_eval\":true,\"do_train\":true,\"eval_steps\":5000,\"evaluation_strategy\":\"epoch\",\"gradient_accumulation_steps\":2,\"hypothesis_column\":\"label\",\"learning_rate\":0.0003,\"max_source_length\":128,\"max_target_length\":128,\"model_name_or_path\":\"t5-base\",\"num_train_epochs\":10,\"output_dir\":\"/opt/ml/model\",\"per_device_train_batch_size\":2,\"reference_column\":\"sentence1_key\",\"save_strategy\":\"steps\",\"save_total_limit\":1,\"test_file\":\"/opt/ml/input/data/test/test.csv\",\"train_file\":\"/opt/ml/input/data/train/train.csv\",\"validation_file\":\"/opt/ml/input/data/validation/test.csv\"},\"input_config_dir\":\"/opt/ml/input/config\",\"input_data_config\":{\"test\":{\"RecordWrapperType\":\"None\",\"S3DistributionType\":\"FullyReplicated\",\"TrainingInputMode\":\"File\"},\"train\":{\"RecordWrapperType\":\"None\",\"S3DistributionType\":\"FullyReplicated\",\"TrainingInputMode\":\"File\"},\"validation\":{\"RecordWrapperType\":\"None\",\"S3DistributionType\":\"FullyReplicated\",\"TrainingInputMode\":\"File\"}},\"input_dir\":\"/opt/ml/input\",\"is_master\":true,\"job_name\":\"train-modelb-stary-0713-crossbook-2022-07-14-06-27-10-862\",\"log_level\":20,\"master_hostname\":\"algo-1\",\"model_dir\":\"/opt/ml/model\",\"module_dir\":\"s3://sagemaker-us-east-1-726335585155/train-modelb-stary-0713-crossbook-2022-07-14-06-27-10-862/source/sourcedir.tar.gz\",\"module_name\":\"run_train\",\"network_interface_name\":\"eth0\",\"num_cpus\":8,\"num_gpus\":1,\"output_data_dir\":\"/opt/ml/output/data\",\"output_dir\":\"/opt/ml/output\",\"output_intermediate_dir\":\"/opt/ml/output/intermediate\",\"resource_config\":{\"current_group_name\":\"homogeneousCluster\",\"current_host\":\"algo-1\",\"current_instance_type\":\"ml.p3.2xlarge\",\"hosts\":[\"algo-1\"],\"instance_groups\":[{\"hosts\":[\"algo-1\"],\"instance_group_name\":\"homogeneousCluster\",\"instance_type\":\"ml.p3.2xlarge\"}],\"network_interface_name\":\"eth0\"},\"user_entry_point\":\"run_train.py\"}\u001b[0m\n",
      "\u001b[34mSM_USER_ARGS=[\"--do_eval\",\"True\",\"--do_train\",\"True\",\"--eval_steps\",\"5000\",\"--evaluation_strategy\",\"epoch\",\"--gradient_accumulation_steps\",\"2\",\"--hypothesis_column\",\"label\",\"--learning_rate\",\"0.0003\",\"--max_source_length\",\"128\",\"--max_target_length\",\"128\",\"--model_name_or_path\",\"t5-base\",\"--num_train_epochs\",\"10\",\"--output_dir\",\"/opt/ml/model\",\"--per_device_train_batch_size\",\"2\",\"--reference_column\",\"sentence1_key\",\"--save_strategy\",\"steps\",\"--save_total_limit\",\"1\",\"--test_file\",\"/opt/ml/input/data/test/test.csv\",\"--train_file\",\"/opt/ml/input/data/train/train.csv\",\"--validation_file\",\"/opt/ml/input/data/validation/test.csv\"]\u001b[0m\n",
      "\u001b[34mSM_OUTPUT_INTERMEDIATE_DIR=/opt/ml/output/intermediate\u001b[0m\n",
      "\u001b[34mSM_CHANNEL_TEST=/opt/ml/input/data/test\u001b[0m\n",
      "\u001b[34mSM_CHANNEL_TRAIN=/opt/ml/input/data/train\u001b[0m\n",
      "\u001b[34mSM_CHANNEL_VALIDATION=/opt/ml/input/data/validation\u001b[0m\n",
      "\u001b[34mSM_HP_DO_EVAL=true\u001b[0m\n",
      "\u001b[34mSM_HP_DO_TRAIN=true\u001b[0m\n",
      "\u001b[34mSM_HP_EVAL_STEPS=5000\u001b[0m\n",
      "\u001b[34mSM_HP_EVALUATION_STRATEGY=epoch\u001b[0m\n",
      "\u001b[34mSM_HP_GRADIENT_ACCUMULATION_STEPS=2\u001b[0m\n",
      "\u001b[34mSM_HP_HYPOTHESIS_COLUMN=label\u001b[0m\n",
      "\u001b[34mSM_HP_LEARNING_RATE=0.0003\u001b[0m\n",
      "\u001b[34mSM_HP_MAX_SOURCE_LENGTH=128\u001b[0m\n",
      "\u001b[34mSM_HP_MAX_TARGET_LENGTH=128\u001b[0m\n",
      "\u001b[34mSM_HP_MODEL_NAME_OR_PATH=t5-base\u001b[0m\n",
      "\u001b[34mSM_HP_NUM_TRAIN_EPOCHS=10\u001b[0m\n",
      "\u001b[34mSM_HP_OUTPUT_DIR=/opt/ml/model\u001b[0m\n",
      "\u001b[34mSM_HP_PER_DEVICE_TRAIN_BATCH_SIZE=2\u001b[0m\n",
      "\u001b[34mSM_HP_REFERENCE_COLUMN=sentence1_key\u001b[0m\n",
      "\u001b[34mSM_HP_SAVE_STRATEGY=steps\u001b[0m\n",
      "\u001b[34mSM_HP_SAVE_TOTAL_LIMIT=1\u001b[0m\n",
      "\u001b[34mSM_HP_TEST_FILE=/opt/ml/input/data/test/test.csv\u001b[0m\n",
      "\u001b[34mSM_HP_TRAIN_FILE=/opt/ml/input/data/train/train.csv\u001b[0m\n",
      "\u001b[34mSM_HP_VALIDATION_FILE=/opt/ml/input/data/validation/test.csv\u001b[0m\n",
      "\u001b[34mPYTHONPATH=/opt/ml/code:/opt/conda/bin:/opt/conda/lib/python36.zip:/opt/conda/lib/python3.6:/opt/conda/lib/python3.6/lib-dynload:/opt/conda/lib/python3.6/site-packages\u001b[0m\n",
      "\u001b[34mInvoking script with the following command:\u001b[0m\n",
      "\u001b[34m/opt/conda/bin/python3.6 run_train.py --do_eval True --do_train True --eval_steps 5000 --evaluation_strategy epoch --gradient_accumulation_steps 2 --hypothesis_column label --learning_rate 0.0003 --max_source_length 128 --max_target_length 128 --model_name_or_path t5-base --num_train_epochs 10 --output_dir /opt/ml/model --per_device_train_batch_size 2 --reference_column sentence1_key --save_strategy steps --save_total_limit 1 --test_file /opt/ml/input/data/test/test.csv --train_file /opt/ml/input/data/train/train.csv --validation_file /opt/ml/input/data/validation/test.csv\u001b[0m\n",
      "\u001b[34m07/14/2022 06:32:53 - WARNING - __main__ -   Process rank: -1, device: cuda:0, n_gpu: 1distributed training: False, 16-bits training: False\u001b[0m\n",
      "\u001b[34m07/14/2022 06:32:53 - INFO - __main__ -   Training/evaluation parameters Seq2SeqTrainingArguments(output_dir='/opt/ml/model', overwrite_output_dir=False, do_train=True, do_eval=True, do_predict=False, evaluation_strategy=<IntervalStrategy.EPOCH: 'epoch'>, prediction_loss_only=False, per_device_train_batch_size=2, per_device_eval_batch_size=8, per_gpu_train_batch_size=None, per_gpu_eval_batch_size=None, gradient_accumulation_steps=2, eval_accumulation_steps=None, learning_rate=0.0003, weight_decay=0.0, adam_beta1=0.9, adam_beta2=0.999, adam_epsilon=1e-08, max_grad_norm=1.0, num_train_epochs=10.0, max_steps=-1, lr_scheduler_type=<SchedulerType.LINEAR: 'linear'>, warmup_ratio=0.0, warmup_steps=0, logging_dir='runs/Jul14_06-32-53_algo-1', logging_strategy=<IntervalStrategy.STEPS: 'steps'>, logging_first_step=False, logging_steps=500, save_strategy=<IntervalStrategy.STEPS: 'steps'>, save_steps=500, save_total_limit=1, no_cuda=False, seed=42, fp16=False, fp16_opt_level='O1', fp16_backend='auto', fp16_full_eval=False, local_rank=-1, tpu_num_cores=None, tpu_metrics_debug=False, debug=[], dataloader_drop_last=False, eval_steps=5000, dataloader_num_workers=0, past_index=-1, run_name='/opt/ml/model', disable_tqdm=False, remove_unused_columns=True, label_names=None, load_best_model_at_end=False, metric_for_best_model=None, greater_is_better=None, ignore_data_skip=False, sharded_ddp=[], deepspeed=None, label_smoothing_factor=0.0, adafactor=False, group_by_length=False, length_column_name='length', report_to=[], ddp_find_unused_parameters=None, dataloader_pin_memory=True, skip_memory_metrics=False, use_legacy_prediction_loop=False, push_to_hub=False, resume_from_checkpoint=None, mp_parameters='', sortish_sampler=False, predict_with_generate=False)\u001b[0m\n",
      "\u001b[34m07/14/2022 06:32:53 - WARNING - datasets.builder -   Using custom data configuration default-fb9395ac89ccbadd\u001b[0m\n",
      "\u001b[34mDownloading and preparing dataset csv/default (download: Unknown size, generated: Unknown size, post-processed: Unknown size, total: Unknown size) to /root/.cache/huggingface/datasets/csv/default-fb9395ac89ccbadd/0.0.0/2dc6629a9ff6b5697d82c25b73731dd440507a69cbce8b425db50b751e8fcfd0...\u001b[0m\n",
      "\u001b[34mDataset csv downloaded and prepared to /root/.cache/huggingface/datasets/csv/default-fb9395ac89ccbadd/0.0.0/2dc6629a9ff6b5697d82c25b73731dd440507a69cbce8b425db50b751e8fcfd0. Subsequent calls will reuse this data.\u001b[0m\n",
      "\u001b[34mhttps://huggingface.co/t5-base/resolve/main/config.json not found in cache or force_download set to True, downloading to /root/.cache/huggingface/transformers/tmplp6re3d4\u001b[0m\n",
      "\u001b[34mstoring https://huggingface.co/t5-base/resolve/main/config.json in cache at /root/.cache/huggingface/transformers/91e9fe874e06c44883b535d6c950b8b89d6eaa3298d8e7fb3b2c78039e9f8b7b.66b9637a52aa11e9285cdd6e668cc0df14b3bcf0b6674cf3ba5353c542649637\u001b[0m\n",
      "\u001b[34mcreating metadata file for /root/.cache/huggingface/transformers/91e9fe874e06c44883b535d6c950b8b89d6eaa3298d8e7fb3b2c78039e9f8b7b.66b9637a52aa11e9285cdd6e668cc0df14b3bcf0b6674cf3ba5353c542649637\u001b[0m\n",
      "\u001b[34mloading configuration file https://huggingface.co/t5-base/resolve/main/config.json from cache at /root/.cache/huggingface/transformers/91e9fe874e06c44883b535d6c950b8b89d6eaa3298d8e7fb3b2c78039e9f8b7b.66b9637a52aa11e9285cdd6e668cc0df14b3bcf0b6674cf3ba5353c542649637\u001b[0m\n",
      "\u001b[34mModel config T5Config {\n",
      "  \"architectures\": [\n",
      "    \"T5WithLMHeadModel\"\n",
      "  ],\n",
      "  \"d_ff\": 3072,\n",
      "  \"d_kv\": 64,\n",
      "  \"d_model\": 768,\n",
      "  \"decoder_start_token_id\": 0,\n",
      "  \"dropout_rate\": 0.1,\n",
      "  \"eos_token_id\": 1,\n",
      "  \"feed_forward_proj\": \"relu\",\n",
      "  \"gradient_checkpointing\": false,\n",
      "  \"initializer_factor\": 1.0,\n",
      "  \"is_encoder_decoder\": true,\n",
      "  \"layer_norm_epsilon\": 1e-06,\n",
      "  \"model_type\": \"t5\",\n",
      "  \"n_positions\": 512,\n",
      "  \"num_decoder_layers\": 12,\n",
      "  \"num_heads\": 12,\n",
      "  \"num_layers\": 12,\n",
      "  \"output_past\": true,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"relative_attention_num_buckets\": 32,\n",
      "  \"task_specific_params\": {\n",
      "    \"summarization\": {\n",
      "      \"early_stopping\": true,\n",
      "      \"length_penalty\": 2.0,\n",
      "      \"max_length\": 200,\n",
      "      \"min_length\": 30,\n",
      "      \"no_repeat_ngram_size\": 3,\n",
      "      \"num_beams\": 4,\n",
      "      \"prefix\": \"summarize: \"\n",
      "    },\n",
      "    \"translation_en_to_de\": {\n",
      "      \"early_stopping\": true,\n",
      "      \"max_length\": 300,\n",
      "      \"num_beams\": 4,\n",
      "      \"prefix\": \"translate English to German: \"\n",
      "    },\n",
      "    \"translation_en_to_fr\": {\n",
      "      \"early_stopping\": true,\n",
      "      \"max_length\": 300,\n",
      "      \"num_beams\": 4,\n",
      "      \"prefix\": \"translate English to French: \"\n",
      "    },\n",
      "    \"translation_en_to_ro\": {\n",
      "      \"early_stopping\": true,\n",
      "      \"max_length\": 300,\n",
      "      \"num_beams\": 4,\n",
      "      \"prefix\": \"translate English to Romanian: \"\n",
      "    }\n",
      "  },\n",
      "  \"transformers_version\": \"4.6.1\",\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 32128\u001b[0m\n",
      "\u001b[34m}\u001b[0m\n",
      "\u001b[34mloading configuration file https://huggingface.co/t5-base/resolve/main/config.json from cache at /root/.cache/huggingface/transformers/91e9fe874e06c44883b535d6c950b8b89d6eaa3298d8e7fb3b2c78039e9f8b7b.66b9637a52aa11e9285cdd6e668cc0df14b3bcf0b6674cf3ba5353c542649637\u001b[0m\n",
      "\u001b[34mModel config T5Config {\n",
      "  \"architectures\": [\n",
      "    \"T5WithLMHeadModel\"\n",
      "  ],\n",
      "  \"d_ff\": 3072,\n",
      "  \"d_kv\": 64,\n",
      "  \"d_model\": 768,\n",
      "  \"decoder_start_token_id\": 0,\n",
      "  \"dropout_rate\": 0.1,\n",
      "  \"eos_token_id\": 1,\n",
      "  \"feed_forward_proj\": \"relu\",\n",
      "  \"gradient_checkpointing\": false,\n",
      "  \"initializer_factor\": 1.0,\n",
      "  \"is_encoder_decoder\": true,\n",
      "  \"layer_norm_epsilon\": 1e-06,\n",
      "  \"model_type\": \"t5\",\n",
      "  \"n_positions\": 512,\n",
      "  \"num_decoder_layers\": 12,\n",
      "  \"num_heads\": 12,\n",
      "  \"num_layers\": 12,\n",
      "  \"output_past\": true,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"relative_attention_num_buckets\": 32,\n",
      "  \"task_specific_params\": {\n",
      "    \"summarization\": {\n",
      "      \"early_stopping\": true,\n",
      "      \"length_penalty\": 2.0,\n",
      "      \"max_length\": 200,\n",
      "      \"min_length\": 30,\n",
      "      \"no_repeat_ngram_size\": 3,\n",
      "      \"num_beams\": 4,\n",
      "      \"prefix\": \"summarize: \"\n",
      "    },\n",
      "    \"translation_en_to_de\": {\n",
      "      \"early_stopping\": true,\n",
      "      \"max_length\": 300,\n",
      "      \"num_beams\": 4,\n",
      "      \"prefix\": \"translate English to German: \"\n",
      "    },\n",
      "    \"translation_en_to_fr\": {\n",
      "      \"early_stopping\": true,\n",
      "      \"max_length\": 300,\n",
      "      \"num_beams\": 4,\n",
      "      \"prefix\": \"translate English to French: \"\n",
      "    },\n",
      "    \"translation_en_to_ro\": {\n",
      "      \"early_stopping\": true,\n",
      "      \"max_length\": 300,\n",
      "      \"num_beams\": 4,\n",
      "      \"prefix\": \"translate English to Romanian: \"\n",
      "    }\n",
      "  },\n",
      "  \"transformers_version\": \"4.6.1\",\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 32128\u001b[0m\n",
      "\u001b[34m}\u001b[0m\n",
      "\u001b[34mhttps://huggingface.co/t5-base/resolve/main/spiece.model not found in cache or force_download set to True, downloading to /root/.cache/huggingface/transformers/tmpqamky2ex\u001b[0m\n",
      "\u001b[34mstoring https://huggingface.co/t5-base/resolve/main/spiece.model in cache at /root/.cache/huggingface/transformers/684a47ca6257e4ca71f0037771464c5b323e945fbc58697d2fad8a7dd1a2f8ba.3b69006860e7b5d0a63ffdddc01ddcd6b7c318a6f4fd793596552c741734c62d\u001b[0m\n",
      "\u001b[34mcreating metadata file for /root/.cache/huggingface/transformers/684a47ca6257e4ca71f0037771464c5b323e945fbc58697d2fad8a7dd1a2f8ba.3b69006860e7b5d0a63ffdddc01ddcd6b7c318a6f4fd793596552c741734c62d\u001b[0m\n",
      "\u001b[34mhttps://huggingface.co/t5-base/resolve/main/tokenizer.json not found in cache or force_download set to True, downloading to /root/.cache/huggingface/transformers/tmp1vj3xhme\u001b[0m\n",
      "\u001b[34mstoring https://huggingface.co/t5-base/resolve/main/tokenizer.json in cache at /root/.cache/huggingface/transformers/90de37880b5ff5ac7ab70ff0bd369f207e9b74133fa153c163d14c5bb0116207.8627f1bd5d270a9fd2e5a51c8bec3223896587cc3cfe13edeabb0992ab43c529\u001b[0m\n",
      "\u001b[34mcreating metadata file for /root/.cache/huggingface/transformers/90de37880b5ff5ac7ab70ff0bd369f207e9b74133fa153c163d14c5bb0116207.8627f1bd5d270a9fd2e5a51c8bec3223896587cc3cfe13edeabb0992ab43c529\u001b[0m\n",
      "\u001b[34mloading file https://huggingface.co/t5-base/resolve/main/spiece.model from cache at /root/.cache/huggingface/transformers/684a47ca6257e4ca71f0037771464c5b323e945fbc58697d2fad8a7dd1a2f8ba.3b69006860e7b5d0a63ffdddc01ddcd6b7c318a6f4fd793596552c741734c62d\u001b[0m\n",
      "\u001b[34mloading file https://huggingface.co/t5-base/resolve/main/tokenizer.json from cache at /root/.cache/huggingface/transformers/90de37880b5ff5ac7ab70ff0bd369f207e9b74133fa153c163d14c5bb0116207.8627f1bd5d270a9fd2e5a51c8bec3223896587cc3cfe13edeabb0992ab43c529\u001b[0m\n",
      "\u001b[34mloading file https://huggingface.co/t5-base/resolve/main/added_tokens.json from cache at None\u001b[0m\n",
      "\u001b[34mloading file https://huggingface.co/t5-base/resolve/main/special_tokens_map.json from cache at None\u001b[0m\n",
      "\u001b[34mloading file https://huggingface.co/t5-base/resolve/main/tokenizer_config.json from cache at None\u001b[0m\n",
      "\u001b[34mhttps://huggingface.co/t5-base/resolve/main/pytorch_model.bin not found in cache or force_download set to True, downloading to /root/.cache/huggingface/transformers/tmpqidctncg\u001b[0m\n",
      "\u001b[34mstoring https://huggingface.co/t5-base/resolve/main/pytorch_model.bin in cache at /root/.cache/huggingface/transformers/ab4e948915b067f5cb6e5105f6f85044fd717b133f43240db67899a8fc7b29a2.26934c75adf19ceac3c268b721ba353356b7609c45f5627550326f275a2163b4\u001b[0m\n",
      "\u001b[34mcreating metadata file for /root/.cache/huggingface/transformers/ab4e948915b067f5cb6e5105f6f85044fd717b133f43240db67899a8fc7b29a2.26934c75adf19ceac3c268b721ba353356b7609c45f5627550326f275a2163b4\u001b[0m\n",
      "\u001b[34mloading weights file https://huggingface.co/t5-base/resolve/main/pytorch_model.bin from cache at /root/.cache/huggingface/transformers/ab4e948915b067f5cb6e5105f6f85044fd717b133f43240db67899a8fc7b29a2.26934c75adf19ceac3c268b721ba353356b7609c45f5627550326f275a2163b4\u001b[0m\n",
      "\u001b[34mAll model checkpoint weights were used when initializing T5ForConditionalGeneration.\u001b[0m\n",
      "\u001b[34mAll the weights of T5ForConditionalGeneration were initialized from the model checkpoint at t5-base.\u001b[0m\n",
      "\u001b[34mIf your task is similar to the task the model of the checkpoint was trained on, you can already use T5ForConditionalGeneration for predictions without further training.\u001b[0m\n",
      "\u001b[34m***** Running training *****\n",
      "  Num examples = 2508\n",
      "  Num Epochs = 10\n",
      "  Instantaneous batch size per device = 2\n",
      "  Total train batch size (w. parallel, distributed & accumulation) = 4\n",
      "  Gradient Accumulation steps = 2\n",
      "  Total optimization steps = 6270\u001b[0m\n",
      "\u001b[34m[2022-07-14 06:33:21.055 algo-1:32 INFO utils.py:27] RULE_JOB_STOP_SIGNAL_FILENAME: None\u001b[0m\n",
      "\u001b[34m[2022-07-14 06:33:21.212 algo-1:32 INFO profiler_config_parser.py:102] User has disabled profiler.\u001b[0m\n",
      "\u001b[34m[2022-07-14 06:33:21.213 algo-1:32 INFO json_config.py:91] Creating hook from json_config at /opt/ml/input/config/debughookconfig.json.\u001b[0m\n",
      "\u001b[34m[2022-07-14 06:33:21.214 algo-1:32 INFO hook.py:201] tensorboard_dir has not been set for the hook. SMDebug will not be exporting tensorboard summaries.\u001b[0m\n",
      "\u001b[34m[2022-07-14 06:33:21.215 algo-1:32 INFO hook.py:255] Saving to /opt/ml/output/tensors\u001b[0m\n",
      "\u001b[34m[2022-07-14 06:33:21.215 algo-1:32 INFO state_store.py:77] The checkpoint config file /opt/ml/input/config/checkpointconfig.json does not exist.\u001b[0m\n",
      "\u001b[34m[2022-07-14 06:33:21.409 algo-1:32 INFO hook.py:591] name:shared.weight count_params:24652800\u001b[0m\n",
      "\u001b[34m[2022-07-14 06:33:21.409 algo-1:32 INFO hook.py:591] name:encoder.block.0.layer.0.SelfAttention.q.weight count_params:589824\u001b[0m\n",
      "\u001b[34m[2022-07-14 06:33:21.409 algo-1:32 INFO hook.py:591] name:encoder.block.0.layer.0.SelfAttention.k.weight count_params:589824\u001b[0m\n",
      "\u001b[34m[2022-07-14 06:33:21.409 algo-1:32 INFO hook.py:591] name:encoder.block.0.layer.0.SelfAttention.v.weight count_params:589824\u001b[0m\n",
      "\u001b[34m[2022-07-14 06:33:21.409 algo-1:32 INFO hook.py:591] name:encoder.block.0.layer.0.SelfAttention.o.weight count_params:589824\u001b[0m\n",
      "\u001b[34m[2022-07-14 06:33:21.410 algo-1:32 INFO hook.py:591] name:encoder.block.0.layer.0.SelfAttention.relative_attention_bias.weight count_params:384\u001b[0m\n",
      "\u001b[34m[2022-07-14 06:33:21.410 algo-1:32 INFO hook.py:591] name:encoder.block.0.layer.0.layer_norm.weight count_params:768\u001b[0m\n",
      "\u001b[34m[2022-07-14 06:33:21.410 algo-1:32 INFO hook.py:591] name:encoder.block.0.layer.1.DenseReluDense.wi.weight count_params:2359296\u001b[0m\n",
      "\u001b[34m[2022-07-14 06:33:21.410 algo-1:32 INFO hook.py:591] name:encoder.block.0.layer.1.DenseReluDense.wo.weight count_params:2359296\u001b[0m\n",
      "\u001b[34m[2022-07-14 06:33:21.410 algo-1:32 INFO hook.py:591] name:encoder.block.0.layer.1.layer_norm.weight count_params:768\u001b[0m\n",
      "\u001b[34m[2022-07-14 06:33:21.410 algo-1:32 INFO hook.py:591] name:encoder.block.1.layer.0.SelfAttention.q.weight count_params:589824\u001b[0m\n",
      "\u001b[34m[2022-07-14 06:33:21.410 algo-1:32 INFO hook.py:591] name:encoder.block.1.layer.0.SelfAttention.k.weight count_params:589824\u001b[0m\n",
      "\u001b[34m[2022-07-14 06:33:21.410 algo-1:32 INFO hook.py:591] name:encoder.block.1.layer.0.SelfAttention.v.weight count_params:589824\u001b[0m\n",
      "\u001b[34m[2022-07-14 06:33:21.411 algo-1:32 INFO hook.py:591] name:encoder.block.1.layer.0.SelfAttention.o.weight count_params:589824\u001b[0m\n",
      "\u001b[34m[2022-07-14 06:33:21.411 algo-1:32 INFO hook.py:591] name:encoder.block.1.layer.0.layer_norm.weight count_params:768\u001b[0m\n",
      "\u001b[34m[2022-07-14 06:33:21.411 algo-1:32 INFO hook.py:591] name:encoder.block.1.layer.1.DenseReluDense.wi.weight count_params:2359296\u001b[0m\n",
      "\u001b[34m[2022-07-14 06:33:21.411 algo-1:32 INFO hook.py:591] name:encoder.block.1.layer.1.DenseReluDense.wo.weight count_params:2359296\u001b[0m\n",
      "\u001b[34m[2022-07-14 06:33:21.411 algo-1:32 INFO hook.py:591] name:encoder.block.1.layer.1.layer_norm.weight count_params:768\u001b[0m\n",
      "\u001b[34m[2022-07-14 06:33:21.411 algo-1:32 INFO hook.py:591] name:encoder.block.2.layer.0.SelfAttention.q.weight count_params:589824\u001b[0m\n",
      "\u001b[34m[2022-07-14 06:33:21.411 algo-1:32 INFO hook.py:591] name:encoder.block.2.layer.0.SelfAttention.k.weight count_params:589824\u001b[0m\n",
      "\u001b[34m[2022-07-14 06:33:21.411 algo-1:32 INFO hook.py:591] name:encoder.block.2.layer.0.SelfAttention.v.weight count_params:589824\u001b[0m\n",
      "\u001b[34m[2022-07-14 06:33:21.411 algo-1:32 INFO hook.py:591] name:encoder.block.2.layer.0.SelfAttention.o.weight count_params:589824\u001b[0m\n",
      "\u001b[34m[2022-07-14 06:33:21.411 algo-1:32 INFO hook.py:591] name:encoder.block.2.layer.0.layer_norm.weight count_params:768\u001b[0m\n",
      "\u001b[34m[2022-07-14 06:33:21.412 algo-1:32 INFO hook.py:591] name:encoder.block.2.layer.1.DenseReluDense.wi.weight count_params:2359296\u001b[0m\n",
      "\u001b[34m[2022-07-14 06:33:21.412 algo-1:32 INFO hook.py:591] name:encoder.block.2.layer.1.DenseReluDense.wo.weight count_params:2359296\u001b[0m\n",
      "\u001b[34m[2022-07-14 06:33:21.412 algo-1:32 INFO hook.py:591] name:encoder.block.2.layer.1.layer_norm.weight count_params:768\u001b[0m\n",
      "\u001b[34m[2022-07-14 06:33:21.412 algo-1:32 INFO hook.py:591] name:encoder.block.3.layer.0.SelfAttention.q.weight count_params:589824\u001b[0m\n",
      "\u001b[34m[2022-07-14 06:33:21.412 algo-1:32 INFO hook.py:591] name:encoder.block.3.layer.0.SelfAttention.k.weight count_params:589824\u001b[0m\n",
      "\u001b[34m[2022-07-14 06:33:21.412 algo-1:32 INFO hook.py:591] name:encoder.block.3.layer.0.SelfAttention.v.weight count_params:589824\u001b[0m\n",
      "\u001b[34m[2022-07-14 06:33:21.412 algo-1:32 INFO hook.py:591] name:encoder.block.3.layer.0.SelfAttention.o.weight count_params:589824\u001b[0m\n",
      "\u001b[34m[2022-07-14 06:33:21.413 algo-1:32 INFO hook.py:591] name:encoder.block.3.layer.0.layer_norm.weight count_params:768\u001b[0m\n",
      "\u001b[34m[2022-07-14 06:33:21.413 algo-1:32 INFO hook.py:591] name:encoder.block.3.layer.1.DenseReluDense.wi.weight count_params:2359296\u001b[0m\n",
      "\u001b[34m[2022-07-14 06:33:21.413 algo-1:32 INFO hook.py:591] name:encoder.block.3.layer.1.DenseReluDense.wo.weight count_params:2359296\u001b[0m\n",
      "\u001b[34m[2022-07-14 06:33:21.413 algo-1:32 INFO hook.py:591] name:encoder.block.3.layer.1.layer_norm.weight count_params:768\u001b[0m\n",
      "\u001b[34m[2022-07-14 06:33:21.413 algo-1:32 INFO hook.py:591] name:encoder.block.4.layer.0.SelfAttention.q.weight count_params:589824\u001b[0m\n",
      "\u001b[34m[2022-07-14 06:33:21.413 algo-1:32 INFO hook.py:591] name:encoder.block.4.layer.0.SelfAttention.k.weight count_params:589824\u001b[0m\n",
      "\u001b[34m[2022-07-14 06:33:21.413 algo-1:32 INFO hook.py:591] name:encoder.block.4.layer.0.SelfAttention.v.weight count_params:589824\u001b[0m\n",
      "\u001b[34m[2022-07-14 06:33:21.413 algo-1:32 INFO hook.py:591] name:encoder.block.4.layer.0.SelfAttention.o.weight count_params:589824\u001b[0m\n",
      "\u001b[34m[2022-07-14 06:33:21.414 algo-1:32 INFO hook.py:591] name:encoder.block.4.layer.0.layer_norm.weight count_params:768\u001b[0m\n",
      "\u001b[34m[2022-07-14 06:33:21.414 algo-1:32 INFO hook.py:591] name:encoder.block.4.layer.1.DenseReluDense.wi.weight count_params:2359296\u001b[0m\n",
      "\u001b[34m[2022-07-14 06:33:21.414 algo-1:32 INFO hook.py:591] name:encoder.block.4.layer.1.DenseReluDense.wo.weight count_params:2359296\u001b[0m\n",
      "\u001b[34m[2022-07-14 06:33:21.414 algo-1:32 INFO hook.py:591] name:encoder.block.4.layer.1.layer_norm.weight count_params:768\u001b[0m\n",
      "\u001b[34m[2022-07-14 06:33:21.414 algo-1:32 INFO hook.py:591] name:encoder.block.5.layer.0.SelfAttention.q.weight count_params:589824\u001b[0m\n",
      "\u001b[34m[2022-07-14 06:33:21.414 algo-1:32 INFO hook.py:591] name:encoder.block.5.layer.0.SelfAttention.k.weight count_params:589824\u001b[0m\n",
      "\u001b[34m[2022-07-14 06:33:21.415 algo-1:32 INFO hook.py:591] name:encoder.block.5.layer.0.SelfAttention.v.weight count_params:589824\u001b[0m\n",
      "\u001b[34m[2022-07-14 06:33:21.415 algo-1:32 INFO hook.py:591] name:encoder.block.5.layer.0.SelfAttention.o.weight count_params:589824\u001b[0m\n",
      "\u001b[34m[2022-07-14 06:33:21.415 algo-1:32 INFO hook.py:591] name:encoder.block.5.layer.0.layer_norm.weight count_params:768\u001b[0m\n",
      "\u001b[34m[2022-07-14 06:33:21.415 algo-1:32 INFO hook.py:591] name:encoder.block.5.layer.1.DenseReluDense.wi.weight count_params:2359296\u001b[0m\n",
      "\u001b[34m[2022-07-14 06:33:21.416 algo-1:32 INFO hook.py:591] name:encoder.block.5.layer.1.DenseReluDense.wo.weight count_params:2359296\u001b[0m\n",
      "\u001b[34m[2022-07-14 06:33:21.416 algo-1:32 INFO hook.py:591] name:encoder.block.5.layer.1.layer_norm.weight count_params:768\u001b[0m\n",
      "\u001b[34m[2022-07-14 06:33:21.416 algo-1:32 INFO hook.py:591] name:encoder.block.6.layer.0.SelfAttention.q.weight count_params:589824\u001b[0m\n",
      "\u001b[34m[2022-07-14 06:33:21.416 algo-1:32 INFO hook.py:591] name:encoder.block.6.layer.0.SelfAttention.k.weight count_params:589824\u001b[0m\n",
      "\u001b[34m[2022-07-14 06:33:21.416 algo-1:32 INFO hook.py:591] name:encoder.block.6.layer.0.SelfAttention.v.weight count_params:589824\u001b[0m\n",
      "\u001b[34m[2022-07-14 06:33:21.416 algo-1:32 INFO hook.py:591] name:encoder.block.6.layer.0.SelfAttention.o.weight count_params:589824\u001b[0m\n",
      "\u001b[34m[2022-07-14 06:33:21.416 algo-1:32 INFO hook.py:591] name:encoder.block.6.layer.0.layer_norm.weight count_params:768\u001b[0m\n",
      "\u001b[34m[2022-07-14 06:33:21.416 algo-1:32 INFO hook.py:591] name:encoder.block.6.layer.1.DenseReluDense.wi.weight count_params:2359296\u001b[0m\n",
      "\u001b[34m[2022-07-14 06:33:21.416 algo-1:32 INFO hook.py:591] name:encoder.block.6.layer.1.DenseReluDense.wo.weight count_params:2359296\u001b[0m\n",
      "\u001b[34m[2022-07-14 06:33:21.417 algo-1:32 INFO hook.py:591] name:encoder.block.6.layer.1.layer_norm.weight count_params:768\u001b[0m\n",
      "\u001b[34m[2022-07-14 06:33:21.417 algo-1:32 INFO hook.py:591] name:encoder.block.7.layer.0.SelfAttention.q.weight count_params:589824\u001b[0m\n",
      "\u001b[34m[2022-07-14 06:33:21.417 algo-1:32 INFO hook.py:591] name:encoder.block.7.layer.0.SelfAttention.k.weight count_params:589824\u001b[0m\n",
      "\u001b[34m[2022-07-14 06:33:21.417 algo-1:32 INFO hook.py:591] name:encoder.block.7.layer.0.SelfAttention.v.weight count_params:589824\u001b[0m\n",
      "\u001b[34m[2022-07-14 06:33:21.417 algo-1:32 INFO hook.py:591] name:encoder.block.7.layer.0.SelfAttention.o.weight count_params:589824\u001b[0m\n",
      "\u001b[34m[2022-07-14 06:33:21.417 algo-1:32 INFO hook.py:591] name:encoder.block.7.layer.0.layer_norm.weight count_params:768\u001b[0m\n",
      "\u001b[34m[2022-07-14 06:33:21.417 algo-1:32 INFO hook.py:591] name:encoder.block.7.layer.1.DenseReluDense.wi.weight count_params:2359296\u001b[0m\n",
      "\u001b[34m[2022-07-14 06:33:21.417 algo-1:32 INFO hook.py:591] name:encoder.block.7.layer.1.DenseReluDense.wo.weight count_params:2359296\u001b[0m\n",
      "\u001b[34m[2022-07-14 06:33:21.417 algo-1:32 INFO hook.py:591] name:encoder.block.7.layer.1.layer_norm.weight count_params:768\u001b[0m\n",
      "\u001b[34m[2022-07-14 06:33:21.417 algo-1:32 INFO hook.py:591] name:encoder.block.8.layer.0.SelfAttention.q.weight count_params:589824\u001b[0m\n",
      "\u001b[34m[2022-07-14 06:33:21.417 algo-1:32 INFO hook.py:591] name:encoder.block.8.layer.0.SelfAttention.k.weight count_params:589824\u001b[0m\n",
      "\u001b[34m[2022-07-14 06:33:21.417 algo-1:32 INFO hook.py:591] name:encoder.block.8.layer.0.SelfAttention.v.weight count_params:589824\u001b[0m\n",
      "\u001b[34m[2022-07-14 06:33:21.417 algo-1:32 INFO hook.py:591] name:encoder.block.8.layer.0.SelfAttention.o.weight count_params:589824\u001b[0m\n",
      "\u001b[34m[2022-07-14 06:33:21.418 algo-1:32 INFO hook.py:591] name:encoder.block.8.layer.0.layer_norm.weight count_params:768\u001b[0m\n",
      "\u001b[34m[2022-07-14 06:33:21.418 algo-1:32 INFO hook.py:591] name:encoder.block.8.layer.1.DenseReluDense.wi.weight count_params:2359296\u001b[0m\n",
      "\u001b[34m[2022-07-14 06:33:21.418 algo-1:32 INFO hook.py:591] name:encoder.block.8.layer.1.DenseReluDense.wo.weight count_params:2359296\u001b[0m\n",
      "\u001b[34m[2022-07-14 06:33:21.418 algo-1:32 INFO hook.py:591] name:encoder.block.8.layer.1.layer_norm.weight count_params:768\u001b[0m\n",
      "\u001b[34m[2022-07-14 06:33:21.418 algo-1:32 INFO hook.py:591] name:encoder.block.9.layer.0.SelfAttention.q.weight count_params:589824\u001b[0m\n",
      "\u001b[34m[2022-07-14 06:33:21.418 algo-1:32 INFO hook.py:591] name:encoder.block.9.layer.0.SelfAttention.k.weight count_params:589824\u001b[0m\n",
      "\u001b[34m[2022-07-14 06:33:21.418 algo-1:32 INFO hook.py:591] name:encoder.block.9.layer.0.SelfAttention.v.weight count_params:589824\u001b[0m\n",
      "\u001b[34m[2022-07-14 06:33:21.418 algo-1:32 INFO hook.py:591] name:encoder.block.9.layer.0.SelfAttention.o.weight count_params:589824\u001b[0m\n",
      "\u001b[34m[2022-07-14 06:33:21.418 algo-1:32 INFO hook.py:591] name:encoder.block.9.layer.0.layer_norm.weight count_params:768\u001b[0m\n",
      "\u001b[34m[2022-07-14 06:33:21.418 algo-1:32 INFO hook.py:591] name:encoder.block.9.layer.1.DenseReluDense.wi.weight count_params:2359296\u001b[0m\n",
      "\u001b[34m[2022-07-14 06:33:21.418 algo-1:32 INFO hook.py:591] name:encoder.block.9.layer.1.DenseReluDense.wo.weight count_params:2359296\u001b[0m\n",
      "\u001b[34m[2022-07-14 06:33:21.418 algo-1:32 INFO hook.py:591] name:encoder.block.9.layer.1.layer_norm.weight count_params:768\u001b[0m\n",
      "\u001b[34m[2022-07-14 06:33:21.418 algo-1:32 INFO hook.py:591] name:encoder.block.10.layer.0.SelfAttention.q.weight count_params:589824\u001b[0m\n",
      "\u001b[34m[2022-07-14 06:33:21.418 algo-1:32 INFO hook.py:591] name:encoder.block.10.layer.0.SelfAttention.k.weight count_params:589824\u001b[0m\n",
      "\u001b[34m[2022-07-14 06:33:21.419 algo-1:32 INFO hook.py:591] name:encoder.block.10.layer.0.SelfAttention.v.weight count_params:589824\u001b[0m\n",
      "\u001b[34m[2022-07-14 06:33:21.419 algo-1:32 INFO hook.py:591] name:encoder.block.10.layer.0.SelfAttention.o.weight count_params:589824\u001b[0m\n",
      "\u001b[34m[2022-07-14 06:33:21.419 algo-1:32 INFO hook.py:591] name:encoder.block.10.layer.0.layer_norm.weight count_params:768\u001b[0m\n",
      "\u001b[34m[2022-07-14 06:33:21.419 algo-1:32 INFO hook.py:591] name:encoder.block.10.layer.1.DenseReluDense.wi.weight count_params:2359296\u001b[0m\n",
      "\u001b[34m[2022-07-14 06:33:21.419 algo-1:32 INFO hook.py:591] name:encoder.block.10.layer.1.DenseReluDense.wo.weight count_params:2359296\u001b[0m\n",
      "\u001b[34m[2022-07-14 06:33:21.419 algo-1:32 INFO hook.py:591] name:encoder.block.10.layer.1.layer_norm.weight count_params:768\u001b[0m\n",
      "\u001b[34m[2022-07-14 06:33:21.419 algo-1:32 INFO hook.py:591] name:encoder.block.11.layer.0.SelfAttention.q.weight count_params:589824\u001b[0m\n",
      "\u001b[34m[2022-07-14 06:33:21.419 algo-1:32 INFO hook.py:591] name:encoder.block.11.layer.0.SelfAttention.k.weight count_params:589824\u001b[0m\n",
      "\u001b[34m[2022-07-14 06:33:21.419 algo-1:32 INFO hook.py:591] name:encoder.block.11.layer.0.SelfAttention.v.weight count_params:589824\u001b[0m\n",
      "\u001b[34m[2022-07-14 06:33:21.419 algo-1:32 INFO hook.py:591] name:encoder.block.11.layer.0.SelfAttention.o.weight count_params:589824\u001b[0m\n",
      "\u001b[34m[2022-07-14 06:33:21.419 algo-1:32 INFO hook.py:591] name:encoder.block.11.layer.0.layer_norm.weight count_params:768\u001b[0m\n",
      "\u001b[34m[2022-07-14 06:33:21.419 algo-1:32 INFO hook.py:591] name:encoder.block.11.layer.1.DenseReluDense.wi.weight count_params:2359296\u001b[0m\n",
      "\u001b[34m[2022-07-14 06:33:21.419 algo-1:32 INFO hook.py:591] name:encoder.block.11.layer.1.DenseReluDense.wo.weight count_params:2359296\u001b[0m\n",
      "\u001b[34m[2022-07-14 06:33:21.419 algo-1:32 INFO hook.py:591] name:encoder.block.11.layer.1.layer_norm.weight count_params:768\u001b[0m\n",
      "\u001b[34m[2022-07-14 06:33:21.420 algo-1:32 INFO hook.py:591] name:encoder.final_layer_norm.weight count_params:768\u001b[0m\n",
      "\u001b[34m[2022-07-14 06:33:21.420 algo-1:32 INFO hook.py:591] name:decoder.block.0.layer.0.SelfAttention.q.weight count_params:589824\u001b[0m\n",
      "\u001b[34m[2022-07-14 06:33:21.420 algo-1:32 INFO hook.py:591] name:decoder.block.0.layer.0.SelfAttention.k.weight count_params:589824\u001b[0m\n",
      "\u001b[34m[2022-07-14 06:33:21.420 algo-1:32 INFO hook.py:591] name:decoder.block.0.layer.0.SelfAttention.v.weight count_params:589824\u001b[0m\n",
      "\u001b[34m[2022-07-14 06:33:21.420 algo-1:32 INFO hook.py:591] name:decoder.block.0.layer.0.SelfAttention.o.weight count_params:589824\u001b[0m\n",
      "\u001b[34m[2022-07-14 06:33:21.420 algo-1:32 INFO hook.py:591] name:decoder.block.0.layer.0.SelfAttention.relative_attention_bias.weight count_params:384\u001b[0m\n",
      "\u001b[34m[2022-07-14 06:33:21.420 algo-1:32 INFO hook.py:591] name:decoder.block.0.layer.0.layer_norm.weight count_params:768\u001b[0m\n",
      "\u001b[34m[2022-07-14 06:33:21.420 algo-1:32 INFO hook.py:591] name:decoder.block.0.layer.1.EncDecAttention.q.weight count_params:589824\u001b[0m\n",
      "\u001b[34m[2022-07-14 06:33:21.421 algo-1:32 INFO hook.py:591] name:decoder.block.0.layer.1.EncDecAttention.k.weight count_params:589824\u001b[0m\n",
      "\u001b[34m[2022-07-14 06:33:21.421 algo-1:32 INFO hook.py:591] name:decoder.block.0.layer.1.EncDecAttention.v.weight count_params:589824\u001b[0m\n",
      "\u001b[34m[2022-07-14 06:33:21.421 algo-1:32 INFO hook.py:591] name:decoder.block.0.layer.1.EncDecAttention.o.weight count_params:589824\u001b[0m\n",
      "\u001b[34m[2022-07-14 06:33:21.421 algo-1:32 INFO hook.py:591] name:decoder.block.0.layer.1.layer_norm.weight count_params:768\u001b[0m\n",
      "\u001b[34m[2022-07-14 06:33:21.421 algo-1:32 INFO hook.py:591] name:decoder.block.0.layer.2.DenseReluDense.wi.weight count_params:2359296\u001b[0m\n",
      "\u001b[34m[2022-07-14 06:33:21.421 algo-1:32 INFO hook.py:591] name:decoder.block.0.layer.2.DenseReluDense.wo.weight count_params:2359296\u001b[0m\n",
      "\u001b[34m[2022-07-14 06:33:21.422 algo-1:32 INFO hook.py:591] name:decoder.block.0.layer.2.layer_norm.weight count_params:768\u001b[0m\n",
      "\u001b[34m[2022-07-14 06:33:21.422 algo-1:32 INFO hook.py:591] name:decoder.block.1.layer.0.SelfAttention.q.weight count_params:589824\u001b[0m\n",
      "\u001b[34m[2022-07-14 06:33:21.422 algo-1:32 INFO hook.py:591] name:decoder.block.1.layer.0.SelfAttention.k.weight count_params:589824\u001b[0m\n",
      "\u001b[34m[2022-07-14 06:33:21.422 algo-1:32 INFO hook.py:591] name:decoder.block.1.layer.0.SelfAttention.v.weight count_params:589824\u001b[0m\n",
      "\u001b[34m[2022-07-14 06:33:21.422 algo-1:32 INFO hook.py:591] name:decoder.block.1.layer.0.SelfAttention.o.weight count_params:589824\u001b[0m\n",
      "\u001b[34m[2022-07-14 06:33:21.422 algo-1:32 INFO hook.py:591] name:decoder.block.1.layer.0.layer_norm.weight count_params:768\u001b[0m\n",
      "\u001b[34m[2022-07-14 06:33:21.422 algo-1:32 INFO hook.py:591] name:decoder.block.1.layer.1.EncDecAttention.q.weight count_params:589824\u001b[0m\n",
      "\u001b[34m[2022-07-14 06:33:21.422 algo-1:32 INFO hook.py:591] name:decoder.block.1.layer.1.EncDecAttention.k.weight count_params:589824\u001b[0m\n",
      "\u001b[34m[2022-07-14 06:33:21.423 algo-1:32 INFO hook.py:591] name:decoder.block.1.layer.1.EncDecAttention.v.weight count_params:589824\u001b[0m\n",
      "\u001b[34m[2022-07-14 06:33:21.423 algo-1:32 INFO hook.py:591] name:decoder.block.1.layer.1.EncDecAttention.o.weight count_params:589824\u001b[0m\n",
      "\u001b[34m[2022-07-14 06:33:21.423 algo-1:32 INFO hook.py:591] name:decoder.block.1.layer.1.layer_norm.weight count_params:768\u001b[0m\n",
      "\u001b[34m[2022-07-14 06:33:21.423 algo-1:32 INFO hook.py:591] name:decoder.block.1.layer.2.DenseReluDense.wi.weight count_params:2359296\u001b[0m\n",
      "\u001b[34m[2022-07-14 06:33:21.423 algo-1:32 INFO hook.py:591] name:decoder.block.1.layer.2.DenseReluDense.wo.weight count_params:2359296\u001b[0m\n",
      "\u001b[34m[2022-07-14 06:33:21.423 algo-1:32 INFO hook.py:591] name:decoder.block.1.layer.2.layer_norm.weight count_params:768\u001b[0m\n",
      "\u001b[34m[2022-07-14 06:33:21.423 algo-1:32 INFO hook.py:591] name:decoder.block.2.layer.0.SelfAttention.q.weight count_params:589824\u001b[0m\n",
      "\u001b[34m[2022-07-14 06:33:21.423 algo-1:32 INFO hook.py:591] name:decoder.block.2.layer.0.SelfAttention.k.weight count_params:589824\u001b[0m\n",
      "\u001b[34m[2022-07-14 06:33:21.424 algo-1:32 INFO hook.py:591] name:decoder.block.2.layer.0.SelfAttention.v.weight count_params:589824\u001b[0m\n",
      "\u001b[34m[2022-07-14 06:33:21.424 algo-1:32 INFO hook.py:591] name:decoder.block.2.layer.0.SelfAttention.o.weight count_params:589824\u001b[0m\n",
      "\u001b[34m[2022-07-14 06:33:21.424 algo-1:32 INFO hook.py:591] name:decoder.block.2.layer.0.layer_norm.weight count_params:768\u001b[0m\n",
      "\u001b[34m[2022-07-14 06:33:21.424 algo-1:32 INFO hook.py:591] name:decoder.block.2.layer.1.EncDecAttention.q.weight count_params:589824\u001b[0m\n",
      "\u001b[34m[2022-07-14 06:33:21.424 algo-1:32 INFO hook.py:591] name:decoder.block.2.layer.1.EncDecAttention.k.weight count_params:589824\u001b[0m\n",
      "\u001b[34m[2022-07-14 06:33:21.424 algo-1:32 INFO hook.py:591] name:decoder.block.2.layer.1.EncDecAttention.v.weight count_params:589824\u001b[0m\n",
      "\u001b[34m[2022-07-14 06:33:21.424 algo-1:32 INFO hook.py:591] name:decoder.block.2.layer.1.EncDecAttention.o.weight count_params:589824\u001b[0m\n",
      "\u001b[34m[2022-07-14 06:33:21.425 algo-1:32 INFO hook.py:591] name:decoder.block.2.layer.1.layer_norm.weight count_params:768\u001b[0m\n",
      "\u001b[34m[2022-07-14 06:33:21.425 algo-1:32 INFO hook.py:591] name:decoder.block.2.layer.2.DenseReluDense.wi.weight count_params:2359296\u001b[0m\n",
      "\u001b[34m[2022-07-14 06:33:21.425 algo-1:32 INFO hook.py:591] name:decoder.block.2.layer.2.DenseReluDense.wo.weight count_params:2359296\u001b[0m\n",
      "\u001b[34m[2022-07-14 06:33:21.425 algo-1:32 INFO hook.py:591] name:decoder.block.2.layer.2.layer_norm.weight count_params:768\u001b[0m\n",
      "\u001b[34m[2022-07-14 06:33:21.425 algo-1:32 INFO hook.py:591] name:decoder.block.3.layer.0.SelfAttention.q.weight count_params:589824\u001b[0m\n",
      "\u001b[34m[2022-07-14 06:33:21.425 algo-1:32 INFO hook.py:591] name:decoder.block.3.layer.0.SelfAttention.k.weight count_params:589824\u001b[0m\n",
      "\u001b[34m[2022-07-14 06:33:21.425 algo-1:32 INFO hook.py:591] name:decoder.block.3.layer.0.SelfAttention.v.weight count_params:589824\u001b[0m\n",
      "\u001b[34m[2022-07-14 06:33:21.425 algo-1:32 INFO hook.py:591] name:decoder.block.3.layer.0.SelfAttention.o.weight count_params:589824\u001b[0m\n",
      "\u001b[34m[2022-07-14 06:33:21.426 algo-1:32 INFO hook.py:591] name:decoder.block.3.layer.0.layer_norm.weight count_params:768\u001b[0m\n",
      "\u001b[34m[2022-07-14 06:33:21.426 algo-1:32 INFO hook.py:591] name:decoder.block.3.layer.1.EncDecAttention.q.weight count_params:589824\u001b[0m\n",
      "\u001b[34m[2022-07-14 06:33:21.426 algo-1:32 INFO hook.py:591] name:decoder.block.3.layer.1.EncDecAttention.k.weight count_params:589824\u001b[0m\n",
      "\u001b[34m[2022-07-14 06:33:21.426 algo-1:32 INFO hook.py:591] name:decoder.block.3.layer.1.EncDecAttention.v.weight count_params:589824\u001b[0m\n",
      "\u001b[34m[2022-07-14 06:33:21.426 algo-1:32 INFO hook.py:591] name:decoder.block.3.layer.1.EncDecAttention.o.weight count_params:589824\u001b[0m\n",
      "\u001b[34m[2022-07-14 06:33:21.426 algo-1:32 INFO hook.py:591] name:decoder.block.3.layer.1.layer_norm.weight count_params:768\u001b[0m\n",
      "\u001b[34m[2022-07-14 06:33:21.427 algo-1:32 INFO hook.py:591] name:decoder.block.3.layer.2.DenseReluDense.wi.weight count_params:2359296\u001b[0m\n",
      "\u001b[34m[2022-07-14 06:33:21.427 algo-1:32 INFO hook.py:591] name:decoder.block.3.layer.2.DenseReluDense.wo.weight count_params:2359296\u001b[0m\n",
      "\u001b[34m[2022-07-14 06:33:21.427 algo-1:32 INFO hook.py:591] name:decoder.block.3.layer.2.layer_norm.weight count_params:768\u001b[0m\n",
      "\u001b[34m[2022-07-14 06:33:21.427 algo-1:32 INFO hook.py:591] name:decoder.block.4.layer.0.SelfAttention.q.weight count_params:589824\u001b[0m\n",
      "\u001b[34m[2022-07-14 06:33:21.427 algo-1:32 INFO hook.py:591] name:decoder.block.4.layer.0.SelfAttention.k.weight count_params:589824\u001b[0m\n",
      "\u001b[34m[2022-07-14 06:33:21.428 algo-1:32 INFO hook.py:591] name:decoder.block.4.layer.0.SelfAttention.v.weight count_params:589824\u001b[0m\n",
      "\u001b[34m[2022-07-14 06:33:21.428 algo-1:32 INFO hook.py:591] name:decoder.block.4.layer.0.SelfAttention.o.weight count_params:589824\u001b[0m\n",
      "\u001b[34m[2022-07-14 06:33:21.428 algo-1:32 INFO hook.py:591] name:decoder.block.4.layer.0.layer_norm.weight count_params:768\u001b[0m\n",
      "\u001b[34m[2022-07-14 06:33:21.428 algo-1:32 INFO hook.py:591] name:decoder.block.4.layer.1.EncDecAttention.q.weight count_params:589824\u001b[0m\n",
      "\u001b[34m[2022-07-14 06:33:21.429 algo-1:32 INFO hook.py:591] name:decoder.block.4.layer.1.EncDecAttention.k.weight count_params:589824\u001b[0m\n",
      "\u001b[34m[2022-07-14 06:33:21.429 algo-1:32 INFO hook.py:591] name:decoder.block.4.layer.1.EncDecAttention.v.weight count_params:589824\u001b[0m\n",
      "\u001b[34m[2022-07-14 06:33:21.429 algo-1:32 INFO hook.py:591] name:decoder.block.4.layer.1.EncDecAttention.o.weight count_params:589824\u001b[0m\n",
      "\u001b[34m[2022-07-14 06:33:21.430 algo-1:32 INFO hook.py:591] name:decoder.block.4.layer.1.layer_norm.weight count_params:768\u001b[0m\n",
      "\u001b[34m[2022-07-14 06:33:21.430 algo-1:32 INFO hook.py:591] name:decoder.block.4.layer.2.DenseReluDense.wi.weight count_params:2359296\u001b[0m\n",
      "\u001b[34m[2022-07-14 06:33:21.430 algo-1:32 INFO hook.py:591] name:decoder.block.4.layer.2.DenseReluDense.wo.weight count_params:2359296\u001b[0m\n",
      "\u001b[34m[2022-07-14 06:33:21.430 algo-1:32 INFO hook.py:591] name:decoder.block.4.layer.2.layer_norm.weight count_params:768\u001b[0m\n",
      "\u001b[34m[2022-07-14 06:33:21.430 algo-1:32 INFO hook.py:591] name:decoder.block.5.layer.0.SelfAttention.q.weight count_params:589824\u001b[0m\n",
      "\u001b[34m[2022-07-14 06:33:21.431 algo-1:32 INFO hook.py:591] name:decoder.block.5.layer.0.SelfAttention.k.weight count_params:589824\u001b[0m\n",
      "\u001b[34m[2022-07-14 06:33:21.431 algo-1:32 INFO hook.py:591] name:decoder.block.5.layer.0.SelfAttention.v.weight count_params:589824\u001b[0m\n",
      "\u001b[34m[2022-07-14 06:33:21.431 algo-1:32 INFO hook.py:591] name:decoder.block.5.layer.0.SelfAttention.o.weight count_params:589824\u001b[0m\n",
      "\u001b[34m[2022-07-14 06:33:21.431 algo-1:32 INFO hook.py:591] name:decoder.block.5.layer.0.layer_norm.weight count_params:768\u001b[0m\n",
      "\u001b[34m[2022-07-14 06:33:21.431 algo-1:32 INFO hook.py:591] name:decoder.block.5.layer.1.EncDecAttention.q.weight count_params:589824\u001b[0m\n",
      "\u001b[34m[2022-07-14 06:33:21.431 algo-1:32 INFO hook.py:591] name:decoder.block.5.layer.1.EncDecAttention.k.weight count_params:589824\u001b[0m\n",
      "\u001b[34m[2022-07-14 06:33:21.431 algo-1:32 INFO hook.py:591] name:decoder.block.5.layer.1.EncDecAttention.v.weight count_params:589824\u001b[0m\n",
      "\u001b[34m[2022-07-14 06:33:21.432 algo-1:32 INFO hook.py:591] name:decoder.block.5.layer.1.EncDecAttention.o.weight count_params:589824\u001b[0m\n",
      "\u001b[34m[2022-07-14 06:33:21.432 algo-1:32 INFO hook.py:591] name:decoder.block.5.layer.1.layer_norm.weight count_params:768\u001b[0m\n",
      "\u001b[34m[2022-07-14 06:33:21.432 algo-1:32 INFO hook.py:591] name:decoder.block.5.layer.2.DenseReluDense.wi.weight count_params:2359296\u001b[0m\n",
      "\u001b[34m[2022-07-14 06:33:21.432 algo-1:32 INFO hook.py:591] name:decoder.block.5.layer.2.DenseReluDense.wo.weight count_params:2359296\u001b[0m\n",
      "\u001b[34m[2022-07-14 06:33:21.432 algo-1:32 INFO hook.py:591] name:decoder.block.5.layer.2.layer_norm.weight count_params:768\u001b[0m\n",
      "\u001b[34m[2022-07-14 06:33:21.432 algo-1:32 INFO hook.py:591] name:decoder.block.6.layer.0.SelfAttention.q.weight count_params:589824\u001b[0m\n",
      "\u001b[34m[2022-07-14 06:33:21.432 algo-1:32 INFO hook.py:591] name:decoder.block.6.layer.0.SelfAttention.k.weight count_params:589824\u001b[0m\n",
      "\u001b[34m[2022-07-14 06:33:21.432 algo-1:32 INFO hook.py:591] name:decoder.block.6.layer.0.SelfAttention.v.weight count_params:589824\u001b[0m\n",
      "\u001b[34m[2022-07-14 06:33:21.432 algo-1:32 INFO hook.py:591] name:decoder.block.6.layer.0.SelfAttention.o.weight count_params:589824\u001b[0m\n",
      "\u001b[34m[2022-07-14 06:33:21.433 algo-1:32 INFO hook.py:591] name:decoder.block.6.layer.0.layer_norm.weight count_params:768\u001b[0m\n",
      "\u001b[34m[2022-07-14 06:33:21.433 algo-1:32 INFO hook.py:591] name:decoder.block.6.layer.1.EncDecAttention.q.weight count_params:589824\u001b[0m\n",
      "\u001b[34m[2022-07-14 06:33:21.433 algo-1:32 INFO hook.py:591] name:decoder.block.6.layer.1.EncDecAttention.k.weight count_params:589824\u001b[0m\n",
      "\u001b[34m[2022-07-14 06:33:21.434 algo-1:32 INFO hook.py:591] name:decoder.block.6.layer.1.EncDecAttention.v.weight count_params:589824\u001b[0m\n",
      "\u001b[34m[2022-07-14 06:33:21.434 algo-1:32 INFO hook.py:591] name:decoder.block.6.layer.1.EncDecAttention.o.weight count_params:589824\u001b[0m\n",
      "\u001b[34m[2022-07-14 06:33:21.434 algo-1:32 INFO hook.py:591] name:decoder.block.6.layer.1.layer_norm.weight count_params:768\u001b[0m\n",
      "\u001b[34m[2022-07-14 06:33:21.434 algo-1:32 INFO hook.py:591] name:decoder.block.6.layer.2.DenseReluDense.wi.weight count_params:2359296\u001b[0m\n",
      "\u001b[34m[2022-07-14 06:33:21.434 algo-1:32 INFO hook.py:591] name:decoder.block.6.layer.2.DenseReluDense.wo.weight count_params:2359296\u001b[0m\n",
      "\u001b[34m[2022-07-14 06:33:21.434 algo-1:32 INFO hook.py:591] name:decoder.block.6.layer.2.layer_norm.weight count_params:768\u001b[0m\n",
      "\u001b[34m[2022-07-14 06:33:21.435 algo-1:32 INFO hook.py:591] name:decoder.block.7.layer.0.SelfAttention.q.weight count_params:589824\u001b[0m\n",
      "\u001b[34m[2022-07-14 06:33:21.435 algo-1:32 INFO hook.py:591] name:decoder.block.7.layer.0.SelfAttention.k.weight count_params:589824\u001b[0m\n",
      "\u001b[34m[2022-07-14 06:33:21.435 algo-1:32 INFO hook.py:591] name:decoder.block.7.layer.0.SelfAttention.v.weight count_params:589824\u001b[0m\n",
      "\u001b[34m[2022-07-14 06:33:21.436 algo-1:32 INFO hook.py:591] name:decoder.block.7.layer.0.SelfAttention.o.weight count_params:589824\u001b[0m\n",
      "\u001b[34m[2022-07-14 06:33:21.436 algo-1:32 INFO hook.py:591] name:decoder.block.7.layer.0.layer_norm.weight count_params:768\u001b[0m\n",
      "\u001b[34m[2022-07-14 06:33:21.436 algo-1:32 INFO hook.py:591] name:decoder.block.7.layer.1.EncDecAttention.q.weight count_params:589824\u001b[0m\n",
      "\u001b[34m[2022-07-14 06:33:21.436 algo-1:32 INFO hook.py:591] name:decoder.block.7.layer.1.EncDecAttention.k.weight count_params:589824\u001b[0m\n",
      "\u001b[34m[2022-07-14 06:33:21.436 algo-1:32 INFO hook.py:591] name:decoder.block.7.layer.1.EncDecAttention.v.weight count_params:589824\u001b[0m\n",
      "\u001b[34m[2022-07-14 06:33:21.436 algo-1:32 INFO hook.py:591] name:decoder.block.7.layer.1.EncDecAttention.o.weight count_params:589824\u001b[0m\n",
      "\u001b[34m[2022-07-14 06:33:21.437 algo-1:32 INFO hook.py:591] name:decoder.block.7.layer.1.layer_norm.weight count_params:768\u001b[0m\n",
      "\u001b[34m[2022-07-14 06:33:21.437 algo-1:32 INFO hook.py:591] name:decoder.block.7.layer.2.DenseReluDense.wi.weight count_params:2359296\u001b[0m\n",
      "\u001b[34m[2022-07-14 06:33:21.437 algo-1:32 INFO hook.py:591] name:decoder.block.7.layer.2.DenseReluDense.wo.weight count_params:2359296\u001b[0m\n",
      "\u001b[34m[2022-07-14 06:33:21.437 algo-1:32 INFO hook.py:591] name:decoder.block.7.layer.2.layer_norm.weight count_params:768\u001b[0m\n",
      "\u001b[34m[2022-07-14 06:33:21.437 algo-1:32 INFO hook.py:591] name:decoder.block.8.layer.0.SelfAttention.q.weight count_params:589824\u001b[0m\n",
      "\u001b[34m[2022-07-14 06:33:21.437 algo-1:32 INFO hook.py:591] name:decoder.block.8.layer.0.SelfAttention.k.weight count_params:589824\u001b[0m\n",
      "\u001b[34m[2022-07-14 06:33:21.437 algo-1:32 INFO hook.py:591] name:decoder.block.8.layer.0.SelfAttention.v.weight count_params:589824\u001b[0m\n",
      "\u001b[34m[2022-07-14 06:33:21.437 algo-1:32 INFO hook.py:591] name:decoder.block.8.layer.0.SelfAttention.o.weight count_params:589824\u001b[0m\n",
      "\u001b[34m[2022-07-14 06:33:21.438 algo-1:32 INFO hook.py:591] name:decoder.block.8.layer.0.layer_norm.weight count_params:768\u001b[0m\n",
      "\u001b[34m[2022-07-14 06:33:21.438 algo-1:32 INFO hook.py:591] name:decoder.block.8.layer.1.EncDecAttention.q.weight count_params:589824\u001b[0m\n",
      "\u001b[34m[2022-07-14 06:33:21.438 algo-1:32 INFO hook.py:591] name:decoder.block.8.layer.1.EncDecAttention.k.weight count_params:589824\u001b[0m\n",
      "\u001b[34m[2022-07-14 06:33:21.438 algo-1:32 INFO hook.py:591] name:decoder.block.8.layer.1.EncDecAttention.v.weight count_params:589824\u001b[0m\n",
      "\u001b[34m[2022-07-14 06:33:21.438 algo-1:32 INFO hook.py:591] name:decoder.block.8.layer.1.EncDecAttention.o.weight count_params:589824\u001b[0m\n",
      "\u001b[34m[2022-07-14 06:33:21.438 algo-1:32 INFO hook.py:591] name:decoder.block.8.layer.1.layer_norm.weight count_params:768\u001b[0m\n",
      "\u001b[34m[2022-07-14 06:33:21.438 algo-1:32 INFO hook.py:591] name:decoder.block.8.layer.2.DenseReluDense.wi.weight count_params:2359296\u001b[0m\n",
      "\u001b[34m[2022-07-14 06:33:21.438 algo-1:32 INFO hook.py:591] name:decoder.block.8.layer.2.DenseReluDense.wo.weight count_params:2359296\u001b[0m\n",
      "\u001b[34m[2022-07-14 06:33:21.438 algo-1:32 INFO hook.py:591] name:decoder.block.8.layer.2.layer_norm.weight count_params:768\u001b[0m\n",
      "\u001b[34m[2022-07-14 06:33:21.439 algo-1:32 INFO hook.py:591] name:decoder.block.9.layer.0.SelfAttention.q.weight count_params:589824\u001b[0m\n",
      "\u001b[34m[2022-07-14 06:33:21.439 algo-1:32 INFO hook.py:591] name:decoder.block.9.layer.0.SelfAttention.k.weight count_params:589824\u001b[0m\n",
      "\u001b[34m[2022-07-14 06:33:21.439 algo-1:32 INFO hook.py:591] name:decoder.block.9.layer.0.SelfAttention.v.weight count_params:589824\u001b[0m\n",
      "\u001b[34m[2022-07-14 06:33:21.439 algo-1:32 INFO hook.py:591] name:decoder.block.9.layer.0.SelfAttention.o.weight count_params:589824\u001b[0m\n",
      "\u001b[34m[2022-07-14 06:33:21.439 algo-1:32 INFO hook.py:591] name:decoder.block.9.layer.0.layer_norm.weight count_params:768\u001b[0m\n",
      "\u001b[34m[2022-07-14 06:33:21.439 algo-1:32 INFO hook.py:591] name:decoder.block.9.layer.1.EncDecAttention.q.weight count_params:589824\u001b[0m\n",
      "\u001b[34m[2022-07-14 06:33:21.439 algo-1:32 INFO hook.py:591] name:decoder.block.9.layer.1.EncDecAttention.k.weight count_params:589824\u001b[0m\n",
      "\u001b[34m[2022-07-14 06:33:21.440 algo-1:32 INFO hook.py:591] name:decoder.block.9.layer.1.EncDecAttention.v.weight count_params:589824\u001b[0m\n",
      "\u001b[34m[2022-07-14 06:33:21.440 algo-1:32 INFO hook.py:591] name:decoder.block.9.layer.1.EncDecAttention.o.weight count_params:589824\u001b[0m\n",
      "\u001b[34m[2022-07-14 06:33:21.440 algo-1:32 INFO hook.py:591] name:decoder.block.9.layer.1.layer_norm.weight count_params:768\u001b[0m\n",
      "\u001b[34m[2022-07-14 06:33:21.440 algo-1:32 INFO hook.py:591] name:decoder.block.9.layer.2.DenseReluDense.wi.weight count_params:2359296\u001b[0m\n",
      "\u001b[34m[2022-07-14 06:33:21.440 algo-1:32 INFO hook.py:591] name:decoder.block.9.layer.2.DenseReluDense.wo.weight count_params:2359296\u001b[0m\n",
      "\u001b[34m[2022-07-14 06:33:21.440 algo-1:32 INFO hook.py:591] name:decoder.block.9.layer.2.layer_norm.weight count_params:768\u001b[0m\n",
      "\u001b[34m[2022-07-14 06:33:21.440 algo-1:32 INFO hook.py:591] name:decoder.block.10.layer.0.SelfAttention.q.weight count_params:589824\u001b[0m\n",
      "\u001b[34m[2022-07-14 06:33:21.441 algo-1:32 INFO hook.py:591] name:decoder.block.10.layer.0.SelfAttention.k.weight count_params:589824\u001b[0m\n",
      "\u001b[34m[2022-07-14 06:33:21.441 algo-1:32 INFO hook.py:591] name:decoder.block.10.layer.0.SelfAttention.v.weight count_params:589824\u001b[0m\n",
      "\u001b[34m[2022-07-14 06:33:21.441 algo-1:32 INFO hook.py:591] name:decoder.block.10.layer.0.SelfAttention.o.weight count_params:589824\u001b[0m\n",
      "\u001b[34m[2022-07-14 06:33:21.441 algo-1:32 INFO hook.py:591] name:decoder.block.10.layer.0.layer_norm.weight count_params:768\u001b[0m\n",
      "\u001b[34m[2022-07-14 06:33:21.441 algo-1:32 INFO hook.py:591] name:decoder.block.10.layer.1.EncDecAttention.q.weight count_params:589824\u001b[0m\n",
      "\u001b[34m[2022-07-14 06:33:21.441 algo-1:32 INFO hook.py:591] name:decoder.block.10.layer.1.EncDecAttention.k.weight count_params:589824\u001b[0m\n",
      "\u001b[34m[2022-07-14 06:33:21.441 algo-1:32 INFO hook.py:591] name:decoder.block.10.layer.1.EncDecAttention.v.weight count_params:589824\u001b[0m\n",
      "\u001b[34m[2022-07-14 06:33:21.441 algo-1:32 INFO hook.py:591] name:decoder.block.10.layer.1.EncDecAttention.o.weight count_params:589824\u001b[0m\n",
      "\u001b[34m[2022-07-14 06:33:21.441 algo-1:32 INFO hook.py:591] name:decoder.block.10.layer.1.layer_norm.weight count_params:768\u001b[0m\n",
      "\u001b[34m[2022-07-14 06:33:21.442 algo-1:32 INFO hook.py:591] name:decoder.block.10.layer.2.DenseReluDense.wi.weight count_params:2359296\u001b[0m\n",
      "\u001b[34m[2022-07-14 06:33:21.442 algo-1:32 INFO hook.py:591] name:decoder.block.10.layer.2.DenseReluDense.wo.weight count_params:2359296\u001b[0m\n",
      "\u001b[34m[2022-07-14 06:33:21.442 algo-1:32 INFO hook.py:591] name:decoder.block.10.layer.2.layer_norm.weight count_params:768\u001b[0m\n",
      "\u001b[34m[2022-07-14 06:33:21.442 algo-1:32 INFO hook.py:591] name:decoder.block.11.layer.0.SelfAttention.q.weight count_params:589824\u001b[0m\n",
      "\u001b[34m[2022-07-14 06:33:21.442 algo-1:32 INFO hook.py:591] name:decoder.block.11.layer.0.SelfAttention.k.weight count_params:589824\u001b[0m\n",
      "\u001b[34m[2022-07-14 06:33:21.442 algo-1:32 INFO hook.py:591] name:decoder.block.11.layer.0.SelfAttention.v.weight count_params:589824\u001b[0m\n",
      "\u001b[34m[2022-07-14 06:33:21.442 algo-1:32 INFO hook.py:591] name:decoder.block.11.layer.0.SelfAttention.o.weight count_params:589824\u001b[0m\n",
      "\u001b[34m[2022-07-14 06:33:21.442 algo-1:32 INFO hook.py:591] name:decoder.block.11.layer.0.layer_norm.weight count_params:768\u001b[0m\n",
      "\u001b[34m[2022-07-14 06:33:21.443 algo-1:32 INFO hook.py:591] name:decoder.block.11.layer.1.EncDecAttention.q.weight count_params:589824\u001b[0m\n",
      "\u001b[34m[2022-07-14 06:33:21.443 algo-1:32 INFO hook.py:591] name:decoder.block.11.layer.1.EncDecAttention.k.weight count_params:589824\u001b[0m\n",
      "\u001b[34m[2022-07-14 06:33:21.443 algo-1:32 INFO hook.py:591] name:decoder.block.11.layer.1.EncDecAttention.v.weight count_params:589824\u001b[0m\n",
      "\u001b[34m[2022-07-14 06:33:21.443 algo-1:32 INFO hook.py:591] name:decoder.block.11.layer.1.EncDecAttention.o.weight count_params:589824\u001b[0m\n",
      "\u001b[34m[2022-07-14 06:33:21.443 algo-1:32 INFO hook.py:591] name:decoder.block.11.layer.1.layer_norm.weight count_params:768\u001b[0m\n",
      "\u001b[34m[2022-07-14 06:33:21.444 algo-1:32 INFO hook.py:591] name:decoder.block.11.layer.2.DenseReluDense.wi.weight count_params:2359296\u001b[0m\n",
      "\u001b[34m[2022-07-14 06:33:21.444 algo-1:32 INFO hook.py:591] name:decoder.block.11.layer.2.DenseReluDense.wo.weight count_params:2359296\u001b[0m\n",
      "\u001b[34m[2022-07-14 06:33:21.444 algo-1:32 INFO hook.py:591] name:decoder.block.11.layer.2.layer_norm.weight count_params:768\u001b[0m\n",
      "\u001b[34m[2022-07-14 06:33:21.444 algo-1:32 INFO hook.py:591] name:decoder.final_layer_norm.weight count_params:768\u001b[0m\n",
      "\u001b[34m[2022-07-14 06:33:21.444 algo-1:32 INFO hook.py:593] Total Trainable Params: 222882048\u001b[0m\n",
      "\u001b[34m[2022-07-14 06:33:21.444 algo-1:32 INFO hook.py:425] Monitoring the collections: losses\u001b[0m\n",
      "\u001b[34m[2022-07-14 06:33:21.450 algo-1:32 INFO hook.py:488] Hook is writing from the hook with pid: 32\u001b[0m\n",
      "\u001b[34m{'loss': 0.338, 'learning_rate': 0.0002760765550239234, 'epoch': 0.8}\u001b[0m\n",
      "\u001b[34mSaving model checkpoint to /opt/ml/model/checkpoint-500\u001b[0m\n",
      "\u001b[34mConfiguration saved in /opt/ml/model/checkpoint-500/config.json\u001b[0m\n",
      "\u001b[34mModel weights saved in /opt/ml/model/checkpoint-500/pytorch_model.bin\u001b[0m\n",
      "\u001b[34mtokenizer config file saved in /opt/ml/model/checkpoint-500/tokenizer_config.json\u001b[0m\n",
      "\u001b[34mSpecial tokens file saved in /opt/ml/model/checkpoint-500/special_tokens_map.json\u001b[0m\n",
      "\u001b[34mCopy vocab file to /opt/ml/model/checkpoint-500/spiece.model\u001b[0m\n",
      "\u001b[34m***** Running Evaluation *****\n",
      "  Num examples = 687\n",
      "  Batch size = 8\u001b[0m\n",
      "\u001b[34m{'eval_loss': 0.1532178521156311, 'eval_runtime': 13.395, 'eval_samples_per_second': 51.288, 'epoch': 1.0}\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "from sagemaker.huggingface import HuggingFace\n",
    "from sagemaker.huggingface import TrainingCompilerConfig\n",
    "\n",
    "#speed up use sagemaker compiler https://towardsdatascience.com/speed-up-hugging-face-training-jobs-on-aws-by-up-to-50-with-sagemaker-training-compiler-9ad2ac5b0eb\n",
    "\n",
    "# hyperparameters which are passed to the training job\n",
    "hyperparameters={'reference_column':'sentence1_key',\n",
    "                 'hypothesis_column':'label',\n",
    "                 'train_file':'/opt/ml/input/data/train/train.csv',\n",
    "                 'validation_file':'/opt/ml/input/data/validation/test.csv',\n",
    "                 'test_file':'/opt/ml/input/data/test/test.csv',\n",
    "                 'output_dir':'/opt/ml/model',\n",
    "                 'do_train':True,\n",
    "                 'do_eval':True,\n",
    "                 'max_source_length': 128,\n",
    "                 'max_target_length': 128,\n",
    "                 'model_name_or_path': 't5-base',\n",
    "                 'learning_rate': 3e-4,\n",
    "                 'num_train_epochs': 10,\n",
    "                 'per_device_train_batch_size': 2,#16\n",
    "                 'gradient_accumulation_steps':2, \n",
    "                 'save_strategy':'steps',\n",
    "                 'evaluation_strategy':'epoch',\n",
    "                 'save_total_limit':1,\n",
    "                 'eval_steps':5000\n",
    "                 }\n",
    "\n",
    "# create the Estimator\n",
    "huggingface_estimator = HuggingFace(\n",
    "        entry_point='run_train.py',\n",
    "        source_dir='./scripts',\n",
    "        instance_type='ml.p3.2xlarge',#'ml.p3dn.24xlarge'\n",
    "        instance_count=1,\n",
    "        role=role,\n",
    "        max_run=24*60*60,\n",
    "        transformers_version='4.6',\n",
    "        pytorch_version='1.7',\n",
    "        py_version='py36',\n",
    "        volume_size=128,\n",
    "        #compiler_config=TrainingCompilerConfig(),\n",
    "        base_job_name='train-modelb-stary-0713-crossbook',\n",
    "        hyperparameters = hyperparameters,\n",
    "#         distribution=distribution\n",
    ")\n",
    "\n",
    "huggingface_estimator.fit({'train':training_input_path,'test':test_input_path,'validation': test_input_path})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "02d87389",
   "metadata": {},
   "source": [
    "# deploy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "39bc8b00",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sagemaker.huggingface import HuggingFaceModel\n",
    "import sagemaker\n",
    "\n",
    "role = sagemaker.get_execution_role()\n",
    "\n",
    "# create Hugging Face Model Class\n",
    "huggingface_model = HuggingFaceModel(\n",
    "   model_data=\"s3://sagemaker-us-east-1-726335585155/train-modelb-stary-0713-crossbook-2022-07-14-06-27-10-862/output/model.tar.gz\",  # path to your trained sagemaker model\n",
    "   role=role, # iam role with permissions to create an Endpoint\n",
    "   transformers_version=\"4.6\", # transformers version used\n",
    "   pytorch_version=\"1.7\", # pytorch version used\n",
    "   py_version=\"py36\", # python version of the DLC\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c0b01a58",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------!"
     ]
    }
   ],
   "source": [
    "predictor = huggingface_model.deploy(\n",
    "   initial_instance_count=1,\n",
    "   instance_type=\"ml.g4dn.xlarge\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a736720b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sagemaker.predictor import Predictor\n",
    "import sagemaker\n",
    "\n",
    "predictor = Predictor(endpoint_name=\"huggingface-pytorch-inference-2022-07-14-03-26-17-325\")\n",
    "predictor.serializer = sagemaker.serializers.JSONSerializer()\n",
    "predictor.deserializer = sagemaker.deserializers.JSONDeserializer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "1deddf08",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 13.3 ms, sys: 0 ns, total: 13.3 ms\n",
      "Wall time: 864 ms\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[{'generated_text': 'Annie said the sentence'}]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%time\n",
    "# example request, you always need to define \"inputs\"\n",
    "import time\n",
    "\n",
    "\n",
    "data = {\n",
    "   \"inputs\": 'prefix: \"Whatever. Chris has a heir to the pack now.\" She says.,\"Um last time I remembered in the laws it was clearly written that the offspring of the  Alpha can only take over if he is the son of the Alpha and the Luna and clearly after today no one will take this mistake,\" I point to her stomach , center: \"seriously again as you are just a mistress not the mate.\" I say putting a lot of emphasis on the word \\'mistress\\' as if it is the world\\'s most disgusting word. after: \"seriously again as you are just a mistress not the mate.\" I say putting a lot of emphasis on the word \\'mistress\\' as if it is the world\\'s most disgusting word.,\"Well at least he loves me.\" She says desperately. roles: [\\'Skylar\\', \\'Logan\\']'\n",
    "}\n",
    "\n",
    "# request\n",
    "predictor.predict(data)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b5fe800a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "df = pd.read_csv('../model_b_data/test.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "91d73cac",
   "metadata": {},
   "outputs": [],
   "source": [
    "res = []\n",
    "for i in df['sentence1_key']:\n",
    "    data = {\"inputs\": i}\n",
    "\n",
    "    # request\n",
    "    res.append(predictor.predict(data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "229189c2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Annie said the sentence',\n",
       " 'Annie said the sentence',\n",
       " 'Annie said the sentence',\n",
       " 'Annie said the sentence',\n",
       " 'Annie said the sentence']"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_pred[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "5331b7b6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    Annie said the sentence\n",
       "1    Annie said the sentence\n",
       "2    Annie said the sentence\n",
       "3    Annie said the sentence\n",
       "4    Annie said the sentence\n",
       "Name: label, dtype: object"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_true[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "a6dd505e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                         precision    recall  f1-score   support\n",
      "\n",
      "Annie said the sentence       0.96      0.99      0.97       211\n",
      "  Ava said the sentence       0.96      0.99      0.97       210\n",
      " Kade said the sentence       0.97      0.93      0.95       120\n",
      " Zach said the sentence       0.98      0.95      0.96       146\n",
      "\n",
      "               accuracy                           0.97       687\n",
      "              macro avg       0.97      0.96      0.96       687\n",
      "           weighted avg       0.97      0.97      0.97       687\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#p f r\n",
    "from sklearn.metrics import classification_report\n",
    "\n",
    "y_pred = [i[0]['generated_text'] for i in res]\n",
    "y_true = df['label']\n",
    " \n",
    "print(classification_report(y_true, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "dcc01895",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'prefix: \"Whatever. Chris has a heir to the pack now.\" She says.,\"Um last time I remembered in the laws it was clearly written that the offspring of the  Alpha can only take over if he is the son of the Alpha and the Luna and clearly after today no one will take this mistake,\" I point to her stomach , center: \"seriously again as you are just a mistress not the mate.\" I say putting a lot of emphasis on the word \\'mistress\\' as if it is the world\\'s most disgusting word. after: \"seriously again as you are just a mistress not the mate.\" I say putting a lot of emphasis on the word \\'mistress\\' as if it is the world\\'s most disgusting word.,\"Well at least he loves me.\" She says desperately. roles: [\\'Skylar\\', \\'Logan\\']'"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['sentence1_key'][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "e653a7ce",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Logan said the sentence'"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['label'][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "b61eac3b",
   "metadata": {},
   "outputs": [],
   "source": [
    "x = 0\n",
    "for i in range(len(y_true)):\n",
    "    if y_true[i]==y_pred[i]:\n",
    "        x = x+1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "0b7c0cd4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.838"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x/len(y_true)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "b728fbb7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\"Who do you think I am? Of course I want it.\" She whispers and look at her stomach. '"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['sentence1_key'][362]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "2edcfd62",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\"What do you mean by that?\" She asks me surprised. '"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['sentence1_key'][370]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "624fe6aa",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "conda_amazonei_mxnet_p36",
   "language": "python",
   "name": "conda_amazonei_mxnet_p36"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
